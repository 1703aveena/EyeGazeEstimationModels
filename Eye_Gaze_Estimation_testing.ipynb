{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eye_Gaze_Estimation-testing.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWSTqsRfAeeI"
      },
      "source": [
        "!pip install coloredlogs\n",
        "!pip install --upgrade --force-reinstall tensorflow==1.15.0 \n",
        "!pip install ujson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5WIMRmwmdLg"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "#drive.flush_and_unmount()\n",
        "!ls \"/content/gdrive/My Drive/test-data-Eye-tracking/GazeML/\"\n",
        "##!cp \"/content/gdrive/My Drive/test-data-Eye-tracking/MPIIGaze.h5\" \"MPIIGaze.h5\"\n",
        "#!ls \"/content/gdrive/My Drive/test-data-Eye-tracking/imgs/imgs\"\n",
        "#dir = \"/content/gdrive/My Drive/test-data-Eye-tracking/GazeML/test-elg\"\n",
        "!ls \"/content/gdrive/My Drive/test-data-Eye-tracking/GazeML/test-elg\"\n",
        "\n",
        "\n",
        "#!rm -rf \"/content/gdrive/My Drive/test-data-Eye-tracking/GazeML-elg-outputs/\"*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Naple377txhh"
      },
      "source": [
        "!unzip \"/content/gdrive/My Drive/test-data-Eye-tracking/imgs.zip\" -d \"/content/data/\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_-_dG6xRw2I"
      },
      "source": [
        "\"\"\"##download weights and training data\n",
        "!wget -Nnv https://raw.githubusercontent.com/aveenakottwani/EyeGazeEstimationModels/main/Gaze%20Estimations/trainedModel/ELG_i180x108_f60x36_n64_m3.zip\n",
        "!wget -Nnv https://raw.githubusercontent.com/aveenakottwani/EyeGazeEstimationModels/main/Gaze%20Estimations/trainedModel/test7.mp4\n",
        "!rm -rf \"/content/outputs/ELG_i180x108_f60x36_n64_m3/\"\n",
        "!ls \"/content/outputs/ELG_i180x108_f60x36_n64_m3/\"\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJU0ppwupFsj"
      },
      "source": [
        "!wget -Nnv \"https://raw.githubusercontent.com/aveenakottwani/EyeGazeEstimationModels/main/Gaze%20Estimations/trainedModel/ELG_i60x36_f60x36_n32_m2.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bs4bzf2SPJX"
      },
      "source": [
        "!unzip \"/content/ELG_i60x36_f60x36_n32_m2.zip\" -d \"/content/outputs/\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUMGZ_k4ruR9"
      },
      "source": [
        "image_dir = \"/content/data/imgs/\" #\"/content/gdrive/My Drive/test-data-Eye-tracking/imgs/imgs\" #/content/gdrive/My Drive/test-data-Eye-tracking/data/imgs\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jfcoqVmWBP8"
      },
      "source": [
        "dir = \"/content/outputs/\"\n",
        "!ls \"/content/outputs/ELG_i60x36_f60x36_n32_m2/checkpoints\"\n",
        "!ls \"/content/gdrive/My Drive/test-data-Eye-tracking/data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iw2fSEiCAfE"
      },
      "source": [
        "##datasource\n",
        "\n",
        "\"\"\"Default specification of a data source.\"\"\"\n",
        "from collections import OrderedDict\n",
        "import multiprocessing\n",
        "import queue\n",
        "import threading\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class BaseDataSource(object):\n",
        "    \"\"\"Base DataSource class.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 tensorflow_session: tf.Session,\n",
        "                 data_format: str = 'NHWC',\n",
        "                 batch_size: int = 32,\n",
        "                 num_threads: int = max(4, multiprocessing.cpu_count()),\n",
        "                 min_after_dequeue: int = 1000,\n",
        "                 fread_queue_capacity: int = 0,\n",
        "                 preprocess_queue_capacity: int = 0,\n",
        "                 staging=False,\n",
        "                 shuffle=None,\n",
        "                 testing=False,\n",
        "                 ):\n",
        "        \"\"\"Initialize a data source instance.\"\"\"\n",
        "        assert tensorflow_session is not None and isinstance(tensorflow_session, tf.Session)\n",
        "        assert isinstance(batch_size, int) and batch_size > 0\n",
        "        if shuffle is None:\n",
        "            shuffle = staging\n",
        "        self.testing = testing\n",
        "        if testing:\n",
        "            assert not shuffle and not staging\n",
        "            # if num_threads != 1:\n",
        "            #     logger.info('Forcing use of single thread for live testing.')\n",
        "            # num_threads = 1\n",
        "        self.staging = staging\n",
        "        self.shuffle = shuffle\n",
        "        self.data_format = data_format.upper()\n",
        "        assert self.data_format == 'NHWC' or self.data_format == 'NCHW'\n",
        "        self.batch_size = batch_size\n",
        "        self.num_threads = num_threads\n",
        "        self._tensorflow_session = tensorflow_session\n",
        "        self._coordinator = tf.train.Coordinator()\n",
        "        self.all_threads = []\n",
        "\n",
        "        # Setup file read queue\n",
        "        self._fread_queue_capacity = fread_queue_capacity\n",
        "        if self._fread_queue_capacity == 0:\n",
        "            self._fread_queue_capacity = (num_threads + 1) * batch_size\n",
        "        self._fread_queue = queue.Queue(maxsize=self._fread_queue_capacity)\n",
        "\n",
        "        with tf.variable_scope(''.join(c for c in self.short_name if c.isalnum())):\n",
        "            # Setup preprocess queue\n",
        "            labels, dtypes, shapes = self._determine_dtypes_and_shapes()\n",
        "            self._preprocess_queue_capacity = (min_after_dequeue + (num_threads + 1) * batch_size\n",
        "                                               if preprocess_queue_capacity == 0\n",
        "                                               else preprocess_queue_capacity)\n",
        "            if shuffle:\n",
        "                self._preprocess_queue = tf.RandomShuffleQueue(\n",
        "                        capacity=self._preprocess_queue_capacity,\n",
        "                        min_after_dequeue=min_after_dequeue,\n",
        "                        dtypes=dtypes, shapes=shapes,\n",
        "                )\n",
        "            else:\n",
        "                self._preprocess_queue = tf.FIFOQueue(\n",
        "                        capacity=self._preprocess_queue_capacity,\n",
        "                        dtypes=dtypes, shapes=shapes,\n",
        "                )\n",
        "            self._tensors_to_enqueue = OrderedDict([\n",
        "                (label, tf.placeholder(dtype, shape=shape, name=label))\n",
        "                for label, dtype, shape in zip(labels, dtypes, shapes)\n",
        "            ])\n",
        "\n",
        "            self._enqueue_op = \\\n",
        "                self._preprocess_queue.enqueue(tuple(self._tensors_to_enqueue.values()))\n",
        "            self._preprocess_queue_close_op = \\\n",
        "                self._preprocess_queue.close(cancel_pending_enqueues=True)\n",
        "            self._preprocess_queue_size_op = self._preprocess_queue.size()\n",
        "            self._preprocess_queue_clear_op = \\\n",
        "                self._preprocess_queue.dequeue_up_to(self._preprocess_queue.size())\n",
        "            if not staging:\n",
        "                output_tensors = self._preprocess_queue.dequeue_many(self.batch_size)\n",
        "                if not isinstance(output_tensors, list):\n",
        "                    output_tensors = [output_tensors]\n",
        "                self._output_tensors = dict([\n",
        "                    (label, tensor) for label, tensor in zip(labels, output_tensors)\n",
        "                ])\n",
        "            else:\n",
        "                # Setup on-GPU staging area\n",
        "                self._staging_area = tf.contrib.staging.StagingArea(\n",
        "                    dtypes=dtypes,\n",
        "                    shapes=[tuple([batch_size] + list(shape)) for shape in shapes],\n",
        "                    capacity=1,  # This does not have to be high\n",
        "                )\n",
        "                self._staging_area_put_op = \\\n",
        "                    self._staging_area.put(self._preprocess_queue.dequeue_many(batch_size))\n",
        "                self._staging_area_clear_op = self._staging_area.clear()\n",
        "\n",
        "                self._output_tensors = dict([\n",
        "                    (label, tensor) for label, tensor in zip(labels, self._staging_area.get())\n",
        "                ])\n",
        "\n",
        "        logger.info('Initialized data source: \"%s\"' % self.short_name)\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Destruct and clean up instance.\"\"\"\n",
        "        self.cleanup()\n",
        "\n",
        "    @property\n",
        "    def num_entries(self):\n",
        "        \"\"\"Number of entries in this data source.\n",
        "        Used to calculate number of steps to train when asked to be trained for # epochs.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('BaseDataSource::num_entries not specified.')\n",
        "\n",
        "    @property\n",
        "    def short_name(self):\n",
        "        \"\"\"Short identifier for data source.\n",
        "        Overload this magic method if the class is generic, eg. supporting h5py/numpy arrays as\n",
        "        input with specific data sources.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('BaseDataSource::short_name not specified.')\n",
        "\n",
        "    __cleaned_up = False\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Force-close all threads.\"\"\"\n",
        "        if self.__cleaned_up:\n",
        "            return\n",
        "\n",
        "        # Clear queues\n",
        "        fread_threads = [t for t in self.all_threads if t.name.startswith('fread_')]\n",
        "        preprocess_threads = [t for t in self.all_threads if t.name.startswith('preprocess_')]\n",
        "        transfer_threads = [t for t in self.all_threads if t.name.startswith('transfer_')]\n",
        "\n",
        "        self._coordinator.request_stop()\n",
        "\n",
        "        # Unblock any self._fread_queue.put calls\n",
        "        while True:\n",
        "            try:\n",
        "                self._fread_queue.get_nowait()\n",
        "            except queue.Empty:\n",
        "                break\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        # Push data through to trigger exits in preprocess/transfer threads\n",
        "        for _ in range(self.batch_size * self.num_threads):\n",
        "            self._fread_queue.put(None)\n",
        "        self._tensorflow_session.run(self._preprocess_queue_close_op)\n",
        "        if self.staging:\n",
        "            self._tensorflow_session.run(self._staging_area_clear_op)\n",
        "\n",
        "        self._coordinator.join(self.all_threads, stop_grace_period_secs=5)\n",
        "        self.__cleaned_up = True\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset threads and empty queues (where possible).\"\"\"\n",
        "        assert self.testing is True\n",
        "\n",
        "        # Clear queues\n",
        "        self._coordinator.request_stop()\n",
        "        with self._fread_queue.mutex:  # Unblock any self._fread_queue.get calls\n",
        "            self._fread_queue.queue.clear()\n",
        "        for _ in range(2*self.num_threads):\n",
        "            self._fread_queue.put(None)\n",
        "        while True:  # Unblock any enqueue requests\n",
        "            preprocess_queue_size = self._tensorflow_session.run(self._preprocess_queue_size_op)\n",
        "            if preprocess_queue_size == 0:\n",
        "                break\n",
        "            self._tensorflow_session.run(self._preprocess_queue_clear_op)\n",
        "            time.sleep(0.1)\n",
        "        while True:  # Unblock any self._fread_queue.put calls\n",
        "            try:\n",
        "                self._fread_queue.get_nowait()\n",
        "            except queue.Empty:\n",
        "                break\n",
        "            time.sleep(0.1)\n",
        "        self._coordinator.join(self.all_threads, stop_grace_period_secs=5)\n",
        "\n",
        "        # Restart threads\n",
        "        self._coordinator.clear_stop()\n",
        "        self.create_and_start_threads()\n",
        "\n",
        "    def _determine_dtypes_and_shapes(self):\n",
        "        \"\"\"Determine the dtypes and shapes of Tensorflow queue and staging area entries.\"\"\"\n",
        "        while True:\n",
        "            raw_entry = next(self.entry_generator(yield_just_one=True))\n",
        "            if raw_entry is None:\n",
        "                continue\n",
        "            preprocessed_entry_dict = self.preprocess_entry(raw_entry)\n",
        "            if preprocessed_entry_dict is not None:\n",
        "                break\n",
        "        labels, values = zip(*list(preprocessed_entry_dict.items()))\n",
        "        dtypes = [value.dtype for value in values]\n",
        "        shapes = [value.shape for value in values]\n",
        "        return labels, dtypes, shapes\n",
        "\n",
        "    def entry_generator(self, yield_just_one=False):\n",
        "        \"\"\"Return a generator which reads an entry from disk or memory.\n",
        "        This method should be thread-safe so make sure to use threading.Lock where necessary.\n",
        "        The implemented method should explicitly handle the `yield_just_one=True` case to only\n",
        "        yield one entry without hanging in the middle of an infinite loop.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('BaseDataSource::entry_generator not implemented.')\n",
        "\n",
        "    def preprocess_entry(self, entry):\n",
        "        \"\"\"Preprocess a \"raw\" data entry and yield a dict.\n",
        "        Each element of an entry is provided to this method as separate arguments.\n",
        "        This method should be thread-safe so make sure to use threading.Lock where necessary.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('BaseDataSource::preprocess_entry not implemented.')\n",
        "\n",
        "    def read_entry_job(self):\n",
        "        \"\"\"Job to read an entry and enqueue to _fread_queue.\"\"\"\n",
        "        read_entry = self.entry_generator()\n",
        "        while not self._coordinator.should_stop():\n",
        "            try:\n",
        "                entry = next(read_entry)\n",
        "            except StopIteration:\n",
        "                if not self.testing:\n",
        "                    continue\n",
        "                else:\n",
        "                    logger.debug('Reached EOF in %s' % threading.current_thread().name)\n",
        "                    break\n",
        "            if entry is not None:\n",
        "                self._fread_queue.put(entry)\n",
        "        read_entry.close()\n",
        "        logger.debug('Exiting thread %s' % threading.current_thread().name)\n",
        "\n",
        "    def preprocess_job(self):\n",
        "        \"\"\"Job to fetch and preprocess an entry.\"\"\"\n",
        "        while not self._coordinator.should_stop():\n",
        "            raw_entry = self._fread_queue.get()\n",
        "            if raw_entry is None:\n",
        "                return\n",
        "            preprocessed_entry_dict = self.preprocess_entry(raw_entry)\n",
        "            if preprocessed_entry_dict is not None:\n",
        "                feed_dict = dict([(self._tensors_to_enqueue[label], value)\n",
        "                                  for label, value in preprocessed_entry_dict.items()])\n",
        "                try:\n",
        "                    self._tensorflow_session.run(self._enqueue_op, feed_dict=feed_dict)\n",
        "                except (tf.errors.CancelledError, RuntimeError):\n",
        "                    break\n",
        "        logger.debug('Exiting thread %s' % threading.current_thread().name)\n",
        "\n",
        "    def transfer_to_gpu_job(self):\n",
        "        \"\"\"Transfer a data entry from CPU memory to GPU memory.\"\"\"\n",
        "        while not self._coordinator.should_stop():\n",
        "            try:\n",
        "                self._tensorflow_session.run(self._staging_area_put_op)\n",
        "            except tf.errors.CancelledError or tf.errors.OutOfRangeError:\n",
        "                break\n",
        "        logger.debug('Exiting thread %s' % threading.current_thread().name)\n",
        "\n",
        "    def create_threads(self):\n",
        "        \"\"\"Create Python threads for multi-threaded read and preprocess jobs.\"\"\"\n",
        "        name = self.short_name\n",
        "        self.all_threads = []\n",
        "\n",
        "        def _create_and_register_thread(*args, **kwargs):\n",
        "            thread = threading.Thread(*args, **kwargs)\n",
        "            thread.daemon = True\n",
        "            self.all_threads.append(thread)\n",
        "\n",
        "        for i in range(self.num_threads):\n",
        "            # File read thread\n",
        "            _create_and_register_thread(target=self.read_entry_job, name='fread_%s_%d' % (name, i))\n",
        "\n",
        "            # Preprocess thread\n",
        "            _create_and_register_thread(target=self.preprocess_job,\n",
        "                                        name='preprocess_%s_%d' % (name, i))\n",
        "\n",
        "        if self.staging:\n",
        "            # Send-to-GPU thread\n",
        "            _create_and_register_thread(target=self.transfer_to_gpu_job,\n",
        "                                        name='transfer_%s_%d' % (name, i))\n",
        "\n",
        "    def start_threads(self):\n",
        "        \"\"\"Begin executing all created threads.\"\"\"\n",
        "        assert len(self.all_threads) > 0\n",
        "        for thread in self.all_threads:\n",
        "            thread.start()\n",
        "\n",
        "    def create_and_start_threads(self):\n",
        "        \"\"\"Create and begin threads for preprocessing.\"\"\"\n",
        "        self.create_threads()\n",
        "        self.start_threads()\n",
        "\n",
        "    @property\n",
        "    def output_tensors(self):\n",
        "        \"\"\"Return tensors holding a preprocessed batch.\"\"\"\n",
        "        return self._output_tensors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H4XhGgfCAif"
      },
      "source": [
        "##CheckpointManager\n",
        "\n",
        "\n",
        "\"\"\"Manage saving and loading of model checkpoints.\"\"\"\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class CheckpointManager(object):\n",
        "    \"\"\"Manager to coordinate saving and loading of trainable parameters.\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        \"\"\"Initialize manager based on given model instance.\"\"\"\n",
        "        self._tensorflow_session = model._tensorflow_session\n",
        "        self._model = model\n",
        "\n",
        "    def build_savers(self):\n",
        "        \"\"\"Create tf.train.Saver instances.\"\"\"\n",
        "        all_saveable_vars = sorted(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) +\n",
        "                                   tf.get_collection(tf.GraphKeys.SAVEABLE_OBJECTS) +\n",
        "                                   tf.get_collection(tf.GraphKeys.MOVING_AVERAGE_VARIABLES) +\n",
        "                                   tf.get_collection_ref('batch_norm_non_trainable'),\n",
        "                                   key=lambda v: v.name)\n",
        "\n",
        "        # Grab all available prefixes\n",
        "        all_prefixes = []\n",
        "        for v in all_saveable_vars:\n",
        "            name = v.name\n",
        "            if '/' not in name:\n",
        "                continue\n",
        "            prefix = name.split('/')[0]\n",
        "            if prefix == 'test' or prefix == 'learning_params':\n",
        "                continue\n",
        "            if prefix not in all_prefixes:\n",
        "                all_prefixes.append(prefix)\n",
        "\n",
        "        # For each prefix, create saver\n",
        "        self._savers = {}\n",
        "        for prefix in all_prefixes:\n",
        "            vars_to_save = [v for v in all_saveable_vars if v.name.startswith(prefix + '/')]\n",
        "            if len(vars_to_save):\n",
        "                self._savers[prefix] = tf.train.Saver(vars_to_save, max_to_keep=2)\n",
        "\n",
        "    def load_all(self):\n",
        "        \"\"\"Load all available weights for each known prefix.\"\"\"\n",
        "        print(\"in load_all - to restore\")\n",
        "        iteration_number = 0\n",
        "        iteration_numbers = []\n",
        "        for prefix, saver in self._savers.items():\n",
        "            output_path = '%s/checkpoints/%s' % (self._model.output_path, prefix)\n",
        "            checkpoint = tf.train.get_checkpoint_state(output_path)\n",
        "            print(\"load_all - output path where restore is tried from\",self._model.output_path,prefix,checkpoint)\n",
        "            if checkpoint and checkpoint.model_checkpoint_path:\n",
        "                checkpoint_name = os.path.basename(checkpoint.model_checkpoint_path)\n",
        "                try:  # Attempt to restore saveable variables\n",
        "                    print(\"loadd_all - output path where restore is tried from\",output_path,checkpoint_name,prefix)\n",
        "                    self._savers[prefix].restore(self._tensorflow_session,\n",
        "                                                 '%s/%s' % (output_path, checkpoint_name))\n",
        "                    print(\"in load_all - tried restore\")\n",
        "                    iteration_numbers.append(\n",
        "                        int(next(re.finditer(\"(\\d+)(?!.*\\d)\", checkpoint_name)).group(0))\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    import traceback\n",
        "                    traceback.print_exc()\n",
        "        if len(iteration_numbers) > 0:\n",
        "            iteration_number = np.amax(iteration_numbers)\n",
        "        return iteration_number\n",
        "\n",
        "    def save_all(self, iteration_number):\n",
        "        \"\"\"Save all prefixes.\"\"\"\n",
        "        prefixes_to_use = []\n",
        "        for schedule in self._model._learning_schedule:\n",
        "            for prefixes in schedule['loss_terms_to_optimize'].values():\n",
        "                prefixes_to_use += prefixes\n",
        "        prefixes_to_use = list(set(prefixes_to_use))\n",
        "\n",
        "        for prefix, saver in self._savers.items():\n",
        "            if prefix not in prefixes_to_use:\n",
        "                continue\n",
        "            output_path = '%s/checkpoints/%s' % (self._model.output_path, prefix)\n",
        "            if not os.path.isdir(output_path):\n",
        "                os.makedirs(output_path)\n",
        "            saver.save(self._tensorflow_session, output_path + '/model',\n",
        "                       global_step=iteration_number)\n",
        "            logger.debug('Saved %s' % output_path)\n",
        "        logger.info('CheckpointManager::save_all call done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU1-6lIDCAk_"
      },
      "source": [
        "##SummaryManager\n",
        "\n",
        "\"\"\"Manage registration and evaluation of summary operations.\"\"\"\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class SummaryManager(object):\n",
        "    \"\"\"Manager to remember and run summary operations as necessary.\"\"\"\n",
        "\n",
        "    def __init__(self, model, cheap_ops_every_n_secs=2, expensive_ops_every_n_mins=2):\n",
        "        \"\"\"Initialize manager based on given model instance.\"\"\"\n",
        "        self._tensorflow_session = model._tensorflow_session\n",
        "        self._model = model\n",
        "        self._cheap_ops = {\n",
        "            'train': {},\n",
        "            'test': {},\n",
        "            'full_test': {},\n",
        "        }\n",
        "        self._expensive_ops = {\n",
        "            'train': {},\n",
        "            'test': {},\n",
        "            'full_test': {},\n",
        "        }\n",
        "        self._cheap_ops_every_n_secs = cheap_ops_every_n_secs\n",
        "        self._expensive_ops_every_n_secs = 60 * expensive_ops_every_n_mins\n",
        "\n",
        "        self._ready_to_write = False\n",
        "\n",
        "    def _prepare_for_write(self):\n",
        "        \"\"\"Merge together cheap and expensive ops separately.\"\"\"\n",
        "        self._writer = tf.summary.FileWriter(self._model.output_path,\n",
        "                                             self._tensorflow_session.graph)\n",
        "        for mode in ('train', 'test', 'full_test'):\n",
        "            self._expensive_ops[mode].update(self._cheap_ops[mode])\n",
        "        self._ready_to_write = True\n",
        "\n",
        "    def get_ops(self, mode='train'):\n",
        "        \"\"\"Retrieve summary ops to evaluate at given iteration number.\"\"\"\n",
        "        if not self._ready_to_write:\n",
        "            self._prepare_for_write()\n",
        "        if mode == 'test' or mode == 'full_test':  # Always return all ops for test case\n",
        "            return self._expensive_ops[mode]\n",
        "        elif mode == 'train':  # Select ops to evaluate based on defined frequency\n",
        "            check_func = self._model.time.has_been_n_seconds_since_last\n",
        "            if check_func('expensive_summaries_train', self._expensive_ops_every_n_secs):\n",
        "                return self._expensive_ops[mode]\n",
        "            elif check_func('cheap_summaries_train', self._cheap_ops_every_n_secs):\n",
        "                return self._cheap_ops[mode]\n",
        "        return {}\n",
        "\n",
        "    def write_summaries(self, summary_outputs, iteration_number):\n",
        "        \"\"\"Write given outputs to `self._writer`.\"\"\"\n",
        "        for _, summary in summary_outputs.items():\n",
        "            self._writer.add_summary(summary, global_step=iteration_number)\n",
        "\n",
        "    def _get_clean_name(self, operation):\n",
        "        name = operation.name\n",
        "\n",
        "        # Determine mode\n",
        "        mode = 'train'\n",
        "        if name.startswith('test/') or name.startswith('test_data/'):\n",
        "            mode = 'test'\n",
        "        elif name.startswith('loss/test/') or name.startswith('metric/test/'):\n",
        "            mode = 'full_test'\n",
        "\n",
        "        # Correct name\n",
        "        if mode == 'test':\n",
        "            name = name[name.index('/') + 1:]\n",
        "        elif mode == 'full_test':\n",
        "            name = '/'.join(name.split('/')[2:])\n",
        "        if name[-2] == ':':\n",
        "            name = name[:-2]\n",
        "        return mode, name\n",
        "\n",
        "    def _register_cheap_op(self, operation):\n",
        "        mode, name = self._get_clean_name(operation)\n",
        "        try:\n",
        "            assert name not in self._cheap_ops[mode] and name not in self._expensive_ops[mode]\n",
        "        except AssertionError:\n",
        "            raise Exception('Duplicate definition of summary item: \"%s\"' % name)\n",
        "        self._cheap_ops[mode][name] = operation\n",
        "\n",
        "    def _register_expensive_op(self, operation):\n",
        "        mode, name = self._get_clean_name(operation)\n",
        "        try:\n",
        "            assert name not in self._cheap_ops[mode] and name not in self._expensive_ops[mode]\n",
        "        except AssertionError:\n",
        "            raise Exception('Duplicate definition of summary item: \"%s\"' % name)\n",
        "        self._expensive_ops[mode][name] = operation\n",
        "\n",
        "    def audio(self, name, tensor, **kwargs):\n",
        "        \"\"\"TODO: Log summary of audio.\"\"\"\n",
        "        raise NotImplementedError('SummaryManager::audio not implemented.')\n",
        "\n",
        "    def text(self, name, tensor, **kwargs):\n",
        "        \"\"\"TODO: Log summary of text.\"\"\"\n",
        "        raise NotImplementedError('SummaryManager::text not implemented.')\n",
        "\n",
        "    def histogram(self, name, tensor, **kwargs):\n",
        "        \"\"\"TODO: Log summary of audio.\"\"\"\n",
        "        operation = tf.summary.histogram(name, tensor, **kwargs)\n",
        "        self._register_expensive_op(operation)\n",
        "\n",
        "    def image(self, name, tensor, data_format='channels_last', **kwargs):\n",
        "        \"\"\"TODO: Log summary of image.\"\"\"\n",
        "        if data_format == 'channels_first':\n",
        "            tensor = tf.transpose(tensor, perm=(0, 2, 3, 1))\n",
        "        c = tensor.shape.as_list()[-1]\n",
        "        if c == 3:  # Assume RGB and convert to BGR for visualization\n",
        "            tensor = tensor[:, :, :, ::-1]   # TODO: find better solution\n",
        "        operation = tf.summary.image(name, tensor, **kwargs)\n",
        "        self._register_expensive_op(operation)\n",
        "\n",
        "    def _4d_tensor(self, name, tensor, **kwargs):\n",
        "        \"\"\"Display all filters in a grid for visualization.\"\"\"\n",
        "        h, w, c, num_tensor = tensor.shape.as_list()\n",
        "\n",
        "        # Try to visualise convolutional filters or feature maps\n",
        "        # See: https://gist.github.com/kukuruza/03731dc494603ceab0c5\n",
        "        # input shape: (Y, X, C, N)\n",
        "        if c != 1 and c != 3:\n",
        "            tensor = tf.reduce_mean(tensor, axis=2, keep_dims=True)\n",
        "            c = 1\n",
        "        # shape is now: (Y, X, 1|C, N)\n",
        "        v_min = tf.reduce_min(tensor)\n",
        "        v_max = tf.reduce_max(tensor)\n",
        "        tensor -= v_min\n",
        "        tensor *= 1.0 / (v_max - v_min)\n",
        "        tensor = tf.pad(tensor, [[1, 0], [1, 0], [0, 0], [0, 0]], 'CONSTANT')\n",
        "        tensor = tf.transpose(tensor, perm=(3, 0, 1, 2))\n",
        "        # shape is now: (N, Y, X, C)\n",
        "        # place tensor on grid\n",
        "        num_tensor_x = int(np.round(np.sqrt(num_tensor)))\n",
        "        num_tensor_y = num_tensor / num_tensor_x\n",
        "        while not num_tensor_y.is_integer():\n",
        "            num_tensor_x += 1\n",
        "            num_tensor_y = num_tensor / num_tensor_x\n",
        "        num_tensor_y = int(num_tensor_y)\n",
        "        h += 1\n",
        "        w += 1\n",
        "        tensor = tf.reshape(tensor, (num_tensor_x, h * num_tensor_y, w, c))\n",
        "        # shape is now: (N_x, Y * N_y, X, c)\n",
        "        tensor = tf.transpose(tensor, (0, 2, 1, 3))\n",
        "        # shape is now: (N_x, X, Y * N_y, c)\n",
        "        tensor = tf.reshape(tensor, (1, w * num_tensor_x, h * num_tensor_y, c))\n",
        "        # shape is now: (1, X * N_x, Y * N_y, c)\n",
        "        tensor = tf.transpose(tensor, (0, 2, 1, 3))\n",
        "        # shape is now: (1, Y * N_y, X * N_x, c)\n",
        "        tensor = tf.pad(tensor, [[0, 0], [0, 1], [0, 1], [0, 0]], 'CONSTANT')\n",
        "\n",
        "        self.image(name, tensor, **kwargs)\n",
        "\n",
        "    def filters(self, name, tensor, **kwargs):\n",
        "        \"\"\"Log summary of convolutional filters.\n",
        "        Note that this method expects the output of the convolutional layer when using\n",
        "        `tf.layers.conv2d` or for the filters to be defined in the same scope as the output tensor.\n",
        "        \"\"\"\n",
        "        assert 'data_format' not in kwargs\n",
        "        with tf.name_scope('viz_filters'):\n",
        "            # Find tensor holding trainable kernel weights\n",
        "            name_stem = '/'.join(tensor.name.split('/')[:-1]) + '/kernel'\n",
        "            matching_tensors = [t for t in tf.trainable_variables() if t.name.startswith(name_stem)]\n",
        "            assert len(matching_tensors) == 1\n",
        "            filters = matching_tensors[0]\n",
        "\n",
        "            # H x W x C x N\n",
        "            h, w, c, n = filters.shape.as_list()\n",
        "            filters = tf.transpose(filters, perm=(3, 2, 0, 1))\n",
        "            # N x C x H x W\n",
        "            filters = tf.reshape(filters, (n*c, 1, h, w))\n",
        "            # NC x 1 x H x W\n",
        "            filters = tf.transpose(filters, perm=(2, 3, 1, 0))\n",
        "            # H x W x 1 x NC\n",
        "\n",
        "            self._4d_tensor(name, filters, **kwargs)\n",
        "\n",
        "    def feature_maps(self, name, tensor, mean_across_channels=True, data_format='channels_last',\n",
        "                     **kwargs):\n",
        "        \"\"\"Log summary of feature maps / image activations.\"\"\"\n",
        "        with tf.name_scope('viz_featuremaps'):\n",
        "            if data_format == 'channels_first':\n",
        "                # N x C x H x W\n",
        "                tensor = tf.transpose(tensor, perm=(0, 2, 3, 1))\n",
        "            # N x H x W x C\n",
        "            if mean_across_channels:\n",
        "                tensor = tf.reduce_mean(tensor, axis=3, keepdims=True)\n",
        "                # N x H x W x 1\n",
        "                tensor = tf.transpose(tensor, perm=(1, 2, 3, 0))\n",
        "            else:\n",
        "                n, c, h, w = tensor.shape.as_list()\n",
        "                tensor = tf.reshape(tensor, (n*c, 1, h, w))\n",
        "                # N x 1 x H x W\n",
        "                tensor = tf.transpose(tensor, perm=(2, 3, 1, 0))\n",
        "            # H x W x 1 x N\n",
        "\n",
        "            self._4d_tensor(name, tensor, **kwargs)\n",
        "\n",
        "    def tiled_images(self, name, tensor, data_format='channels_last', **kwargs):\n",
        "        \"\"\"Log summary of feature maps / image activations.\"\"\"\n",
        "        with tf.name_scope('viz_featuremaps'):\n",
        "            if data_format == 'channels_first':\n",
        "                # N x C x H x W\n",
        "                tensor = tf.transpose(tensor, perm=(0, 2, 3, 1))\n",
        "            # N x H x W x C\n",
        "            tensor = tf.transpose(tensor, perm=(1, 2, 3, 0))\n",
        "            # H x W x C x N\n",
        "            self._4d_tensor(name, tensor, **kwargs)\n",
        "\n",
        "    def scalar(self, name, tensor, **kwargs):\n",
        "        \"\"\"Log summary of scalar.\"\"\"\n",
        "        operation = tf.summary.scalar(name, tensor, **kwargs)\n",
        "        self._register_cheap_op(operation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0vFp7sOFXnE"
      },
      "source": [
        "##TimeManager\n",
        "\n",
        "\"\"\"Routines to time events and restrict logs or operations by frequency.\"\"\"\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class TimeManager(object):\n",
        "    \"\"\"Manage timing of event executions or measure timings.\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        \"\"\"Initialize manager based on given model instance.\"\"\"\n",
        "        self._tensorflow_session = model._tensorflow_session\n",
        "        self._model = model\n",
        "\n",
        "        self._timers = {}\n",
        "        self._last_time = {}\n",
        "\n",
        "    def start(self, name, **kwargs):\n",
        "        \"\"\"Begin timer for given event/operation.\"\"\"\n",
        "        if name not in self._timers:\n",
        "            timer = Timer(**kwargs)\n",
        "            self._timers[name] = timer\n",
        "        else:\n",
        "            timer = self._timers[name]\n",
        "        timer.start()\n",
        "\n",
        "    def end(self, name):\n",
        "        \"\"\"End timer for given event/operation.\"\"\"\n",
        "        assert name in self._timers\n",
        "        return self._timers[name].end()\n",
        "\n",
        "    def has_been_n_seconds_since_last(self, identifier, seconds):\n",
        "        \"\"\"Indicate if enough time has passed since last time.\n",
        "        Also updates the `last time` record based on identifier.\n",
        "        \"\"\"\n",
        "        current_time = time.time()\n",
        "        if identifier not in self._last_time or \\\n",
        "           (current_time - self._last_time[identifier] > seconds):\n",
        "            self._last_time[identifier] = current_time\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def log_every(self, identifier, message, seconds=1):\n",
        "        \"\"\"Limit logging of messages based on specified interval and identifier.\"\"\"\n",
        "        if self.has_been_n_seconds_since_last(identifier, seconds):\n",
        "            logger.info(message)\n",
        "        else:\n",
        "            logger.debug(message)\n",
        "\n",
        "\n",
        "# A local Timer class for timing\n",
        "class Timer(object):\n",
        "    \"\"\"Record start and end times as requested and provide summaries.\"\"\"\n",
        "\n",
        "    def __init__(self, average_over_last_n_timings=10):\n",
        "        \"\"\"Store keyword parameters.\"\"\"\n",
        "        self._average_over_last_n_timings = average_over_last_n_timings\n",
        "        self._active = False\n",
        "        self._timings = []\n",
        "        self._start_time = -1\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Cache starting time.\"\"\"\n",
        "        # assert not self._active\n",
        "        self._start_time = time.time()\n",
        "        self._active = True\n",
        "\n",
        "    def end(self):\n",
        "        \"\"\"Check ending time and store difference.\"\"\"\n",
        "        assert self._active and self._start_time > 0\n",
        "\n",
        "        # Calculate difference\n",
        "        end_time = time.time()\n",
        "        time_difference = end_time - self._start_time\n",
        "\n",
        "        # Record timing (and trim history)\n",
        "        self._timings.append(time_difference)\n",
        "        if len(self._timings) > self._average_over_last_n_timings:\n",
        "            self._timings = self._timings[-self._average_over_last_n_timings:]\n",
        "\n",
        "        # Reset\n",
        "        self._start_time = -1\n",
        "        self._active = False\n",
        "\n",
        "        return time_difference\n",
        "\n",
        "    @property\n",
        "    def current_mean(self):\n",
        "        \"\"\"Calculate mean timing for as many trials as specified in constructor.\"\"\"\n",
        "        values = self._timings\n",
        "        return np.mean(values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsP0-AinFYG6"
      },
      "source": [
        "##LiveTester\n",
        "\n",
        "\"\"\"Concurrent testing during training.\"\"\"\n",
        "import collections\n",
        "import platform\n",
        "import threading\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class LiveTester(object):\n",
        "    \"\"\"Manage concurrent testing on test data source.\"\"\"\n",
        "\n",
        "    def __init__(self, model, data_source, use_batch_statistics=True):\n",
        "        \"\"\"Initialize tester with reference to model and data sources.\"\"\"\n",
        "        self.model = model\n",
        "        self.data = data_source\n",
        "        self.time = self.model.time\n",
        "        self.summary = self.model.summary\n",
        "        self._tensorflow_session = model._tensorflow_session\n",
        "\n",
        "        self._is_testing = False\n",
        "        self._condition = threading.Condition()\n",
        "\n",
        "        self._use_batch_statistics = use_batch_statistics\n",
        "\n",
        "    def stop(self):\n",
        "        logger.info('LiveTester::stop is being called.')\n",
        "        self._is_testing = False\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Handle deletion of instance by closing thread.\"\"\"\n",
        "        if not hasattr(self, '_coordinator'):\n",
        "            return\n",
        "        self._coordinator.request_stop()\n",
        "        with self._condition:\n",
        "            self._is_testing = True  # Break wait if waiting\n",
        "            self._condition.notify_all()\n",
        "        self._coordinator.join([self._thread], stop_grace_period_secs=1)\n",
        "\n",
        "    def _true_if_testing(self):\n",
        "        return self._is_testing\n",
        "\n",
        "    def trigger_test_if_not_testing(self, current_step):\n",
        "        \"\"\"If not currently testing, run test.\"\"\"\n",
        "        if not self._is_testing:\n",
        "            with self._condition:\n",
        "                self._is_testing = True\n",
        "                self._testing_at_step = current_step\n",
        "                self._condition.notify_all()\n",
        "\n",
        "    def test_job(self):\n",
        "        \"\"\"Evaluate requested metric over entire test set.\"\"\"\n",
        "        while not self._coordinator.should_stop():\n",
        "            with self._condition:\n",
        "                self._condition.wait_for(self._true_if_testing)\n",
        "                if self._coordinator.should_stop():\n",
        "                    break\n",
        "                should_stop = False\n",
        "                try:\n",
        "                    should_stop = self.do_full_test()\n",
        "                except:\n",
        "                    traceback.print_exc()\n",
        "                self._is_testing = False\n",
        "                if should_stop is True:\n",
        "                    break\n",
        "        logger.debug('Exiting thread %s' % threading.current_thread().name)\n",
        "\n",
        "    def do_full_test(self, sleep_between_batches=0.2):\n",
        "        # Copy current weights over\n",
        "        self.copy_model_weights()\n",
        "\n",
        "        # Reset data sources\n",
        "        for data_source_name, data_source in self.data.items():\n",
        "            data_source.reset()\n",
        "            num_batches = int(data_source.num_entries / data_source.batch_size)\n",
        "\n",
        "        # Decide what to evaluate\n",
        "        fetches = self._tensors_to_evaluate\n",
        "        outputs = dict([(name, list()) for name in fetches.keys()])\n",
        "\n",
        "        # Select random index to produce (image) summaries at\n",
        "        summary_index = np.random.randint(num_batches)\n",
        "\n",
        "        self.time.start('full test')\n",
        "        for i in range(num_batches):\n",
        "            if self._is_testing is not True:\n",
        "                logger.debug('Testing flag found to be `False` at iter. %d' % i)\n",
        "                break\n",
        "            logger.debug('Testing on %03d/%03d batches.' % (i + 1, num_batches))\n",
        "            if i == summary_index:\n",
        "                fetches['summaries'] = self.summary.get_ops(mode='test')\n",
        "            try:\n",
        "                output = self._tensorflow_session.run(\n",
        "                    fetches=fetches,\n",
        "                    feed_dict={\n",
        "                        self.model.is_training: False,\n",
        "                        self.model.use_batch_statistics: self._use_batch_statistics,\n",
        "                    },\n",
        "                )\n",
        "            except (tf.errors.CancelledError, RuntimeError):\n",
        "                return True\n",
        "            time.sleep(sleep_between_batches)  # Brief pause to prioritise training\n",
        "            if 'summaries' in output:  # Write summaries on first batch\n",
        "                self.summary.write_summaries(output['summaries'], self._testing_at_step)\n",
        "                del fetches['summaries']\n",
        "                del output['summaries']\n",
        "            for name, value in output.items():  # Gather results from this batch\n",
        "                outputs[name].append(output[name])\n",
        "        self.time.end('full test')\n",
        "\n",
        "        # If incomplete, skip this round of tests (most likely shutting down)\n",
        "        if len(list(outputs.values())[0]) != num_batches:\n",
        "            return True\n",
        "\n",
        "        # Calculate mean values\n",
        "        for name, values in outputs.items():\n",
        "            outputs[name] = np.mean(values)\n",
        "\n",
        "        # TODO: Log metric as summary\n",
        "        to_print = '[Test at step %06d] ' % self._testing_at_step\n",
        "        to_print += ', '.join([\n",
        "            '%s = %f' % (name, value) for name, value in outputs.items()\n",
        "        ])\n",
        "        logger.info(to_print)\n",
        "\n",
        "        # Store mean metrics/losses (and other summaries)\n",
        "        feed_dict = dict([(self._placeholders[name], value)\n",
        "                         for name, value in outputs.items()])\n",
        "        feed_dict[self.model.is_training] = False\n",
        "        feed_dict[self.model.use_batch_statistics] = True\n",
        "        try:\n",
        "            summaries = self._tensorflow_session.run(\n",
        "                fetches=self.summary.get_ops(mode='full_test'),\n",
        "                feed_dict=feed_dict,\n",
        "            )\n",
        "        except (tf.errors.CancelledError, RuntimeError):\n",
        "            return True\n",
        "        self.summary.write_summaries(summaries, self._testing_at_step)\n",
        "\n",
        "        return False\n",
        "\n",
        "    def do_final_full_test(self, current_step):\n",
        "        logger.info('Stopping the live testing threads.')\n",
        "\n",
        "        # Stop thread(s)\n",
        "        self._is_testing = False\n",
        "        self._coordinator.request_stop()\n",
        "        with self._condition:\n",
        "            self._is_testing = True  # Break wait if waiting\n",
        "            self._condition.notify_all()\n",
        "        self._coordinator.join([self._thread], stop_grace_period_secs=1)\n",
        "\n",
        "        # Start final full test\n",
        "        logger.info('Running final full test')\n",
        "        self.copy_model_weights()\n",
        "        self._is_testing = True\n",
        "        self._testing_at_step = current_step\n",
        "        self.do_full_test(sleep_between_batches=0)\n",
        "\n",
        "    def _post_model_build(self):\n",
        "        \"\"\"Prepare combined operation to copy model parameters over from CPU/GPU to CPU.\"\"\"\n",
        "        with tf.variable_scope('copy2test'):\n",
        "            all_variables = tf.global_variables()\n",
        "            train_vars = dict([(v.name, v) for v in all_variables\n",
        "                               if not v.name.startswith('test/')])\n",
        "            test_vars = dict([(v.name, v) for v in all_variables\n",
        "                              if v.name.startswith('test/')])\n",
        "            self._copy_variables_to_test_model_op = tf.tuple([\n",
        "                test_vars['test/' + k].assign(train_vars[k]) for k in train_vars.keys()\n",
        "                if 'test/' + k in test_vars\n",
        "            ])\n",
        "\n",
        "        # Begin testing thread\n",
        "        self._coordinator = tf.train.Coordinator()\n",
        "        self._thread = threading.Thread(target=self.test_job,\n",
        "                                        name='%s_tester' % self.model.identifier)\n",
        "        self._thread.daemon = True\n",
        "        self._thread.start()\n",
        "\n",
        "        # Pick tensors we need to evaluate\n",
        "        all_tensors = dict(self.model.loss_terms['test'], **self.model.metrics['test'])\n",
        "        self._tensors_to_evaluate = dict([(n, t) for n, t in all_tensors.items()])\n",
        "        loss_terms_to_evaluate = dict([(n, t) for n, t in self.model.loss_terms['test'].items()\n",
        "                                       if t in self._tensors_to_evaluate.values()])\n",
        "        metrics_to_evaluate = dict([(n, t) for n, t in self.model.metrics['test'].items()\n",
        "                                    if t in self._tensors_to_evaluate.values()])\n",
        "\n",
        "        # Placeholders for writing summaries at end of test run\n",
        "        self._placeholders = {}\n",
        "        for type_, tensors in (('loss', loss_terms_to_evaluate),\n",
        "                               ('metric', metrics_to_evaluate)):\n",
        "            for name in tensors.keys():\n",
        "                name = '%s/test/%s' % (type_, name)\n",
        "                placeholder = tf.placeholder(dtype=np.float32, name=name + '_placeholder')\n",
        "                self.summary.scalar(name, placeholder)\n",
        "                self._placeholders[name.split('/')[-1]] = placeholder\n",
        "\n",
        "    def copy_model_weights(self):\n",
        "        \"\"\"Copy weights from main model used for training.\n",
        "        This operation should stop-the-world, that is, training should not occur.\n",
        "        \"\"\"\n",
        "        assert self._copy_variables_to_test_model_op is not None\n",
        "        self._tensorflow_session.run(self._copy_variables_to_test_model_op)\n",
        "##Model        logger.debug('Copied over trainable model parameters for testing.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwDNyWwDFYS5"
      },
      "source": [
        "\n",
        "\n",
        "\"\"\"Base model class for Tensorflow-based model construction.\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\"\"\"from .data_source import BaseDataSource\n",
        "from .live_tester import LiveTester\n",
        "from .time_manager import TimeManager\n",
        "from .summary_manager import SummaryManager\n",
        "from .checkpoint_manager import CheckpointManager\"\"\"\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class BaseModel(object):\n",
        "    \"\"\"Base model class for Tensorflow-based model construction.\n",
        "    This class assumes that there exist no other Tensorflow models defined.\n",
        "    That is, any variable that exists in the Python session will be grabbed by the class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 tensorflow_session: tf.Session,\n",
        "                 learning_schedule: List[Dict[str, Any]] = [],\n",
        "                 train_data: Dict[str, BaseDataSource] = {},\n",
        "                 test_data: Dict[str, BaseDataSource] = {},\n",
        "                 test_losses_or_metrics: str = None,\n",
        "                 use_batch_statistics_at_test: bool = True,\n",
        "                 identifier: str = None):\n",
        "        \"\"\"Initialize model with data sources and parameters.\"\"\"\n",
        "        self._tensorflow_session = tensorflow_session\n",
        "        self._train_data = train_data\n",
        "        self._test_data = test_data\n",
        "        self._test_losses_or_metrics = test_losses_or_metrics\n",
        "        self._initialized = False\n",
        "        self.__identifier = identifier\n",
        "\n",
        "        # Extract and keep known prefixes/scopes\n",
        "        self._learning_schedule = learning_schedule\n",
        "        self._known_prefixes = [schedule for schedule in learning_schedule]\n",
        "\n",
        "        # Check consistency of given data sources\n",
        "        train_data_sources = list(train_data.values())\n",
        "        test_data_sources = list(test_data.values())\n",
        "        all_data_sources = train_data_sources + test_data_sources\n",
        "        first_data_source = all_data_sources.pop()\n",
        "        self._batch_size = first_data_source.batch_size\n",
        "        self._data_format = first_data_source.data_format\n",
        "        for data_source in all_data_sources:\n",
        "            if data_source.batch_size != self._batch_size:\n",
        "                raise ValueError(('Data source \"%s\" has anomalous batch size of %d ' +\n",
        "                                  'when detected batch size is %d.') % (data_source.short_name,\n",
        "                                                                        data_source.batch_size,\n",
        "                                                                        self._batch_size))\n",
        "            if data_source.data_format != self._data_format:\n",
        "                raise ValueError(('Data source \"%s\" has anomalous data_format of %s ' +\n",
        "                                  'when detected data_format is %s.') % (data_source.short_name,\n",
        "                                                                         data_source.data_format,\n",
        "                                                                         self._data_format))\n",
        "        self._data_format_longer = ('channels_first' if self._data_format == 'NCHW'\n",
        "                                    else 'channels_last')\n",
        "\n",
        "        # Make output dir\n",
        "        if not os.path.isdir(self.output_path):\n",
        "            os.makedirs(self.output_path)\n",
        "        #self.output_path = dir\n",
        "\n",
        "        # Log messages to file\n",
        "        root_logger = logging.getLogger()\n",
        "        file_handler = logging.FileHandler(self.output_path + '/messages.log')\n",
        "        file_handler.setFormatter(root_logger.handlers[0].formatter)\n",
        "        for handler in root_logger.handlers[1:]:  # all except stdout\n",
        "            root_logger.removeHandler(handler)\n",
        "        root_logger.addHandler(file_handler)\n",
        "\n",
        "        # Register a manager for tf.Summary\n",
        "        self.summary = SummaryManager(self)\n",
        "\n",
        "        # Register a manager for checkpoints\n",
        "        self.checkpoint = CheckpointManager(self)\n",
        "\n",
        "        # Register a manager for timing related operations\n",
        "        self.time = TimeManager(self)\n",
        "\n",
        "        # Prepare for live (concurrent) validation/testing during training, on the CPU\n",
        "        self._enable_live_testing = (len(self._train_data) > 0) and (len(self._test_data) > 0)\n",
        "        self._tester = LiveTester(self, self._test_data, use_batch_statistics_at_test)\n",
        "\n",
        "        # Run-time parameters\n",
        "        with tf.variable_scope('learning_params'):\n",
        "            self.is_training = tf.placeholder(tf.bool)\n",
        "            self.use_batch_statistics = tf.placeholder(tf.bool)\n",
        "            self.learning_rate_multiplier = tf.Variable(1.0, trainable=False, dtype=tf.float32)\n",
        "            self.learning_rate_multiplier_placeholder = tf.placeholder(dtype=tf.float32)\n",
        "            self.assign_learning_rate_multiplier = \\\n",
        "                tf.assign(self.learning_rate_multiplier, self.learning_rate_multiplier_placeholder)\n",
        "\n",
        "        self._build_all_models()\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Explicitly call methods to cleanup any live threads.\"\"\"\n",
        "        train_data_sources = list(self._train_data.values())\n",
        "        test_data_sources = list(self._test_data.values())\n",
        "        all_data_sources = train_data_sources + test_data_sources\n",
        "        for data_source in all_data_sources:\n",
        "            data_source.cleanup()\n",
        "        self._tester.__del__()\n",
        "\n",
        "    __identifier_stem = None\n",
        "\n",
        "    @property\n",
        "    def identifier(self):\n",
        "        \"\"\"Identifier for model based on time.\"\"\"\n",
        "        if self.__identifier is not None:  # If loading from checkpoints or having naming enforced\n",
        "            return self.__identifier\n",
        "        if self.__identifier_stem is None:\n",
        "            self.__identifier_stem = self.__class__.__name__ + '/' + time.strftime('%y%m%d%H%M%S')\n",
        "        return self.__identifier_stem + self._identifier_suffix\n",
        "\n",
        "    @property\n",
        "    def _identifier_suffix(self):\n",
        "        \"\"\"Identifier suffix for model based on data sources and parameters.\"\"\"\n",
        "        return ''\n",
        "\n",
        "    @property\n",
        "    def output_path(self):\n",
        "        \"\"\"Path to store logs and model weights into.\"\"\"\n",
        "        \"\"\"return '%s/%s' % (os.path.abspath(os.path.dirname(__file__) + '/../../outputs'),\n",
        "                          self.identifier)\"\"\"\n",
        "        return '%s/%s' % (dir,\n",
        "                          self.identifier)\n",
        "\n",
        "    def _build_all_models(self):\n",
        "        \"\"\"Build training (GPU/CPU) and testing (CPU) streams.\"\"\"\n",
        "        self.output_tensors = {}\n",
        "        self.loss_terms = {}\n",
        "        self.metrics = {}\n",
        "\n",
        "        def _build_datasource_summaries(data_sources, mode):\n",
        "            \"\"\"Register summary operations for input data from given data sources.\"\"\"\n",
        "            with tf.variable_scope('%s_data' % mode):\n",
        "                for data_source_name, data_source in data_sources.items():\n",
        "                    tensors = data_source.output_tensors\n",
        "                    for key, tensor in tensors.items():\n",
        "                        summary_name = '%s/%s' % (data_source_name, key)\n",
        "                        shape = tensor.shape.as_list()\n",
        "                        num_dims = len(shape)\n",
        "                        if num_dims == 4:  # Image data\n",
        "                            if shape[1] == 1 or shape[1] == 3:\n",
        "                                self.summary.image(summary_name, tensor,\n",
        "                                                   data_format='channels_first')\n",
        "                            elif shape[3] == 1 or shape[3] == 3:\n",
        "                                self.summary.image(summary_name, tensor,\n",
        "                                                   data_format='channels_last')\n",
        "                            # TODO: fix issue with no summary otherwise\n",
        "                        elif num_dims == 2:\n",
        "                            self.summary.histogram(summary_name, tensor)\n",
        "                        else:\n",
        "                            logger.debug('I do not know how to create a summary for %s (%s)' %\n",
        "                                         (summary_name, tensor.shape.as_list()))\n",
        "\n",
        "        def _build_train_or_test(mode):\n",
        "            data_sources = self._train_data if mode == 'train' else self._test_data\n",
        "\n",
        "            # Build model\n",
        "            output_tensors, loss_terms, metrics = self.build_model(data_sources, mode=mode)\n",
        "\n",
        "            # Record important tensors\n",
        "            self.output_tensors[mode] = output_tensors\n",
        "            self.loss_terms[mode] = loss_terms\n",
        "            self.metrics[mode] = metrics\n",
        "\n",
        "            # Create summaries for scalars\n",
        "            if mode == 'train':\n",
        "                for name, loss_term in loss_terms.items():\n",
        "                    self.summary.scalar('loss/%s/%s' % (mode, name), loss_term)\n",
        "                for name, metric in metrics.items():\n",
        "                    self.summary.scalar('metric/%s/%s' % (mode, name), metric)\n",
        "\n",
        "        # Build the main model\n",
        "        if len(self._train_data) > 0:\n",
        "            _build_datasource_summaries(self._train_data, mode='train')\n",
        "            _build_train_or_test(mode='train')\n",
        "            logger.info('Built model.')\n",
        "\n",
        "            # Print no. of parameters and lops\n",
        "            flops = tf.profiler.profile(\n",
        "                options=tf.profiler.ProfileOptionBuilder(\n",
        "                    tf.profiler.ProfileOptionBuilder.float_operation()\n",
        "                ).with_empty_output().build())\n",
        "            logger.info('------------------------------')\n",
        "            logger.info(' Approximate Model Statistics ')\n",
        "            logger.info('------------------------------')\n",
        "            logger.info('FLOPS per input: {:,}'.format(flops.total_float_ops / self._batch_size))\n",
        "            logger.info(\n",
        "                'Trainable Parameters: {:,}'.format(\n",
        "                    np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()])\n",
        "                )\n",
        "            )\n",
        "            logger.info('------------------------------')\n",
        "\n",
        "        # If there are any test data streams, build same model with different scope\n",
        "        # Trainable parameters will be copied at test time\n",
        "        if len(self._test_data) > 0:\n",
        "            _build_datasource_summaries(self._test_data, mode='test')\n",
        "            with tf.variable_scope('test'):\n",
        "                _build_train_or_test(mode='test')\n",
        "            logger.info('Built model for live testing.')\n",
        "\n",
        "        if self._enable_live_testing:\n",
        "            self._tester._post_model_build()  # Create copy ops to be run before every test run\n",
        "\n",
        "    def build_model(self, data_sources: Dict[str, BaseDataSource], mode: str):\n",
        "        \"\"\"Build model.\"\"\"\n",
        "        raise NotImplementedError('BaseModel::build_model is not yet implemented.')\n",
        "\n",
        "    def initialize_if_not(self, training=False):\n",
        "        \"\"\"Initialize variables and begin preprocessing threads.\"\"\"\n",
        "        if self._initialized:\n",
        "            return\n",
        "\n",
        "        # Build supporting operations\n",
        "        with tf.variable_scope('savers'):\n",
        "            self.checkpoint.build_savers()  # Create savers\n",
        "        if training:\n",
        "            with tf.variable_scope('optimize'):\n",
        "                self._build_optimizers()\n",
        "\n",
        "        # Start pre-processing routines\n",
        "        for _, datasource in self._train_data.items():\n",
        "            datasource.create_and_start_threads()\n",
        "\n",
        "        # Initialize all variables\n",
        "        self._tensorflow_session.run(tf.global_variables_initializer())\n",
        "        self._initialized = True\n",
        "\n",
        "    def _build_optimizers(self):\n",
        "        \"\"\"Based on learning schedule, create optimizer instances.\"\"\"\n",
        "        self._optimize_ops = []\n",
        "        all_trainable_variables = tf.trainable_variables()\n",
        "        all_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        all_reg_losses = tf.losses.get_regularization_losses()\n",
        "        for spec in self._learning_schedule:\n",
        "            optimize_ops = []\n",
        "            update_ops = []\n",
        "            loss_terms = spec['loss_terms_to_optimize']\n",
        "            reg_losses = []\n",
        "            assert isinstance(loss_terms, dict)\n",
        "            for loss_term_key, prefixes in loss_terms.items():\n",
        "                assert loss_term_key in self.loss_terms['train'].keys()\n",
        "                variables_to_train = []\n",
        "                for prefix in prefixes:\n",
        "                    variables_to_train += [\n",
        "                        v for v in all_trainable_variables\n",
        "                        if v.name.startswith(prefix)\n",
        "                    ]\n",
        "                    update_ops += [\n",
        "                        o for o in all_update_ops\n",
        "                        if o.name.startswith(prefix)\n",
        "                    ]\n",
        "                    reg_losses += [\n",
        "                        l for l in all_reg_losses\n",
        "                        if l.name.startswith(prefix)\n",
        "                    ]\n",
        "\n",
        "                optimizer_class = tf.train.AdamOptimizer\n",
        "                optimizer = optimizer_class(\n",
        "                    learning_rate=self.learning_rate_multiplier * spec['learning_rate'],\n",
        "                    # beta1=0.9,\n",
        "                    # beta2=0.999,\n",
        "                )\n",
        "                final_loss = self.loss_terms['train'][loss_term_key]\n",
        "                if len(reg_losses) > 0:\n",
        "                    final_loss += tf.reduce_sum(reg_losses)\n",
        "                with tf.control_dependencies(update_ops):\n",
        "                    gradients, variables = zip(*optimizer.compute_gradients(\n",
        "                        loss=final_loss,\n",
        "                        var_list=variables_to_train,\n",
        "                        aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N,\n",
        "                    ))\n",
        "                    # gradients, _ = tf.clip_by_global_norm(gradients, 5.0)  # TODO: generalize\n",
        "                    optimize_op = optimizer.apply_gradients(zip(gradients, variables))\n",
        "                optimize_ops.append(optimize_op)\n",
        "            self._optimize_ops.append(optimize_ops)\n",
        "            logger.info('Built optimizer for: %s' % ', '.join(loss_terms.keys()))\n",
        "\n",
        "    def train_loop_pre(self, current_step):\n",
        "        \"\"\"Run this at beginning of training loop.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def train_loop_post(self, current_step):\n",
        "        \"\"\"Run this at end of training loop.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def train(self, num_epochs=None, num_steps=None):\n",
        "        \"\"\"Train model as requested.\"\"\"\n",
        "        if num_steps is None:\n",
        "            num_entries = np.min([s.num_entries for s in list(self._train_data.values())])\n",
        "            print(\"num_entries\",num_entries)\n",
        "            num_steps = int(num_epochs * num_entries / self._batch_size)\n",
        "            print(\"steps\",num_steps)\n",
        "        self.initialize_if_not(training=True)\n",
        "\n",
        "        try:\n",
        "            initial_step = self.checkpoint.load_all()\n",
        "            current_step = initial_step\n",
        "            for current_step in range(initial_step, num_steps):\n",
        "                print(\"current_step\",current_step)\n",
        "                # Extra operations defined in implementation of this base class\n",
        "                self.train_loop_pre(current_step)\n",
        "\n",
        "                # Select loss terms, optimize operations, and metrics tensors to evaluate\n",
        "                fetches = {}\n",
        "                schedule_id = current_step % len(self._learning_schedule)\n",
        "                schedule = self._learning_schedule[schedule_id]\n",
        "                fetches['optimize_ops'] = self._optimize_ops[schedule_id]\n",
        "                loss_term_keys, _ = zip(*list(schedule['loss_terms_to_optimize'].items()))\n",
        "                fetches['loss_terms'] = [self.loss_terms['train'][k] for k in loss_term_keys]\n",
        "                summary_op = self.summary.get_ops(mode='train')\n",
        "                if len(summary_op) > 0:\n",
        "                    fetches['summaries'] = summary_op\n",
        "\n",
        "                # Run one optimization iteration and retrieve calculated loss values\n",
        "                self.time.start('train_iteration', average_over_last_n_timings=100)\n",
        "                outcome = self._tensorflow_session.run(\n",
        "                    fetches=fetches,\n",
        "                    feed_dict={\n",
        "                        self.is_training: True,\n",
        "                        self.use_batch_statistics: True,\n",
        "                    }\n",
        "                )\n",
        "                self.time.end('train_iteration')\n",
        "\n",
        "                # Print progress\n",
        "                to_print = '%07d> ' % current_step\n",
        "                to_print += ', '.join(['%s = %g' % (k, v)\n",
        "                                       for k, v in zip(loss_term_keys, outcome['loss_terms'])])\n",
        "                self.time.log_every('train_iteration', to_print, seconds=2)\n",
        "\n",
        "                # Trigger copy weights & concurrent testing (if not already running)\n",
        "                if self._enable_live_testing:\n",
        "                    self._tester.trigger_test_if_not_testing(current_step)\n",
        "\n",
        "                # Write summaries\n",
        "                if 'summaries' in outcome:\n",
        "                    self.summary.write_summaries(outcome['summaries'], current_step)\n",
        "\n",
        "                # Save model weights\n",
        "                if self.time.has_been_n_seconds_since_last('save_weights', 600) \\\n",
        "                        and current_step > initial_step:\n",
        "                    self.checkpoint.save_all(current_step)\n",
        "\n",
        "                # Extra operations defined in implementation of this base class\n",
        "                self.train_loop_post(current_step)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            # Handle CTRL-C graciously\n",
        "            self.checkpoint.save_all(current_step)\n",
        "            sys.exit(0)\n",
        "\n",
        "        # Stop live testing, and run final full test\n",
        "        if self._enable_live_testing:\n",
        "            self._tester.do_final_full_test(current_step)\n",
        "\n",
        "        # Save final weights\n",
        "        if current_step > initial_step:\n",
        "            self.checkpoint.save_all(current_step)\n",
        "\n",
        "    def inference_generator(self):\n",
        "        \"\"\"Perform inference on test data and yield a batch of output.\"\"\"\n",
        "        self.initialize_if_not(training=False)\n",
        "        self.checkpoint.load_all()  # Load available weights\n",
        "\n",
        "        # TODO: Make more generic by not picking first source\n",
        "        print(\"self._train_data.values()\",len(self._train_data.values()))\n",
        "        data_source = next(iter(self._train_data.values()))\n",
        "        while True:\n",
        "            fetches = dict(self.output_tensors['train'], **data_source.output_tensors)\n",
        "            start_time = time.time()\n",
        "            outputs = self._tensorflow_session.run(\n",
        "                fetches=fetches,\n",
        "                feed_dict={\n",
        "                    self.is_training: False,\n",
        "                    self.use_batch_statistics: True,\n",
        "                },\n",
        "            )\n",
        "            print(\"generate one output\")\n",
        "            outputs['inference_time'] = 1e3*(time.time() - start_time)\n",
        "            yield outputs\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLVaJod7FYWm"
      },
      "source": [
        "##frames\n",
        "\n",
        "\"\"\"Data source of stream of frames.\"\"\"\n",
        "import bz2\n",
        "import dlib\n",
        "import queue\n",
        "import shutil\n",
        "import threading\n",
        "import time\n",
        "from typing import Tuple\n",
        "import os\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "##from core import BaseDataSource\n",
        "\n",
        "\n",
        "class FramesSource(BaseDataSource):\n",
        "    \"\"\"Preprocessing of stream of frames.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 tensorflow_session: tf.Session,\n",
        "                 batch_size: int,\n",
        "                 eye_image_shape: Tuple[int, int],\n",
        "                 staging: bool=False,\n",
        "                 **kwargs):\n",
        "        \"\"\"Create queues and threads to read and preprocess data.\"\"\"\n",
        "        self._eye_image_shape = eye_image_shape\n",
        "        self._proc_mutex = threading.Lock()\n",
        "        self._read_mutex = threading.Lock()\n",
        "\n",
        "        self._frame_read_queue = queue.Queue(maxsize=1)\n",
        "        self._frame_read_thread = threading.Thread(target=self.frame_read_job, name='frame_read')\n",
        "        self._frame_read_thread.daemon = True\n",
        "        self._frame_read_thread.start()\n",
        "\n",
        "        self._current_index = 0\n",
        "        self._last_frame_index = 0\n",
        "        self._indices = []\n",
        "        self._frames = {}\n",
        "        self._open = True\n",
        "\n",
        "        # Call parent class constructor\n",
        "        super().__init__(tensorflow_session, batch_size=batch_size, num_threads=1,\n",
        "                         fread_queue_capacity=batch_size, preprocess_queue_capacity=batch_size,\n",
        "                         shuffle=False, staging=staging, **kwargs)\n",
        "\n",
        "    _short_name = 'Frames'\n",
        "\n",
        "    @property\n",
        "    def short_name(self):\n",
        "        \"\"\"Short name specifying source.\"\"\"\n",
        "        return self._short_name\n",
        "\n",
        "    def frame_read_job(self):\n",
        "        \"\"\"Read frame from webcam.\"\"\"\n",
        "        generate_frame = self.frame_generator()\n",
        "        while True:\n",
        "            before_frame_read = time.time()\n",
        "            bgr = next(generate_frame)\n",
        "            if bgr is not None:\n",
        "                after_frame_read = time.time()\n",
        "                with self._read_mutex:\n",
        "                    self._frame_read_queue.queue.clear()\n",
        "                    self._frame_read_queue.put_nowait((before_frame_read, bgr, after_frame_read))\n",
        "        self._open = False\n",
        "\n",
        "    def frame_generator(self):\n",
        "        \"\"\"Read frame from webcam.\"\"\"\n",
        "        raise NotImplementedError('Frames::frame_generator not implemented.')\n",
        "\n",
        "    def entry_generator(self, yield_just_one=False):\n",
        "        \"\"\"Generate eye image entries by detecting faces and facial landmarks.\"\"\"\n",
        "        try:\n",
        "            while range(1) if yield_just_one else True:\n",
        "                # Grab frame\n",
        "                with self._proc_mutex:\n",
        "                    before_frame_read, bgr, after_frame_read = self._frame_read_queue.get()\n",
        "                    bgr = cv.flip(bgr, flipCode=1)  # Mirror\n",
        "                    current_index = self._last_frame_index + 1\n",
        "                    self._last_frame_index = current_index\n",
        "\n",
        "                    grey = cv.cvtColor(bgr, cv.COLOR_BGR2GRAY)\n",
        "                    frame = {\n",
        "                        'frame_index': current_index,\n",
        "                        'time': {\n",
        "                            'before_frame_read': before_frame_read,\n",
        "                            'after_frame_read': after_frame_read,\n",
        "                        },\n",
        "                        'bgr': bgr,\n",
        "                        'grey': grey,\n",
        "                    }\n",
        "                    self._frames[current_index] = frame\n",
        "                    self._indices.append(current_index)\n",
        "\n",
        "                    # Keep just a few frames around\n",
        "                    frames_to_keep = 120\n",
        "                    if len(self._indices) > frames_to_keep:\n",
        "                        for index in self._indices[:-frames_to_keep]:\n",
        "                            del self._frames[index]\n",
        "                        self._indices = self._indices[-frames_to_keep:]\n",
        "\n",
        "                # Eye image segmentation pipeline\n",
        "                self.detect_faces(frame)\n",
        "                self.detect_landmarks(frame)\n",
        "                self.calculate_smoothed_landmarks(frame)\n",
        "                self.segment_eyes(frame)\n",
        "                self.update_face_boxes(frame)\n",
        "                frame['time']['after_preprocessing'] = time.time()\n",
        "\n",
        "                for i, eye_dict in enumerate(frame['eyes']):\n",
        "                    yield {\n",
        "                        'frame_index': np.int64(current_index),\n",
        "                        'eye': eye_dict['image'],\n",
        "                        'eye_index': np.uint8(i),\n",
        "                    }\n",
        "\n",
        "        finally:\n",
        "            # Execute any cleanup operations as necessary\n",
        "            pass\n",
        "\n",
        "    def preprocess_entry(self, entry):\n",
        "        \"\"\"Preprocess segmented eye images for use as neural network input.\"\"\"\n",
        "        eye = entry['eye']\n",
        "        eye = cv.equalizeHist(eye)\n",
        "        eye = eye.astype(np.float32)\n",
        "        eye *= 2.0 / 255.0\n",
        "        eye -= 1.0\n",
        "        eye = np.expand_dims(eye, -1 if self.data_format == 'NHWC' else 0)\n",
        "        entry['eye'] = eye\n",
        "        return entry\n",
        "\n",
        "    def detect_faces(self, frame):\n",
        "        \"\"\"Detect all faces in a frame.\"\"\"\n",
        "        frame_index = frame['frame_index']\n",
        "        previous_index = self._indices[self._indices.index(frame_index) - 1]\n",
        "        previous_frame = self._frames[previous_index]\n",
        "        if ('last_face_detect_index' not in previous_frame or\n",
        "                frame['frame_index'] - previous_frame['last_face_detect_index'] > 59):\n",
        "            detector = get_face_detector()\n",
        "            if detector.__class__.__name__ == 'CascadeClassifier':\n",
        "                detections = detector.detectMultiScale(frame['grey'])\n",
        "            else:\n",
        "                detections = detector(cv.resize(frame['grey'], (0, 0), fx=0.5, fy=0.5), 0)\n",
        "            faces = []\n",
        "            for d in detections:\n",
        "                try:\n",
        "                    l, t, r, b = d.rect.left(), d.rect.top(), d.rect.right(), d.rect.bottom()\n",
        "                    l *= 2\n",
        "                    t *= 2\n",
        "                    r *= 2\n",
        "                    b *= 2\n",
        "                    w, h = r - l, b - t\n",
        "                except AttributeError:  # Using OpenCV LBP detector on CPU\n",
        "                    l, t, w, h = d\n",
        "                faces.append((l, t, w, h))\n",
        "            faces.sort(key=lambda bbox: bbox[0])\n",
        "            frame['faces'] = faces\n",
        "            frame['last_face_detect_index'] = frame['frame_index']\n",
        "\n",
        "            # Clear previous known landmarks. This is to disable smoothing when new face detect\n",
        "            # occurs. This allows for recovery of drifted detections.\n",
        "            previous_frame['landmarks'] = []\n",
        "        else:\n",
        "            frame['faces'] = previous_frame['faces']\n",
        "            frame['last_face_detect_index'] = previous_frame['last_face_detect_index']\n",
        "\n",
        "    def detect_landmarks(self, frame):\n",
        "        \"\"\"Detect 5-point facial landmarks for faces in frame.\"\"\"\n",
        "        predictor = get_landmarks_predictor()\n",
        "        landmarks = []\n",
        "        for face in frame['faces']:\n",
        "            l, t, w, h = face\n",
        "            rectangle = dlib.rectangle(left=int(l), top=int(t), right=int(l+w), bottom=int(t+h))\n",
        "            landmarks_dlib = predictor(frame['grey'], rectangle)\n",
        "\n",
        "            def tuple_from_dlib_shape(index):\n",
        "                p = landmarks_dlib.part(index)\n",
        "                return (p.x, p.y)\n",
        "\n",
        "            num_landmarks = landmarks_dlib.num_parts\n",
        "            landmarks.append(np.array([tuple_from_dlib_shape(i) for i in range(num_landmarks)]))\n",
        "        frame['landmarks'] = landmarks\n",
        "\n",
        "    _smoothing_window_size = 10\n",
        "    _smoothing_coefficient_decay = 0.5\n",
        "    _smoothing_coefficients = None\n",
        "\n",
        "    def calculate_smoothed_landmarks(self, frame):\n",
        "        \"\"\"If there are previous landmark detections, try to smooth current prediction.\"\"\"\n",
        "        # Cache coefficients based on defined sliding window size\n",
        "        if self._smoothing_coefficients is None:\n",
        "            coefficients = np.power(self._smoothing_coefficient_decay,\n",
        "                                    list(reversed(list(range(self._smoothing_window_size)))))\n",
        "            coefficients /= np.sum(coefficients)\n",
        "            self._smoothing_coefficients = coefficients.reshape(-1, 1)\n",
        "\n",
        "        # Get a window of frames\n",
        "        current_index = self._indices.index(frame['frame_index'])\n",
        "        a = current_index - self._smoothing_window_size + 1\n",
        "        if a < 0:\n",
        "            \"\"\"If slice extends before last known frame.\"\"\"\n",
        "            return\n",
        "        window_indices = self._indices[a:current_index + 1]\n",
        "        window_frames = [self._frames[idx] for idx in window_indices]\n",
        "        window_num_landmark_entries = np.array([len(f['landmarks']) for f in window_frames])\n",
        "        if np.any(window_num_landmark_entries == 0):\n",
        "            \"\"\"Any frame has zero faces detected.\"\"\"\n",
        "            return\n",
        "        if not np.all(window_num_landmark_entries == window_num_landmark_entries[0]):\n",
        "            \"\"\"Not the same number of faces detected in entire window.\"\"\"\n",
        "            return\n",
        "\n",
        "        # Apply coefficients to landmarks in window\n",
        "        window_landmarks = np.asarray([f['landmarks'] for f in window_frames])\n",
        "        frame['smoothed_landmarks'] = np.sum(\n",
        "            np.multiply(window_landmarks.reshape(self._smoothing_window_size, -1),\n",
        "                        self._smoothing_coefficients),\n",
        "            axis=0,\n",
        "        ).reshape(window_num_landmark_entries[-1], -1, 2)\n",
        "\n",
        "    def segment_eyes(self, frame):\n",
        "        \"\"\"From found landmarks in previous steps, segment eye image.\"\"\"\n",
        "        eyes = []\n",
        "\n",
        "        # Final output dimensions\n",
        "        oh, ow = self._eye_image_shape\n",
        "\n",
        "        # Select which landmarks (raw/smoothed) to use\n",
        "        frame_landmarks = (frame['smoothed_landmarks'] if 'smoothed_landmarks' in frame\n",
        "                           else frame['landmarks'])\n",
        "\n",
        "        for face, landmarks in zip(frame['faces'], frame_landmarks):\n",
        "            # Segment eyes\n",
        "            # for corner1, corner2, is_left in [(36, 39, True), (42, 45, False)]:\n",
        "            for corner1, corner2, is_left in [(2, 3, True), (0, 1, False)]:\n",
        "                x1, y1 = landmarks[corner1, :]\n",
        "                x2, y2 = landmarks[corner2, :]\n",
        "                eye_width = 1.5 * np.linalg.norm(landmarks[corner1, :] - landmarks[corner2, :])\n",
        "                if eye_width == 0.0:\n",
        "                    continue\n",
        "                cx, cy = 0.5 * (x1 + x2), 0.5 * (y1 + y2)\n",
        "\n",
        "                # Centre image on middle of eye\n",
        "                translate_mat = np.asmatrix(np.eye(3))\n",
        "                translate_mat[:2, 2] = [[-cx], [-cy]]\n",
        "                inv_translate_mat = np.asmatrix(np.eye(3))\n",
        "                inv_translate_mat[:2, 2] = -translate_mat[:2, 2]\n",
        "\n",
        "                # Rotate to be upright\n",
        "                roll = 0.0 if x1 == x2 else np.arctan((y2 - y1) / (x2 - x1))\n",
        "                rotate_mat = np.asmatrix(np.eye(3))\n",
        "                cos = np.cos(-roll)\n",
        "                sin = np.sin(-roll)\n",
        "                rotate_mat[0, 0] = cos\n",
        "                rotate_mat[0, 1] = -sin\n",
        "                rotate_mat[1, 0] = sin\n",
        "                rotate_mat[1, 1] = cos\n",
        "                inv_rotate_mat = rotate_mat.T\n",
        "\n",
        "                # Scale\n",
        "                scale = ow / eye_width\n",
        "                scale_mat = np.asmatrix(np.eye(3))\n",
        "                scale_mat[0, 0] = scale_mat[1, 1] = scale\n",
        "                inv_scale = 1.0 / scale\n",
        "                inv_scale_mat = np.asmatrix(np.eye(3))\n",
        "                inv_scale_mat[0, 0] = inv_scale_mat[1, 1] = inv_scale\n",
        "\n",
        "                # Centre image\n",
        "                centre_mat = np.asmatrix(np.eye(3))\n",
        "                centre_mat[:2, 2] = [[0.5 * ow], [0.5 * oh]]\n",
        "                inv_centre_mat = np.asmatrix(np.eye(3))\n",
        "                inv_centre_mat[:2, 2] = -centre_mat[:2, 2]\n",
        "\n",
        "                # Get rotated and scaled, and segmented image\n",
        "                transform_mat = centre_mat * scale_mat * rotate_mat * translate_mat\n",
        "                inv_transform_mat = (inv_translate_mat * inv_rotate_mat * inv_scale_mat *\n",
        "                                     inv_centre_mat)\n",
        "                eye_image = cv.warpAffine(frame['grey'], transform_mat[:2, :], (ow, oh))\n",
        "                if is_left:\n",
        "                    eye_image = np.fliplr(eye_image)\n",
        "                eyes.append({\n",
        "                    'image': eye_image,\n",
        "                    'inv_landmarks_transform_mat': inv_transform_mat,\n",
        "                    'side': 'left' if is_left else 'right',\n",
        "                })\n",
        "        frame['eyes'] = eyes\n",
        "\n",
        "    def update_face_boxes(self, frame):\n",
        "        \"\"\"Update face bounding box based on detected landmarks.\"\"\"\n",
        "        frame_landmarks = (frame['smoothed_landmarks'] if 'smoothed_landmarks' in frame\n",
        "                           else frame['landmarks'])\n",
        "        for i, (face, landmarks) in enumerate(zip(frame['faces'], frame_landmarks)):\n",
        "            x_min, y_min = np.amin(landmarks, axis=0)\n",
        "            x_max, y_max = np.amax(landmarks, axis=0)\n",
        "            x_mid, y_mid = 0.5 * (x_max + x_min), 0.5 * (y_max + y_min)\n",
        "            w, h = x_max - x_min, y_max - y_min\n",
        "            new_w = 2.2 * max(h, w)\n",
        "            half_w = 0.5 * new_w\n",
        "            frame['faces'][i] = (int(x_mid - half_w), int(y_mid - half_w), int(new_w), int(new_w))\n",
        "\n",
        "            # x1, y1 = landmarks[0, :]\n",
        "            # x2, y2 = landmarks[3, :]\n",
        "            # face_width = 2.5 * np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
        "            # if face_width == 0.0:\n",
        "            #     continue\n",
        "            #\n",
        "            # cx, cy = landmarks[4, :]\n",
        "            # roll = 0.0 if x1 == x2 else np.arctan((y2 - y1) / (x2 - x1))\n",
        "            #\n",
        "            # hdx = 0.5 * face_width * (2. - np.abs(np.cos(roll)))\n",
        "            # hdy = 0.5 * face_width * (1. + np.abs(np.sin(roll)))\n",
        "            # print(np.degrees(roll), face_width, hdx, hdy)\n",
        "            # frame['faces'][i] = (int(cx - hdx), int(cy - hdy), int(2*hdx), int(2*hdy))\n",
        "\n",
        "_face_detector = None\n",
        "_landmarks_predictor = None\n",
        "\n",
        "\n",
        "def _get_dlib_data_file(dat_name):\n",
        "    dat_dir = os.path.relpath('%s/../3rdparty' % dir)\n",
        "    dat_path = '%s/%s' % (dat_dir, dat_name)\n",
        "    if not os.path.isdir(dat_dir):\n",
        "        os.mkdir(dat_dir)\n",
        "\n",
        "    # Download trained shape detector\n",
        "    if not os.path.isfile(dat_path):\n",
        "        with urlopen('http://dlib.net/files/%s.bz2' % dat_name) as response:\n",
        "            with bz2.BZ2File(response) as bzf, open(dat_path, 'wb') as f:\n",
        "                shutil.copyfileobj(bzf, f)\n",
        "\n",
        "    return dat_path\n",
        "\n",
        "\n",
        "def _get_opencv_xml(xml_name):\n",
        "    xml_dir = os.path.relpath('%s/../3rdparty' % dir ) ##os.path.basename(__file__)\n",
        "    xml_path = '%s/%s' % (xml_dir, xml_name)\n",
        "    if not os.path.isdir(xml_dir):\n",
        "        os.mkdir(xml_dir)\n",
        "\n",
        "    # Download trained shape detector\n",
        "    if not os.path.isfile(xml_path):\n",
        "        url_stem = 'https://raw.githubusercontent.com/opencv/opencv/master/data/lbpcascades'\n",
        "        with urlopen('%s/%s' % (url_stem, xml_name)) as response:\n",
        "            with open(xml_path, 'wb') as f:\n",
        "                shutil.copyfileobj(response, f)\n",
        "\n",
        "    return xml_path\n",
        "\n",
        "\n",
        "def get_face_detector():\n",
        "    \"\"\"Get a singleton dlib face detector.\"\"\"\n",
        "    global _face_detector\n",
        "    if not _face_detector:\n",
        "        try:\n",
        "            dat_path = _get_dlib_data_file('mmod_human_face_detector.dat')\n",
        "            _face_detector = dlib.cnn_face_detection_model_v1(dat_path)\n",
        "        except:\n",
        "            xml_path = _get_opencv_xml('lbpcascade_frontalface_improved.xml')\n",
        "            _face_detector = cv.CascadeClassifier(xml_path)\n",
        "    return _face_detector\n",
        "\n",
        "\n",
        "def get_landmarks_predictor():\n",
        "    \"\"\"Get a singleton dlib face landmark predictor.\"\"\"\n",
        "    global _landmarks_predictor\n",
        "    if not _landmarks_predictor:\n",
        "        dat_path = _get_dlib_data_file('shape_predictor_5_face_landmarks.dat')\n",
        "        # dat_path = _get_dlib_data_file('shape_predictor_68_face_landmarks.dat')\n",
        "        _landmarks_predictor = dlib.shape_predictor(dat_path)\n",
        "    return _landmarks_predictor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt85hXu8HHde"
      },
      "source": [
        "##gaze\n",
        "\n",
        "\n",
        "\"\"\"Utility methods for gaze angle and error calculations.\"\"\"\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def pitchyaw_to_vector(pitchyaws):\n",
        "    r\"\"\"Convert given yaw (:math:`\\theta`) and pitch (:math:`\\phi`) angles to unit gaze vectors.\n",
        "    Args:\n",
        "        pitchyaws (:obj:`numpy.array`): yaw and pitch angles :math:`(n\\times 2)` in radians.\n",
        "    Returns:\n",
        "        :obj:`numpy.array` of shape :math:`(n\\times 3)` with 3D vectors per row.\n",
        "    \"\"\"\n",
        "    n = pitchyaws.shape[0]\n",
        "    sin = np.sin(pitchyaws)\n",
        "    cos = np.cos(pitchyaws)\n",
        "    out = np.empty((n, 3))\n",
        "    out[:, 0] = np.multiply(cos[:, 0], sin[:, 1])\n",
        "    out[:, 1] = sin[:, 0]\n",
        "    out[:, 2] = np.multiply(cos[:, 0], cos[:, 1])\n",
        "    return out\n",
        "\n",
        "\n",
        "def vector_to_pitchyaw(vectors):\n",
        "    r\"\"\"Convert given gaze vectors to yaw (:math:`\\theta`) and pitch (:math:`\\phi`) angles.\n",
        "    Args:\n",
        "        vectors (:obj:`numpy.array`): gaze vectors in 3D :math:`(n\\times 3)`.\n",
        "    Returns:\n",
        "        :obj:`numpy.array` of shape :math:`(n\\times 2)` with values in radians.\n",
        "    \"\"\"\n",
        "    n = vectors.shape[0]\n",
        "    out = np.empty((n, 2))\n",
        "    vectors = np.divide(vectors, np.linalg.norm(vectors, axis=1).reshape(n, 1))\n",
        "    out[:, 0] = np.arcsin(vectors[:, 1])  # theta\n",
        "    out[:, 1] = np.arctan2(vectors[:, 0], vectors[:, 2])  # phi\n",
        "    return out\n",
        "\n",
        "radians_to_degrees = 180.0 / np.pi\n",
        "\n",
        "\n",
        "def angular_error(a, b):\n",
        "    \"\"\"Calculate angular error (via cosine similarity).\"\"\"\n",
        "    a = pitchyaw_to_vector(a) if a.shape[1] == 2 else a\n",
        "    b = pitchyaw_to_vector(b) if b.shape[1] == 2 else b\n",
        "\n",
        "    ab = np.sum(np.multiply(a, b), axis=1)\n",
        "    a_norm = np.linalg.norm(a, axis=1)\n",
        "    b_norm = np.linalg.norm(b, axis=1)\n",
        "\n",
        "    # Avoid zero-values (to avoid NaNs)\n",
        "    a_norm = np.clip(a_norm, a_min=1e-7, a_max=None)\n",
        "    b_norm = np.clip(b_norm, a_min=1e-7, a_max=None)\n",
        "\n",
        "    similarity = np.divide(ab, np.multiply(a_norm, b_norm))\n",
        "\n",
        "    return np.arccos(similarity) * radians_to_degrees\n",
        "\n",
        "\n",
        "def mean_angular_error(a, b):\n",
        "    \"\"\"Calculate mean angular error (via cosine similarity).\"\"\"\n",
        "    return np.mean(angular_error(a, b))\n",
        "\n",
        "\n",
        "def tensorflow_angular_error_from_pitchyaw(y_true, y_pred):\n",
        "    \"\"\"Tensorflow method to calculate angular loss from head angles.\"\"\"\n",
        "    def angles_to_unit_vectors(y):\n",
        "        sin = tf.sin(y)\n",
        "        cos = tf.cos(y)\n",
        "        return tf.stack([\n",
        "            tf.multiply(cos[:, 0], sin[:, 1]),\n",
        "            sin[:, 0],\n",
        "            tf.multiply(cos[:, 0], cos[:, 1]),\n",
        "        ], axis=1)\n",
        "\n",
        "    with tf.name_scope('mean_angular_error'):\n",
        "        v_true = angles_to_unit_vectors(y_true)\n",
        "        v_pred = angles_to_unit_vectors(y_pred)\n",
        "        return tensorflow_angular_error_from_vector(v_true, v_pred)\n",
        "\n",
        "\n",
        "def tensorflow_angular_error_from_vector(v_true, v_pred):\n",
        "    \"\"\"Tensorflow method to calculate angular loss from 3D vector.\"\"\"\n",
        "    with tf.name_scope('mean_angular_error'):\n",
        "        v_true_norm = tf.sqrt(tf.reduce_sum(tf.square(v_true), axis=1))\n",
        "        v_pred_norm = tf.sqrt(tf.reduce_sum(tf.square(v_pred), axis=1))\n",
        "\n",
        "        sim = tf.div(tf.reduce_sum(tf.multiply(v_true, v_pred), axis=1),\n",
        "                     tf.multiply(v_true_norm, v_pred_norm))\n",
        "\n",
        "        # Floating point precision can cause sim values to be slightly outside of\n",
        "        # [-1, 1] so we clip values\n",
        "        sim = tf.clip_by_value(sim, -1.0 + 1e-6, 1.0 - 1e-6)\n",
        "\n",
        "        ang = tf.scalar_mul(radians_to_degrees, tf.acos(sim))\n",
        "        return tf.reduce_mean(ang)\n",
        "\n",
        "\n",
        "def draw_gaze(image_in, eye_pos, pitchyaw, length=40.0, thickness=3, color=(0, 255,0)):\n",
        "    \"\"\"Draw gaze angle on given image with a given eye positions.\"\"\"\n",
        "    image_out = image_in\n",
        "    if len(image_out.shape) == 2 or image_out.shape[2] == 1:\n",
        "        image_out = cv.cvtColor(image_out, cv.COLOR_GRAY2BGR)\n",
        "    dx = -length * np.sin(pitchyaw[1])\n",
        "    dy = -length * np.sin(pitchyaw[0])\n",
        "    cv.arrowedLine(image_out, tuple(np.round(eye_pos).astype(np.int32)),\n",
        "                   tuple(np.round([eye_pos[0] + dx, eye_pos[1] + dy]).astype(int)), color,\n",
        "                   thickness, cv.LINE_AA, tipLength=0.2)\n",
        "    return image_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pea1qUNJHHgu"
      },
      "source": [
        "##gazemap \n",
        "\n",
        "\"\"\"Utility methods for generating gazemaps.\"\"\"\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "height_to_eyeball_radius_ratio = 1.1\n",
        "eyeball_radius_to_iris_diameter_ratio = 1.0\n",
        "\n",
        "def from_gaze2d(gaze, output_size, scale=1.0):\n",
        "    \"\"\"Generate a normalized pictorial representation of 3D gaze direction.\"\"\"\n",
        "    gazemaps = []\n",
        "    oh, ow = np.round(scale * np.asarray(output_size)).astype(np.int32)\n",
        "    oh_2 = int(np.round(0.5 * oh))\n",
        "    ow_2 = int(np.round(0.5 * ow))\n",
        "    r = int(height_to_eyeball_radius_ratio * oh_2)\n",
        "    theta, phi = gaze\n",
        "    theta = -theta\n",
        "    sin_theta = np.sin(theta)\n",
        "    cos_theta = np.cos(theta)\n",
        "    sin_phi = np.sin(phi)\n",
        "    cos_phi = np.cos(phi)\n",
        "\n",
        "    # Draw iris\n",
        "    eyeball_radius = int(height_to_eyeball_radius_ratio * oh_2)\n",
        "    iris_radius_angle = np.arcsin(0.5 * eyeball_radius_to_iris_diameter_ratio)\n",
        "    iris_radius = eyeball_radius_to_iris_diameter_ratio * eyeball_radius\n",
        "    iris_distance = float(eyeball_radius) * np.cos(iris_radius_angle)\n",
        "    iris_offset = np.asarray([\n",
        "        -iris_distance * sin_phi * cos_theta,\n",
        "        iris_distance * sin_theta,\n",
        "    ])\n",
        "    iris_centre = np.asarray([ow_2, oh_2]) + iris_offset\n",
        "    angle = np.degrees(np.arctan2(iris_offset[1], iris_offset[0]))\n",
        "    ellipse_max = eyeball_radius_to_iris_diameter_ratio * iris_radius\n",
        "    ellipse_min = np.abs(ellipse_max * cos_phi * cos_theta)\n",
        "    gazemap = np.zeros((oh, ow), dtype=np.float32)\n",
        "    gazemap = cv.ellipse(gazemap, box=(iris_centre, (ellipse_min, ellipse_max), angle),\n",
        "                         color=1.0, thickness=-1, lineType=cv.LINE_AA)\n",
        "    gazemaps.append(gazemap)\n",
        "\n",
        "    # Draw eyeball\n",
        "    gazemap = np.zeros((oh, ow), dtype=np.float32)\n",
        "    gazemap = cv.circle(gazemap, (ow_2, oh_2), r, color=1, thickness=-1)\n",
        "    gazemaps.append(gazemap)\n",
        "\n",
        "    return np.asarray(gazemaps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfrS-cYpHHkJ"
      },
      "source": [
        "\"\"\"Utility methods for generating and visualizing heatmaps.\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def gaussian_2d(shape, centre, sigma=1.0):\n",
        "    \"\"\"Generate heatmap with single 2D gaussian.\"\"\"\n",
        "    xs = np.arange(0.5, shape[1] + 0.5, step=1.0, dtype=np.float32)\n",
        "    ys = np.expand_dims(np.arange(0.5, shape[0] + 0.5, step=1.0, dtype=np.float32), -1)\n",
        "    alpha = -0.5 / (sigma**2)\n",
        "    heatmap = np.exp(alpha * ((xs - centre[0])**2 + (ys - centre[1])**2))\n",
        "    return heatmap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0ghHPlTGvUh"
      },
      "source": [
        "##hdf5\n",
        "\n",
        "\n",
        "\"\"\"HDF5 data source for gaze estimation.\"\"\"\n",
        "from threading import Lock\n",
        "from typing import List\n",
        "\n",
        "import cv2 as cv\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\"\"\"from core import BaseDataSource\n",
        "import util.gazemap\"\"\"\n",
        "\n",
        "\n",
        "class HDF5Source(BaseDataSource):\n",
        "    \"\"\"HDF5 data loading class (using h5py).\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 tensorflow_session: tf.Session,\n",
        "                 batch_size: int,\n",
        "                 keys_to_use: List[str],\n",
        "                 hdf_path: str,\n",
        "                 testing=False,\n",
        "                 eye_image_shape=(36, 60),\n",
        "                 **kwargs):\n",
        "        \"\"\"Create queues and threads to read and preprocess data from specified keys.\"\"\"\n",
        "        hdf5 = h5py.File(hdf_path, 'r')\n",
        "        self._short_name = 'HDF:%s' % '/'.join(hdf_path.split('/')[-2:])\n",
        "        if testing:\n",
        "            self._short_name += ':test'\n",
        "\n",
        "        # Cache some parameters\n",
        "        self._eye_image_shape = eye_image_shape\n",
        "\n",
        "        # Create global index over all specified keys\n",
        "        self._index_to_key = {}\n",
        "        index_counter = 0\n",
        "        for key in keys_to_use:\n",
        "            n = hdf5[key]['eye'].shape[0]\n",
        "            for i in range(n):\n",
        "                self._index_to_key[index_counter] = (key, i)\n",
        "                index_counter += 1\n",
        "        self._num_entries = index_counter\n",
        "\n",
        "        self._hdf5 = hdf5\n",
        "        self._mutex = Lock()\n",
        "        self._current_index = 0\n",
        "        super().__init__(tensorflow_session, batch_size=batch_size, testing=testing, **kwargs)\n",
        "\n",
        "        # Set index to 0 again as base class constructor called HDF5Source::entry_generator once to\n",
        "        # get preprocessed sample.\n",
        "        self._current_index = 0\n",
        "\n",
        "    @property\n",
        "    def num_entries(self):\n",
        "        \"\"\"Number of entries in this data source.\"\"\"\n",
        "        return self._num_entries\n",
        "\n",
        "    @property\n",
        "    def short_name(self):\n",
        "        \"\"\"Short name specifying source HDF5.\"\"\"\n",
        "        return self._short_name\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Close HDF5 file before running base class cleanup routine.\"\"\"\n",
        "        super().cleanup()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset index.\"\"\"\n",
        "        with self._mutex:\n",
        "            super().reset()\n",
        "            self._current_index = 0\n",
        "\n",
        "    def entry_generator(self, yield_just_one=False):\n",
        "        \"\"\"Read entry from HDF5.\"\"\"\n",
        "        try:\n",
        "            while range(1) if yield_just_one else True:\n",
        "                with self._mutex:\n",
        "                    if self._current_index >= self.num_entries:\n",
        "                        if self.testing:\n",
        "                            break\n",
        "                        else:\n",
        "                            self._current_index = 0\n",
        "                    current_index = self._current_index\n",
        "                    self._current_index += 1\n",
        "\n",
        "                key, index = self._index_to_key[current_index]\n",
        "                data = self._hdf5[key]\n",
        "                entry = {}\n",
        "                for name in ('eye', 'gaze', 'head'):\n",
        "                    if name in data:\n",
        "                        entry[name] = data[name][index, :]\n",
        "                yield entry\n",
        "        finally:\n",
        "            # Execute any cleanup operations as necessary\n",
        "            pass\n",
        "\n",
        "    def preprocess_entry(self, entry):\n",
        "        \"\"\"Resize eye image and normalize intensities.\"\"\"\n",
        "        oh, ow = self._eye_image_shape\n",
        "        eye = entry['eye']\n",
        "        eye = cv.resize(eye, (ow, oh))\n",
        "        eye = eye.astype(np.float32)\n",
        "        eye *= 2.0 / 255.0\n",
        "        eye -= 1.0\n",
        "        eye = np.expand_dims(eye, axis=0 if self.data_format == 'NCHW' else -1)\n",
        "        entry['eye'] = eye\n",
        "\n",
        "        entry['gazemaps'] = from_gaze2d( ##util.gazemap.from_gaze2d(\n",
        "            entry['gaze'], output_size=(oh, ow), scale=0.5,\n",
        "        ).astype(np.float32)\n",
        "        if self.data_format == 'NHWC':\n",
        "            np.transpose(entry['gazemaps'], (1, 2, 0))\n",
        "\n",
        "        # Ensure all values in an entry are 4-byte floating point numbers\n",
        "        for key, value in entry.items():\n",
        "            entry[key] = value.astype(np.float32)\n",
        "\n",
        "        return entry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4-AJ_zqGvc-"
      },
      "source": [
        "##unityeyes\n",
        "\n",
        "\"\"\"UnityEyes data source for gaze estimation.\"\"\"\n",
        "import os\n",
        "from threading import Lock\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import ujson\n",
        "\n",
        "\"\"\"from core import BaseDataSource\n",
        "import util.gaze\n",
        "import util.heatmap\"\"\"\n",
        "\n",
        "\n",
        "class UnityEyes(BaseDataSource):\n",
        "    \"\"\"UnityEyes data loading class.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 tensorflow_session: tf.Session,\n",
        "                 batch_size: int,\n",
        "                 unityeyes_path: str,\n",
        "                 testing=False,\n",
        "                 generate_heatmaps=False,\n",
        "                 eye_image_shape=(36, 60),\n",
        "                 heatmaps_scale=1.0,\n",
        "                 **kwargs):\n",
        "        \"\"\"Create queues and threads to read and preprocess data.\"\"\"\n",
        "        self._short_name = 'UnityEyes'\n",
        "        if testing:\n",
        "            self._short_name += ':test'\n",
        "\n",
        "        # Cache some parameters\n",
        "        self._eye_image_shape = eye_image_shape\n",
        "        self._heatmaps_scale = heatmaps_scale\n",
        "\n",
        "        # Create global index over all specified keys\n",
        "        self._images_path = unityeyes_path\n",
        "        self._file_stems = sorted([p[:-5] for p in os.listdir(unityeyes_path)\n",
        "                                   if p.endswith('.json')])\n",
        "        self._num_entries = len(self._file_stems)\n",
        "        self._num_entries = self._num_entries//10\n",
        "\n",
        "        self._mutex = Lock()\n",
        "        self._current_index = 0\n",
        "\n",
        "        # Define bounds for noise values for different augmentation types\n",
        "        self._difficulty = 0.0\n",
        "        self._augmentation_ranges = {  # (easy, hard)\n",
        "            'translation': (2.0, 10.0),\n",
        "            'rotation': (0.1, 2.0),\n",
        "            'intensity': (0.5, 20.0),\n",
        "            'blur': (0.1, 1.0),\n",
        "            'scale': (0.01, 0.1),\n",
        "            'rescale': (1.0, 0.2),\n",
        "            'num_line': (0.0, 2.0),\n",
        "            'heatmap_sigma': (5.0, 2.5),\n",
        "        }\n",
        "        self._generate_heatmaps = generate_heatmaps\n",
        "\n",
        "        # Call parent class constructor\n",
        "        super().__init__(tensorflow_session, batch_size=batch_size, testing=testing, **kwargs)\n",
        "\n",
        "    @property\n",
        "    def num_entries(self):\n",
        "        \"\"\"Number of entries in this data source.\"\"\"\n",
        "        return self._num_entries\n",
        "\n",
        "    @property\n",
        "    def short_name(self):\n",
        "        \"\"\"Short name specifying source UnityEyes.\"\"\"\n",
        "        return self._short_name\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset index.\"\"\"\n",
        "        with self._mutex:\n",
        "            super().reset()\n",
        "            self._current_index = 0\n",
        "\n",
        "    def entry_generator(self, yield_just_one=False):\n",
        "        \"\"\"Read entry from UnityEyes.\"\"\"\n",
        "        try:\n",
        "            while range(1) if yield_just_one else True:\n",
        "                with self._mutex:\n",
        "                    if self._current_index >= self.num_entries:\n",
        "                        if self.testing:\n",
        "                            break\n",
        "                        else:\n",
        "                            self._current_index = 0\n",
        "                    current_index = self._current_index\n",
        "                    self._current_index += 1\n",
        "\n",
        "                file_stem = self._file_stems[current_index]\n",
        "                jpg_path = '%s/%s.jpg' % (self._images_path, file_stem)\n",
        "                json_path = '%s/%s.json' % (self._images_path, file_stem)\n",
        "                if not os.path.isfile(jpg_path) or not os.path.isfile(json_path):\n",
        "                    continue\n",
        "                with open(json_path, 'r') as f:\n",
        "                    json_data = ujson.load(f)\n",
        "                entry = {\n",
        "                    'full_image': cv.imread(jpg_path, cv.IMREAD_GRAYSCALE),\n",
        "                    'json_data': json_data,\n",
        "                }\n",
        "                assert entry['full_image'] is not None\n",
        "                yield entry\n",
        "        finally:\n",
        "            # Execute any cleanup operations as necessary\n",
        "            pass\n",
        "\n",
        "    def set_difficulty(self, difficulty):\n",
        "        \"\"\"Set difficulty of training data.\"\"\"\n",
        "        assert isinstance(difficulty, float)\n",
        "        assert 0.0 <= difficulty <= 1.0\n",
        "        self._difficulty = difficulty\n",
        "\n",
        "    def set_augmentation_range(self, augmentation_type, easy_value, hard_value):\n",
        "        \"\"\"Set 'range' for a known augmentation type.\"\"\"\n",
        "        assert isinstance(augmentation_type, str)\n",
        "        assert augmentation_type in self._augmentation_ranges\n",
        "        assert isinstance(easy_value, float) or isinstance(easy_value, int)\n",
        "        assert isinstance(hard_value, float) or isinstance(hard_value, int)\n",
        "        self._augmentation_ranges[augmentation_type] = (easy_value, hard_value)\n",
        "\n",
        "    def preprocess_entry(self, entry):\n",
        "        \"\"\"Use annotations to segment eyes and calculate gaze direction.\"\"\"\n",
        "        full_image = entry['full_image']\n",
        "        json_data = entry['json_data']\n",
        "        del entry['full_image']\n",
        "        del entry['json_data']\n",
        "\n",
        "        ih, iw = full_image.shape\n",
        "        iw_2, ih_2 = 0.5 * iw, 0.5 * ih\n",
        "        oh, ow = self._eye_image_shape\n",
        "\n",
        "        def process_coords(coords_list):\n",
        "            coords = [eval(l) for l in coords_list]\n",
        "            return np.array([(x, ih-y, z) for (x, y, z) in coords])\n",
        "        interior_landmarks = process_coords(json_data['interior_margin_2d'])\n",
        "        caruncle_landmarks = process_coords(json_data['caruncle_2d'])\n",
        "        iris_landmarks = process_coords(json_data['iris_2d'])\n",
        "\n",
        "        random_multipliers = []\n",
        "\n",
        "        def value_from_type(augmentation_type):\n",
        "            # Scale to be in range\n",
        "            easy_value, hard_value = self._augmentation_ranges[augmentation_type]\n",
        "            value = (hard_value - easy_value) * self._difficulty + easy_value\n",
        "            value = (np.clip(value, easy_value, hard_value)\n",
        "                     if easy_value < hard_value\n",
        "                     else np.clip(value, hard_value, easy_value))\n",
        "            return value\n",
        "\n",
        "        def noisy_value_from_type(augmentation_type):\n",
        "            # Get normal distributed random value\n",
        "            if len(random_multipliers) == 0:\n",
        "                random_multipliers.extend(\n",
        "                        list(np.random.normal(size=(len(self._augmentation_ranges),))))\n",
        "            return random_multipliers.pop() * value_from_type(augmentation_type)\n",
        "\n",
        "        # Only select almost frontal images\n",
        "        h_pitch, h_yaw, _ = eval(json_data['head_pose'])\n",
        "        if h_pitch > 180.0:  # Need to correct pitch\n",
        "            h_pitch -= 360.0\n",
        "        h_yaw -= 180.0  # Need to correct yaw\n",
        "        if abs(h_pitch) > 20 or abs(h_yaw) > 20:\n",
        "            return None\n",
        "\n",
        "        # Prepare to segment eye image\n",
        "        left_corner = np.mean(caruncle_landmarks[:, :2], axis=0)\n",
        "        right_corner = interior_landmarks[8, :2]\n",
        "        eye_width = 1.5 * abs(left_corner[0] - right_corner[0])\n",
        "        eye_middle = np.mean([np.amin(interior_landmarks[:, :2], axis=0),\n",
        "                              np.amax(interior_landmarks[:, :2], axis=0)], axis=0)\n",
        "\n",
        "        # Centre axes to eyeball centre\n",
        "        translate_mat = np.asmatrix(np.eye(3))\n",
        "        translate_mat[:2, 2] = [[-iw_2], [-ih_2]]\n",
        "\n",
        "        # Rotate eye image if requested\n",
        "        rotate_mat = np.asmatrix(np.eye(3))\n",
        "        rotation_noise = noisy_value_from_type('rotation')\n",
        "        if rotation_noise > 0:\n",
        "            rotate_angle = np.radians(rotation_noise)\n",
        "            cos_rotate = np.cos(rotate_angle)\n",
        "            sin_rotate = np.sin(rotate_angle)\n",
        "            rotate_mat[0, 0] = cos_rotate\n",
        "            rotate_mat[0, 1] = -sin_rotate\n",
        "            rotate_mat[1, 0] = sin_rotate\n",
        "            rotate_mat[1, 1] = cos_rotate\n",
        "\n",
        "        # Scale image to fit output dimensions (with a little bit of noise)\n",
        "        scale_mat = np.asmatrix(np.eye(3))\n",
        "        scale = 1. + noisy_value_from_type('scale')\n",
        "        scale_inv = 1. / scale\n",
        "        np.fill_diagonal(scale_mat, ow / eye_width * scale)\n",
        "        original_eyeball_radius = 71.7593\n",
        "        eyeball_radius = original_eyeball_radius * scale_mat[0, 0]  # See: https://goo.gl/ZnXgDE\n",
        "        entry['radius'] = np.float32(eyeball_radius)\n",
        "\n",
        "        # Re-centre eye image such that eye fits (based on determined `eye_middle`)\n",
        "        recentre_mat = np.asmatrix(np.eye(3))\n",
        "        recentre_mat[0, 2] = iw/2 - eye_middle[0] + 0.5 * eye_width * scale_inv\n",
        "        recentre_mat[1, 2] = ih/2 - eye_middle[1] + 0.5 * oh / ow * eye_width * scale_inv\n",
        "        recentre_mat[0, 2] += noisy_value_from_type('translation')  # x\n",
        "        recentre_mat[1, 2] += noisy_value_from_type('translation')  # y\n",
        "\n",
        "        # Apply transforms\n",
        "        transform_mat = recentre_mat * scale_mat * rotate_mat * translate_mat\n",
        "        eye = cv.warpAffine(full_image, transform_mat[:2, :3], (ow, oh))\n",
        "\n",
        "        # Convert look vector to gaze direction in polar angles\n",
        "        look_vec = np.array(eval(json_data['eye_details']['look_vec']))[:3]\n",
        "        look_vec[0] = -look_vec[0]\n",
        "        original_gaze = vector_to_pitchyaw(look_vec.reshape((1, 3))).flatten() ##util.gaze.\n",
        "        look_vec = rotate_mat * look_vec.reshape(3, 1)\n",
        "        gaze = vector_to_pitchyaw(look_vec.reshape((1, 3))).flatten() ##util.gaze.\n",
        "        if gaze[1] > 0.0:\n",
        "            gaze[1] = np.pi - gaze[1]\n",
        "        elif gaze[1] < 0.0:\n",
        "            gaze[1] = -(np.pi + gaze[1])\n",
        "        entry['gaze'] = gaze.astype(np.float32)\n",
        "\n",
        "        # Draw line randomly\n",
        "        num_line_noise = int(np.round(noisy_value_from_type('num_line')))\n",
        "        if num_line_noise > 0:\n",
        "            line_rand_nums = np.random.rand(5 * num_line_noise)\n",
        "            for i in range(num_line_noise):\n",
        "                j = 5 * i\n",
        "                lx0, ly0 = int(ow * line_rand_nums[j]), oh\n",
        "                lx1, ly1 = ow, int(oh * line_rand_nums[j + 1])\n",
        "                direction = line_rand_nums[j + 2]\n",
        "                if direction < 0.25:\n",
        "                    lx1 = ly0 = 0\n",
        "                elif direction < 0.5:\n",
        "                    lx1 = 0\n",
        "                elif direction < 0.75:\n",
        "                    ly0 = 0\n",
        "                line_colour = int(255 * line_rand_nums[j + 3])\n",
        "                eye = cv.line(eye, (lx0, ly0), (lx1, ly1),\n",
        "                              color=(line_colour, line_colour, line_colour),\n",
        "                              thickness=max(1, int(6*line_rand_nums[j + 4])),\n",
        "                              lineType=cv.LINE_AA)\n",
        "\n",
        "        # Rescale image if required\n",
        "        rescale_max = value_from_type('rescale')\n",
        "        if rescale_max < 1.0:\n",
        "            rescale_noise = np.random.uniform(low=rescale_max, high=1.0)\n",
        "            interpolation = cv.INTER_CUBIC\n",
        "            eye = cv.resize(eye, dsize=(0, 0), fx=rescale_noise, fy=rescale_noise,\n",
        "                            interpolation=interpolation)\n",
        "            eye = cv.equalizeHist(eye)\n",
        "            eye = cv.resize(eye, dsize=(ow, oh), interpolation=interpolation)\n",
        "\n",
        "        # Add rgb noise to eye image\n",
        "        intensity_noise = int(value_from_type('intensity'))\n",
        "        if intensity_noise > 0:\n",
        "            eye = eye.astype(np.int16)\n",
        "            eye += np.random.randint(low=-intensity_noise, high=intensity_noise,\n",
        "                                     size=eye.shape, dtype=np.int16)\n",
        "            cv.normalize(eye, eye, alpha=0, beta=255, norm_type=cv.NORM_MINMAX)\n",
        "            eye = eye.astype(np.uint8)\n",
        "\n",
        "        # Add blur to eye image\n",
        "        blur_noise = noisy_value_from_type('blur')\n",
        "        if blur_noise > 0:\n",
        "            eye = cv.GaussianBlur(eye, (7, 7), 0.5 + np.abs(blur_noise))\n",
        "\n",
        "        # Histogram equalization and preprocessing for NN\n",
        "        eye = cv.equalizeHist(eye)\n",
        "        eye = eye.astype(np.float32)\n",
        "        eye *= 2.0 / 255.0\n",
        "        eye -= 1.0\n",
        "        eye = np.expand_dims(eye, -1 if self.data_format == 'NHWC' else 0)\n",
        "        entry['eye'] = eye\n",
        "\n",
        "        # Select and transform landmark coordinates\n",
        "        iris_centre = np.asarray([\n",
        "            iw_2 + original_eyeball_radius * -np.cos(original_gaze[0]) * np.sin(original_gaze[1]),\n",
        "            ih_2 + original_eyeball_radius * -np.sin(original_gaze[0]),\n",
        "        ])\n",
        "        landmarks = np.concatenate([interior_landmarks[::2, :2],  # 8\n",
        "                                    iris_landmarks[::4, :2],  # 8\n",
        "                                    iris_centre.reshape((1, 2)),\n",
        "                                    [[iw_2, ih_2]],  # Eyeball centre\n",
        "                                    ])  # 18 in total\n",
        "        landmarks = np.asmatrix(np.pad(landmarks, ((0, 0), (0, 1)), 'constant',\n",
        "                                       constant_values=1))\n",
        "        landmarks = np.asarray(landmarks * transform_mat.T)\n",
        "        landmarks = landmarks[:, :2]  # We only need x, y\n",
        "        entry['landmarks'] = landmarks.astype(np.float32)\n",
        "\n",
        "        # Generate heatmaps if necessary\n",
        "        if self._generate_heatmaps:\n",
        "            # Should be half-scale (compared to eye image). # util.heatmap.gaussian_2d\n",
        "            entry['heatmaps'] = np.asarray([\n",
        "                    gaussian_2d(\n",
        "                    shape=(self._heatmaps_scale*oh, self._heatmaps_scale*ow),\n",
        "                    centre=self._heatmaps_scale*landmark,\n",
        "                    sigma=value_from_type('heatmap_sigma'),\n",
        "                )\n",
        "                for landmark in entry['landmarks']\n",
        "            ]).astype(np.float32)\n",
        "            if self.data_format == 'NHWC':\n",
        "                entry['heatmaps'] = np.transpose(entry['heatmaps'], (1, 2, 0))\n",
        "\n",
        "        return entry\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8EH7R4sGvgV"
      },
      "source": [
        "##video\n",
        "\n",
        "\"\"\"Video (file) data source for gaze estimation.\"\"\"\n",
        "import os\n",
        "import time\n",
        "\n",
        "import cv2 as cv\n",
        "\n",
        "##from .frames import FramesSource\n",
        "\n",
        "\n",
        "class Video(FramesSource):\n",
        "    \"\"\"Video frame grabbing and preprocessing.\"\"\"\n",
        "\n",
        "    def __init__(self, video_path, **kwargs):\n",
        "        \"\"\"Create queues and threads to read and preprocess data.\"\"\"\n",
        "        self._short_name = 'Video'\n",
        "\n",
        "        assert os.path.isfile(video_path)\n",
        "        self._video_path = video_path\n",
        "        self._capture = cv.VideoCapture(video_path)\n",
        "\n",
        "        # Call parent class constructor\n",
        "        super().__init__(staging=False, **kwargs)\n",
        "\n",
        "    def frame_generator(self):\n",
        "        \"\"\"Read frame from webcam.\"\"\"\n",
        "        last_frame = None\n",
        "        while True:\n",
        "            ret, frame = self._capture.read()\n",
        "            if ret:\n",
        "                yield frame\n",
        "                last_frame = frame\n",
        "            else:\n",
        "                yield last_frame\n",
        "                break\n",
        "\n",
        "    def frame_read_job(self):\n",
        "        \"\"\"Read frame from video (without skipping).\"\"\"\n",
        "        generate_frame = self.frame_generator()\n",
        "        while True:\n",
        "            before_frame_read = time.time()\n",
        "            try:\n",
        "                bgr = next(generate_frame)\n",
        "            except StopIteration:\n",
        "                break\n",
        "            if bgr is not None:\n",
        "                after_frame_read = time.time()\n",
        "                with self._read_mutex:\n",
        "                    self._frame_read_queue.put((before_frame_read, bgr, after_frame_read))\n",
        "\n",
        "        print('Video \"%s\" closed.' % self._video_path)\n",
        "        self._open = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdUqtarfCAoU"
      },
      "source": [
        "##train Deep Pictorial Gaze  model\n",
        "\n",
        "\"\"\"Deep Pictorial Gaze architecture.\"\"\"\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "\n",
        "\"\"\"from core import BaseDataSource, BaseModel\n",
        "from datasources import UnityEyes\n",
        "import util.gaze\"\"\"\n",
        "\n",
        "\n",
        "class DPG(BaseModel):\n",
        "    \"\"\"Deep Pictorial Gaze architecture as introduced in [Park et al. ECCV'18].\"\"\"\n",
        "\n",
        "    def __init__(self, tensorflow_session=None, first_layer_stride=2, num_modules=3,\n",
        "                 num_feature_maps=32, growth_rate=8, extra_tags=[], **kwargs):\n",
        "        \"\"\"Specify DPG-specific parameters.\"\"\"\n",
        "        self._hg_first_layer_stride = first_layer_stride\n",
        "        self._hg_num_modules = num_modules\n",
        "        self._hg_num_feature_maps= num_feature_maps\n",
        "        self._dn_growth_rate = growth_rate\n",
        "        self._extra_tags = extra_tags\n",
        "\n",
        "        # Call parent class constructor\n",
        "        super().__init__(tensorflow_session, **kwargs)\n",
        "\n",
        "    _hg_first_layer_stride = 2\n",
        "    _hg_num_modules = 3\n",
        "    _hg_num_feature_maps = 32\n",
        "    _hg_num_residual_blocks = 1\n",
        "    _hg_num_gazemaps = 2\n",
        "\n",
        "    _dn_growth_rate = 8\n",
        "    _dn_compression_factor = 0.5\n",
        "    _dn_num_layers_per_block = (4, 4, 4, 4)\n",
        "    _dn_num_dense_blocks = len(_dn_num_layers_per_block)\n",
        "\n",
        "    @property\n",
        "    def identifier(self):\n",
        "        \"\"\"Identifier for model based on data sources and parameters.\"\"\"\n",
        "        first_data_source = next(iter(self._train_data.values()))\n",
        "        input_tensors = first_data_source.output_tensors\n",
        "        if self._data_format == 'NHWC':\n",
        "            _, eh, ew, _ = input_tensors['eye'].shape.as_list()\n",
        "        else:\n",
        "            _, _, eh, ew = input_tensors['eye'].shape.as_list()\n",
        "        return 'DPG_i%dx%d_f%dx%d_n%d_m%d_k%d_%s' % (\n",
        "            ew, eh,\n",
        "            int(ew / self._hg_first_layer_stride),\n",
        "            int(eh / self._hg_first_layer_stride),\n",
        "            self._hg_num_feature_maps, self._hg_num_modules,\n",
        "            self._dn_growth_rate,\n",
        "            '-'.join(self._extra_tags) if len(self._extra_tags) > 0 else '',\n",
        "        )\n",
        "\n",
        "    def train_loop_pre(self, current_step):\n",
        "        \"\"\"Run this at beginning of training loop.\"\"\"\n",
        "        # Step learning rate decay\n",
        "        multiplier = np.power(0.1, int(current_step / 10000))\n",
        "        self._tensorflow_session.run(self.assign_learning_rate_multiplier, feed_dict={\n",
        "            self.learning_rate_multiplier_placeholder: multiplier,\n",
        "        })\n",
        "\n",
        "    _column_of_ones = None\n",
        "    _column_of_zeros = None\n",
        "\n",
        "    def _augment_training_images(self, images, mode):\n",
        "        if mode == 'test':\n",
        "            return images\n",
        "        with tf.variable_scope('augment'):\n",
        "            if self._data_format == 'NCHW':\n",
        "                images = tf.transpose(images, perm=[0, 2, 3, 1])\n",
        "            n, h, w, _ = images.shape.as_list()\n",
        "            if self._column_of_ones is None:\n",
        "                self._column_of_ones = tf.ones((n, 1))\n",
        "                self._column_of_zeros = tf.zeros((n, 1))\n",
        "            transforms = tf.concat([\n",
        "                self._column_of_ones,\n",
        "                self._column_of_zeros,\n",
        "                tf.truncated_normal((n, 1), mean=0, stddev=.05*w),\n",
        "                self._column_of_zeros,\n",
        "                self._column_of_ones,\n",
        "                tf.truncated_normal((n, 1), mean=0, stddev=.05*h),\n",
        "                self._column_of_zeros,\n",
        "                self._column_of_zeros,\n",
        "            ], axis=1)\n",
        "            images = tf.contrib.image.transform(images, transforms, interpolation='BILINEAR')\n",
        "            if self._data_format == 'NCHW':\n",
        "                images = tf.transpose(images, perm=[0, 3, 1, 2])\n",
        "        return images\n",
        "\n",
        "    def build_model(self, data_sources: Dict[str, BaseDataSource], mode: str):\n",
        "        \"\"\"Build model.\"\"\"\n",
        "        data_source = next(iter(data_sources.values()))\n",
        "        input_tensors = data_source.output_tensors\n",
        "        x = input_tensors['eye']\n",
        "        y1 = input_tensors['gazemaps'] if 'gazemaps' in input_tensors else None\n",
        "        y2 = input_tensors['gaze'] if 'gaze' in input_tensors else None\n",
        "\n",
        "        with tf.variable_scope('input_data'):\n",
        "            # self.summary.feature_maps('eyes', x, data_format=self._data_format_longer)\n",
        "            if y1 is not None:\n",
        "                self.summary.feature_maps('gazemaps', y1, data_format=self._data_format_longer)\n",
        "\n",
        "        outputs = {}\n",
        "        loss_terms = {}\n",
        "        metrics = {}\n",
        "\n",
        "        # Lightly augment training data\n",
        "        x = self._augment_training_images(x, mode)\n",
        "\n",
        "        with tf.variable_scope('hourglass'):\n",
        "            # Prepare for Hourglass by downscaling via conv\n",
        "            with tf.variable_scope('pre'):\n",
        "                n = self._hg_num_feature_maps\n",
        "                x = self._apply_conv(x, num_features=n, kernel_size=7,\n",
        "                                     stride=self._hg_first_layer_stride)\n",
        "                x = tf.nn.relu(self._apply_bn(x))\n",
        "                x = self._build_residual_block(x, n, 2*n, name='res1')\n",
        "                x = self._build_residual_block(x, 2*n, n, name='res2')\n",
        "\n",
        "            # Hourglass blocks\n",
        "            x_prev = x\n",
        "            gmap = None\n",
        "            for i in range(self._hg_num_modules):\n",
        "                with tf.variable_scope('hg_%d' % (i + 1)):\n",
        "                    x = self._build_hourglass(x, steps_to_go=4, num_features=self._hg_num_feature_maps)\n",
        "                    x, gmap = self._build_hourglass_after(\n",
        "                        x_prev, x, do_merge=(i < (self._hg_num_modules - 1)),\n",
        "                    )\n",
        "                    x_prev = x\n",
        "            if y1 is not None:\n",
        "                # Cross-entropy loss\n",
        "                metrics['gazemaps_ce'] = -tf.reduce_mean(tf.reduce_sum(\n",
        "                    y1 * tf.log(tf.clip_by_value(gmap, 1e-10, 1.0)),  # avoid NaN\n",
        "                    axis=[1, 2, 3]))\n",
        "                # metrics['gazemaps_ce'] = tf.losses.softmax_cross_entropy(\n",
        "                #     tf.reshape(y1, (self._batch_size, -1)),\n",
        "                #     tf.reshape(gmap, (self._batch_size, -1)),\n",
        "                #     loss_collection=None,\n",
        "                # )\n",
        "            x = gmap\n",
        "            outputs['gazemaps'] = gmap\n",
        "            self.summary.feature_maps('bottleneck', gmap, data_format=self._data_format_longer)\n",
        "\n",
        "        with tf.variable_scope('densenet'):\n",
        "            # DenseNet blocks to regress to gaze\n",
        "            for i in range(self._dn_num_dense_blocks):\n",
        "                with tf.variable_scope('block%d' % (i + 1)):\n",
        "                    x = self._apply_dense_block(x,\n",
        "                                                num_layers=self._dn_num_layers_per_block[i])\n",
        "                    if i == self._dn_num_dense_blocks - 1:\n",
        "                        break\n",
        "                with tf.variable_scope('trans%d' % (i + 1)):\n",
        "                    x = self._apply_transition_layer(x)\n",
        "\n",
        "            # Global average pooling\n",
        "            with tf.variable_scope('post'):\n",
        "                x = self._apply_bn(x)\n",
        "                x = tf.nn.relu(x)\n",
        "                if self._data_format == 'NCHW':\n",
        "                    x = tf.reduce_mean(x, axis=[2, 3])\n",
        "                else:\n",
        "                    x = tf.reduce_mean(x, axis=[1, 2])\n",
        "                x = tf.contrib.layers.flatten(x)\n",
        "\n",
        "            # Output layer\n",
        "            with tf.variable_scope('output'):\n",
        "                x = self._apply_fc(x, 2)\n",
        "                outputs['gaze'] = x\n",
        "                if y2 is not None:\n",
        "                    metrics['gaze_mse'] = tf.reduce_mean(tf.squared_difference(x, y2))\n",
        "                    metrics['gaze_ang'] = tensorflow_angular_error_from_pitchyaw(y2, x) #util.gaze.tensorflow_angular_error_from_pitchyaw(y2, x)\n",
        "\n",
        "        # Combine two loss terms\n",
        "        if y1 is not None and y2 is not None:\n",
        "            loss_terms['combined_loss'] = 1e-5*metrics['gazemaps_ce'] + metrics['gaze_mse']\n",
        "\n",
        "        # Define outputs\n",
        "        return outputs, loss_terms, metrics\n",
        "\n",
        "    def _apply_conv(self, tensor, num_features, kernel_size=3, stride=1):\n",
        "        return tf.layers.conv2d(\n",
        "            tensor,\n",
        "            num_features,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=stride,\n",
        "            padding='SAME',\n",
        "            kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            data_format=self._data_format_longer,\n",
        "            name='conv',\n",
        "        )\n",
        "\n",
        "    def _apply_fc(self, tensor, num_outputs):\n",
        "        return tf.layers.dense(\n",
        "            tensor,\n",
        "            num_outputs,\n",
        "            use_bias=True,\n",
        "            kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            name='fc',\n",
        "        )\n",
        "\n",
        "    def _apply_pool(self, tensor, kernel_size=3, stride=2):\n",
        "        tensor = tf.layers.max_pooling2d(\n",
        "            tensor,\n",
        "            pool_size=kernel_size,\n",
        "            strides=stride,\n",
        "            padding='SAME',\n",
        "            data_format=self._data_format_longer,\n",
        "            name='pool',\n",
        "        )\n",
        "        return tensor\n",
        "\n",
        "    def _apply_bn(self, tensor):\n",
        "        return tf.contrib.layers.batch_norm(\n",
        "            tensor,\n",
        "            scale=True,\n",
        "            center=True,\n",
        "            is_training=self.use_batch_statistics,\n",
        "            trainable=True,\n",
        "            data_format=self._data_format,\n",
        "            updates_collections=None,\n",
        "        )\n",
        "\n",
        "    def _build_residual_block(self, x, num_in, num_out, name='res_block'):\n",
        "        with tf.variable_scope(name):\n",
        "            half_num_out = max(int(num_out/2), 1)\n",
        "            c = x\n",
        "            with tf.variable_scope('conv1'):\n",
        "                c = tf.nn.relu(self._apply_bn(c))\n",
        "                c = self._apply_conv(c, num_features=half_num_out, kernel_size=1, stride=1)\n",
        "            with tf.variable_scope('conv2'):\n",
        "                c = tf.nn.relu(self._apply_bn(c))\n",
        "                c = self._apply_conv(c, num_features=half_num_out, kernel_size=3, stride=1)\n",
        "            with tf.variable_scope('conv3'):\n",
        "                c = tf.nn.relu(self._apply_bn(c))\n",
        "                c = self._apply_conv(c, num_features=num_out, kernel_size=1, stride=1)\n",
        "            with tf.variable_scope('skip'):\n",
        "                if num_in == num_out:\n",
        "                    s = tf.identity(x)\n",
        "                else:\n",
        "                    s = self._apply_conv(x, num_features=num_out, kernel_size=1, stride=1)\n",
        "            x = c + s\n",
        "        return x\n",
        "\n",
        "    def _build_hourglass(self, x, steps_to_go, num_features, depth=1):\n",
        "        with tf.variable_scope('depth%d' % depth):\n",
        "            # Upper branch\n",
        "            up1 = x\n",
        "            for i in range(self._hg_num_residual_blocks):\n",
        "                up1 = self._build_residual_block(up1, num_features, num_features,\n",
        "                                                 name='up1_%d' % (i + 1))\n",
        "            # Lower branch\n",
        "            low1 = self._apply_pool(x, kernel_size=2, stride=2)\n",
        "            for i in range(self._hg_num_residual_blocks):\n",
        "                low1 = self._build_residual_block(low1, num_features, num_features,\n",
        "                                                  name='low1_%d' % (i + 1))\n",
        "            # Recursive\n",
        "            low2 = None\n",
        "            if steps_to_go > 1:\n",
        "                low2 = self._build_hourglass(low1, steps_to_go - 1, num_features, depth=depth+1)\n",
        "            else:\n",
        "                low2 = low1\n",
        "                for i in range(self._hg_num_residual_blocks):\n",
        "                    low2 = self._build_residual_block(low2, num_features, num_features,\n",
        "                                                      name='low2_%d' % (i + 1))\n",
        "            # Additional residual blocks\n",
        "            low3 = low2\n",
        "            for i in range(self._hg_num_residual_blocks):\n",
        "                low3 = self._build_residual_block(low3, num_features, num_features,\n",
        "                                                  name='low3_%d' % (i + 1))\n",
        "            # Upsample\n",
        "            if self._data_format == 'NCHW':  # convert to NHWC\n",
        "                low3 = tf.transpose(low3, (0, 2, 3, 1))\n",
        "            up2 = tf.image.resize_bilinear(\n",
        "                    low3,\n",
        "                    up1.shape[1:3] if self._data_format == 'NHWC' else up1.shape[2:4],\n",
        "                    align_corners=True,\n",
        "                  )\n",
        "            if self._data_format == 'NCHW':  # convert back from NHWC\n",
        "                up2 = tf.transpose(up2, (0, 3, 1, 2))\n",
        "\n",
        "        return up1 + up2\n",
        "\n",
        "    def _build_hourglass_after(self, x_prev, x_now, do_merge=True):\n",
        "        with tf.variable_scope('after'):\n",
        "            for j in range(self._hg_num_residual_blocks):\n",
        "                x_now = self._build_residual_block(x_now, self._hg_num_feature_maps,\n",
        "                                                   self._hg_num_feature_maps,\n",
        "                                                   name='after_hg_%d' % (j + 1))\n",
        "            x_now = self._apply_conv(x_now, self._hg_num_feature_maps, kernel_size=1, stride=1)\n",
        "            x_now = self._apply_bn(x_now)\n",
        "            x_now = tf.nn.relu(x_now)\n",
        "\n",
        "            with tf.variable_scope('gmap'):\n",
        "                gmap = self._apply_conv(x_now, self._hg_num_gazemaps, kernel_size=1, stride=1)\n",
        "\n",
        "        x_next = x_now\n",
        "        if do_merge:\n",
        "            with tf.variable_scope('merge'):\n",
        "                with tf.variable_scope('gmap'):\n",
        "                    x_gmaps = self._apply_conv(gmap, self._hg_num_feature_maps, kernel_size=1, stride=1)\n",
        "                with tf.variable_scope('x'):\n",
        "                    x_now = self._apply_conv(x_now, self._hg_num_feature_maps, kernel_size=1, stride=1)\n",
        "                x_next += x_prev + x_gmaps\n",
        "\n",
        "        # Perform softmax on gazemaps\n",
        "        if self._data_format == 'NCHW':\n",
        "            n, c, h, w = gmap.shape.as_list()\n",
        "            gmap = tf.reshape(gmap, (n, -1))\n",
        "            gmap = tf.nn.softmax(gmap)\n",
        "            gmap = tf.reshape(gmap, (n, c, h, w))\n",
        "        else:\n",
        "            n, h, w, c = gmap.shape.as_list()\n",
        "            gmap = tf.transpose(gmap, perm=[0, 3, 1, 2])\n",
        "            gmap = tf.reshape(gmap, (n, -1))\n",
        "            gmap = tf.nn.softmax(gmap)\n",
        "            gmap = tf.reshape(gmap, (n, c, h, w))\n",
        "            gmap = tf.transpose(gmap, perm=[0, 2, 3, 1])\n",
        "        return x_next, gmap\n",
        "\n",
        "    def _apply_dense_block(self, x, num_layers):\n",
        "        assert isinstance(num_layers, int) and num_layers > 0\n",
        "        c_index = 1 if self._data_format == 'NCHW' else 3\n",
        "        x_prev = x\n",
        "        for i in range(num_layers):\n",
        "            with tf.variable_scope('layer%d' % (i + 1)):\n",
        "                n = x.shape.as_list()[c_index]\n",
        "                with tf.variable_scope('bottleneck'):\n",
        "                    x = self._apply_composite_function(x,\n",
        "                                                       num_features=min(n, 4*self._dn_growth_rate),\n",
        "                                                       kernel_size=1)\n",
        "                with tf.variable_scope('composite'):\n",
        "                    x = self._apply_composite_function(x, num_features=self._dn_growth_rate,\n",
        "                                                       kernel_size=3)\n",
        "                if self._data_format == 'NCHW':\n",
        "                    x = tf.concat([x, x_prev], axis=1)\n",
        "                else:\n",
        "                    x = tf.concat([x, x_prev], axis=-1)\n",
        "                x_prev = x\n",
        "        return x\n",
        "\n",
        "    def _apply_transition_layer(self, x):\n",
        "        c_index = 1 if self._data_format == 'NCHW' else 3\n",
        "        x = self._apply_composite_function(\n",
        "            x, num_features=int(self._dn_compression_factor * x.shape.as_list()[c_index]),\n",
        "            kernel_size=1)\n",
        "        x = tf.layers.average_pooling2d(x, pool_size=2, strides=2, padding='valid',\n",
        "                                        data_format=self._data_format_longer)\n",
        "        return x\n",
        "\n",
        "    def _apply_composite_function(self, x, num_features=_dn_growth_rate, kernel_size=3):\n",
        "        x = self._apply_bn(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self._apply_conv(x, num_features=num_features, kernel_size=kernel_size, stride=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_MPCuuiCArR"
      },
      "source": [
        "\n",
        "\n",
        "import argparse\n",
        "\n",
        "import coloredlogs\n",
        "import tensorflow as tf\n",
        "\n",
        "def main():##if __name__ == '__main__':\n",
        "\n",
        "    # Set global log level\n",
        "    \"\"\"parser = argparse.ArgumentParser(description='Train the Deep Pictorial Gaze model.')\n",
        "    parser.add_argument('-v', type=str, help='logging level', default='info',\n",
        "                        choices=['debug', 'info', 'warning', 'error', 'critical'])\n",
        "    args = parser.parse_args()\"\"\"\n",
        "    coloredlogs.install(\n",
        "        datefmt='%d/%m %H:%M',\n",
        "        fmt='%(asctime)s %(levelname)s %(message)s',\n",
        "        level=\"info\".upper() # args.v.upper(),\n",
        "    )\n",
        "\n",
        "    for i in range(0, 5):\n",
        "        # Specify which people to train on, and which to test on\n",
        "        person_id = 'p%02d' % i\n",
        "        other_person_ids = ['p%02d' % j for j in range(5) if i != j]\n",
        "\n",
        "        # Initialize Tensorflow session\n",
        "        tf.reset_default_graph()\n",
        "        tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "        gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as session:\n",
        "\n",
        "            # Declare some parameters\n",
        "            batch_size = 32\n",
        "\n",
        "            # Define training data source\n",
        "            #from datasources import HDF5Source\n",
        "\n",
        "            # Define model\n",
        "            #from models import DPG\n",
        "            model = DPG(\n",
        "                session,\n",
        "                learning_schedule=[\n",
        "                    {\n",
        "                        'loss_terms_to_optimize': {\n",
        "                            'combined_loss': ['hourglass', 'densenet'],\n",
        "                        },\n",
        "                        'metrics': ['gaze_mse', 'gaze_ang'],\n",
        "                        'learning_rate': 0.0001, ##0.0002\n",
        "                    },\n",
        "                ],\n",
        "                extra_tags=[person_id],\n",
        "\n",
        "                # Data sources for training (and testing).\n",
        "                train_data={\n",
        "                    'mpi': HDF5Source(\n",
        "                        session,\n",
        "                        data_format='NCHW',\n",
        "                        batch_size=batch_size,\n",
        "                        keys_to_use=['train/' + s for s in other_person_ids],\n",
        "                        hdf_path='/content/MPIIGaze.h5', ##'../datasets/MPIIGaze.h5',\n",
        "                        eye_image_shape=(90, 150),\n",
        "                        testing=False,\n",
        "                        min_after_dequeue=30000,\n",
        "                        staging=True,\n",
        "                        shuffle=True,\n",
        "                    ),\n",
        "                },\n",
        "                test_data={\n",
        "                    'mpi': HDF5Source(\n",
        "                        session,\n",
        "                        data_format='NCHW',\n",
        "                        batch_size=batch_size,\n",
        "                        keys_to_use=['test/' + person_id],\n",
        "                        hdf_path='/content/MPIIGaze.h5', ## '../datasets/MPIIGaze.h5',\n",
        "                        eye_image_shape=(90, 150),\n",
        "                        testing=True,\n",
        "                    ),\n",
        "                },\n",
        "            )\n",
        "\n",
        "            # Train this model for a set number of epochs\n",
        "            model.train(\n",
        "                num_epochs=20, ##20,\n",
        "            )\n",
        "\n",
        "            model.__del__()\n",
        "            session.close()\n",
        "            del session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42fj1ZgiAxcn"
      },
      "source": [
        "#main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf3XqLuYKXC2"
      },
      "source": [
        "\"\"\"Main script for gaze direction inference from webcam feed.\"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import queue\n",
        "import threading\n",
        "import time\n",
        "\n",
        "import coloredlogs\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "##from datasources import Video, Webcam\n",
        "##from models import ELG\n",
        "##import util.gaze\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "def mainTestModelDPG():\n",
        "    # Set global log level\n",
        "    parser = argparse.ArgumentParser(description='Demonstration of landmarks localization.')\n",
        "    parser.add_argument('-v', type=str, help='logging level', default='info',\n",
        "                        choices=['debug', 'info', 'warning', 'error', 'critical'])\n",
        "    parser.add_argument('--from_video', type=str, help='Use this video path instead of webcam')\n",
        "    parser.add_argument('--record_video', type=str, help='Output path of video of demonstration.')\n",
        "    parser.add_argument('--fullscreen', action='store_true')\n",
        "    parser.add_argument('--headless', action='store_true')\n",
        "\n",
        "    parser.add_argument('--fps', type=int, default=60, help='Desired sampling rate of webcam')\n",
        "    parser.add_argument('--camera_id', type=int, default=0, help='ID of webcam to use')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    coloredlogs.install(\n",
        "        datefmt='%d/%m %H:%M',\n",
        "        fmt='%(asctime)s %(levelname)s %(message)s',\n",
        "        level=args.v.upper(),\n",
        "    )\n",
        "\n",
        "    fps = 60\n",
        "    record_video = dir + \"/outputVideo/\"\n",
        "    from_video = dir + \"/inputVideo/\"\n",
        "\n",
        "\n",
        "    # Check if GPU is available\n",
        "    from tensorflow.python.client import device_lib\n",
        "    session_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "    gpu_available = False\n",
        "    try:\n",
        "        gpus = [d for d in device_lib.list_local_devices(config=session_config)\n",
        "                if d.device_type == 'GPU']\n",
        "        gpu_available = len(gpus) > 0\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Initialize Tensorflow session\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "    with tf.Session(config=session_config) as session:\n",
        "\n",
        "        # Declare some parameters\n",
        "        batch_size = 2\n",
        "\n",
        "        # Define webcam stream data source\n",
        "        # Change data_format='NHWC' if not using CUDA\n",
        "        if from_video:\n",
        "            assert os.path.isfile(from_video)\n",
        "            data_source = Video(from_video,\n",
        "                                tensorflow_session=session, batch_size=batch_size,\n",
        "                                data_format='NCHW' if gpu_available else 'NHWC',\n",
        "                                eye_image_shape=(108, 180))\n",
        "        \"\"\"else:\n",
        "            data_source = Webcam(tensorflow_session=session, batch_size=batch_size,\n",
        "                                 camera_id=args.camera_id, fps=args.fps,\n",
        "                                 data_format='NCHW' if gpu_available else 'NHWC',\n",
        "                                 eye_image_shape=(36, 60))\"\"\"\n",
        "\n",
        "        # Define model\n",
        "        if args.from_video:\n",
        "            model = ELG(\n",
        "                session, train_data={'videostream': data_source},\n",
        "                first_layer_stride=3,\n",
        "                num_modules=3,\n",
        "                num_feature_maps=64,\n",
        "                learning_schedule=[\n",
        "                    {\n",
        "                        'loss_terms_to_optimize': {'dummy': ['hourglass', 'radius']},\n",
        "                    },\n",
        "                ],\n",
        "            )\n",
        "        else:\n",
        "            model = ELG(\n",
        "                session, train_data={'videostream': data_source},\n",
        "                first_layer_stride=1,\n",
        "                num_modules=2,\n",
        "                num_feature_maps=32,\n",
        "                learning_schedule=[\n",
        "                    {\n",
        "                        'loss_terms_to_optimize': {'dummy': ['hourglass', 'radius']},\n",
        "                    },\n",
        "                ],\n",
        "            )\n",
        "\n",
        "        # Record output frames to file if requested\n",
        "        if args.record_video:\n",
        "            video_out = None\n",
        "            video_out_queue = queue.Queue()\n",
        "            video_out_should_stop = False\n",
        "            video_out_done = threading.Condition()\n",
        "\n",
        "            def _record_frame():\n",
        "                global video_out\n",
        "                last_frame_time = None\n",
        "                out_fps = 30\n",
        "                out_frame_interval = 1.0 / out_fps\n",
        "                while not video_out_should_stop:\n",
        "                    frame_index = video_out_queue.get()\n",
        "                    if frame_index is None:\n",
        "                        break\n",
        "                    assert frame_index in data_source._frames\n",
        "                    frame = data_source._frames[frame_index]['bgr']\n",
        "                    h, w, _ = frame.shape\n",
        "                    if video_out is None:\n",
        "                        video_out = cv.VideoWriter(\n",
        "                            args.record_video, cv.VideoWriter_fourcc(*'H264'),\n",
        "                            out_fps, (w, h),\n",
        "                        )\n",
        "                    now_time = time.time()\n",
        "                    if last_frame_time is not None:\n",
        "                        time_diff = now_time - last_frame_time\n",
        "                        while time_diff > 0.0:\n",
        "                            video_out.write(frame)\n",
        "                            time_diff -= out_frame_interval\n",
        "                    last_frame_time = now_time\n",
        "                video_out.release()\n",
        "                with video_out_done:\n",
        "                    video_out_done.notify_all()\n",
        "            record_thread = threading.Thread(target=_record_frame, name='record')\n",
        "            record_thread.daemon = True\n",
        "            record_thread.start()\n",
        "\n",
        "        # Begin visualization thread\n",
        "        inferred_stuff_queue = queue.Queue()\n",
        "\n",
        "        def _visualize_output():\n",
        "            last_frame_index = 0\n",
        "            last_frame_time = time.time()\n",
        "            fps_history = []\n",
        "            all_gaze_histories = []\n",
        "\n",
        "            if args.fullscreen:\n",
        "                cv.namedWindow('vis', cv.WND_PROP_FULLSCREEN)\n",
        "                cv.setWindowProperty('vis', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
        "\n",
        "            while True:\n",
        "                # If no output to visualize, show unannotated frame\n",
        "                if inferred_stuff_queue.empty():\n",
        "                    next_frame_index = last_frame_index + 1\n",
        "                    if next_frame_index in data_source._frames:\n",
        "                        next_frame = data_source._frames[next_frame_index]\n",
        "                        if 'faces' in next_frame and len(next_frame['faces']) == 0:\n",
        "                            if not args.headless:\n",
        "                                cv.imshow('vis', next_frame['bgr'])\n",
        "                            if args.record_video:\n",
        "                                video_out_queue.put_nowait(next_frame_index)\n",
        "                            last_frame_index = next_frame_index\n",
        "                    if cv.waitKey(1) & 0xFF == ord('q'):\n",
        "                        return\n",
        "                    continue\n",
        "\n",
        "                # Get output from neural network and visualize\n",
        "                output = inferred_stuff_queue.get()\n",
        "                bgr = None\n",
        "                for j in range(batch_size):\n",
        "                    frame_index = output['frame_index'][j]\n",
        "                    if frame_index not in data_source._frames:\n",
        "                        continue\n",
        "                    frame = data_source._frames[frame_index]\n",
        "\n",
        "                    # Decide which landmarks are usable\n",
        "                    heatmaps_amax = np.amax(output['heatmaps'][j, :].reshape(-1, 18), axis=0)\n",
        "                    can_use_eye = np.all(heatmaps_amax > 0.7)\n",
        "                    can_use_eyelid = np.all(heatmaps_amax[0:8] > 0.75)\n",
        "                    can_use_iris = np.all(heatmaps_amax[8:16] > 0.8)\n",
        "\n",
        "                    start_time = time.time()\n",
        "                    eye_index = output['eye_index'][j]\n",
        "                    bgr = frame['bgr']\n",
        "                    eye = frame['eyes'][eye_index]\n",
        "                    eye_image = eye['image']\n",
        "                    eye_side = eye['side']\n",
        "                    eye_landmarks = output['landmarks'][j, :]\n",
        "                    eye_radius = output['radius'][j][0]\n",
        "                    if eye_side == 'left':\n",
        "                        eye_landmarks[:, 0] = eye_image.shape[1] - eye_landmarks[:, 0]\n",
        "                        eye_image = np.fliplr(eye_image)\n",
        "\n",
        "                    # Embed eye image and annotate for picture-in-picture\n",
        "                    eye_upscale = 2\n",
        "                    eye_image_raw = cv.cvtColor(cv.equalizeHist(eye_image), cv.COLOR_GRAY2BGR)\n",
        "                    eye_image_raw = cv.resize(eye_image_raw, (0, 0), fx=eye_upscale, fy=eye_upscale)\n",
        "                    eye_image_annotated = np.copy(eye_image_raw)\n",
        "                    if can_use_eyelid:\n",
        "                        cv.polylines(\n",
        "                            eye_image_annotated,\n",
        "                            [np.round(eye_upscale*eye_landmarks[0:8]).astype(np.int32)\n",
        "                                                                     .reshape(-1, 1, 2)],\n",
        "                            isClosed=True, color=(255, 255, 0), thickness=1, lineType=cv.LINE_AA,\n",
        "                        )\n",
        "                    if can_use_iris:\n",
        "                        cv.polylines(\n",
        "                            eye_image_annotated,\n",
        "                            [np.round(eye_upscale*eye_landmarks[8:16]).astype(np.int32)\n",
        "                                                                      .reshape(-1, 1, 2)],\n",
        "                            isClosed=True, color=(0, 255, 255), thickness=1, lineType=cv.LINE_AA,\n",
        "                        )\n",
        "                        cv.drawMarker(\n",
        "                            eye_image_annotated,\n",
        "                            tuple(np.round(eye_upscale*eye_landmarks[16, :]).astype(np.int32)),\n",
        "                            color=(0, 255, 255), markerType=cv.MARKER_CROSS, markerSize=4,\n",
        "                            thickness=1, line_type=cv.LINE_AA,\n",
        "                        )\n",
        "                    face_index = int(eye_index / 2)\n",
        "                    eh, ew, _ = eye_image_raw.shape\n",
        "                    v0 = face_index * 2 * eh\n",
        "                    v1 = v0 + eh\n",
        "                    v2 = v1 + eh\n",
        "                    u0 = 0 if eye_side == 'left' else ew\n",
        "                    u1 = u0 + ew\n",
        "                    bgr[v0:v1, u0:u1] = eye_image_raw\n",
        "                    bgr[v1:v2, u0:u1] = eye_image_annotated\n",
        "\n",
        "                    # Visualize preprocessing results\n",
        "                    frame_landmarks = (frame['smoothed_landmarks']\n",
        "                                       if 'smoothed_landmarks' in frame\n",
        "                                       else frame['landmarks'])\n",
        "                    for f, face in enumerate(frame['faces']):\n",
        "                        for landmark in frame_landmarks[f][:-1]:\n",
        "                            cv.drawMarker(bgr, tuple(np.round(landmark).astype(np.int32)),\n",
        "                                          color=(0, 0, 255), markerType=cv.MARKER_STAR,\n",
        "                                          markerSize=2, thickness=1, line_type=cv.LINE_AA)\n",
        "                        cv.rectangle(\n",
        "                            bgr, tuple(np.round(face[:2]).astype(np.int32)),\n",
        "                            tuple(np.round(np.add(face[:2], face[2:])).astype(np.int32)),\n",
        "                            color=(0, 255, 255), thickness=1, lineType=cv.LINE_AA,\n",
        "                        )\n",
        "\n",
        "                    # Transform predictions\n",
        "                    eye_landmarks = np.concatenate([eye_landmarks,\n",
        "                                                    [[eye_landmarks[-1, 0] + eye_radius,\n",
        "                                                      eye_landmarks[-1, 1]]]])\n",
        "                    eye_landmarks = np.asmatrix(np.pad(eye_landmarks, ((0, 0), (0, 1)),\n",
        "                                                       'constant', constant_values=1.0))\n",
        "                    eye_landmarks = (eye_landmarks *\n",
        "                                     eye['inv_landmarks_transform_mat'].T)[:, :2]\n",
        "                    eye_landmarks = np.asarray(eye_landmarks)\n",
        "                    eyelid_landmarks = eye_landmarks[0:8, :]\n",
        "                    iris_landmarks = eye_landmarks[8:16, :]\n",
        "                    iris_centre = eye_landmarks[16, :]\n",
        "                    eyeball_centre = eye_landmarks[17, :]\n",
        "                    eyeball_radius = np.linalg.norm(eye_landmarks[18, :] -\n",
        "                                                    eye_landmarks[17, :])\n",
        "\n",
        "                    # Smooth and visualize gaze direction\n",
        "                    num_total_eyes_in_frame = len(frame['eyes'])\n",
        "                    if len(all_gaze_histories) != num_total_eyes_in_frame:\n",
        "                        all_gaze_histories = [list() for _ in range(num_total_eyes_in_frame)]\n",
        "                    gaze_history = all_gaze_histories[eye_index]\n",
        "                    if can_use_eye:\n",
        "                        # Visualize landmarks\n",
        "                        cv.drawMarker(  # Eyeball centre\n",
        "                            bgr, tuple(np.round(eyeball_centre).astype(np.int32)),\n",
        "                            color=(0, 255, 0), markerType=cv.MARKER_CROSS, markerSize=4,\n",
        "                            thickness=1, line_type=cv.LINE_AA,\n",
        "                        )\n",
        "                        # cv.circle(  # Eyeball outline\n",
        "                        #     bgr, tuple(np.round(eyeball_centre).astype(np.int32)),\n",
        "                        #     int(np.round(eyeball_radius)), color=(0, 255, 0),\n",
        "                        #     thickness=1, lineType=cv.LINE_AA,\n",
        "                        # )\n",
        "\n",
        "                        # Draw \"gaze\"\n",
        "                        # from models.elg import estimate_gaze_from_landmarks\n",
        "                        # current_gaze = estimate_gaze_from_landmarks(\n",
        "                        #     iris_landmarks, iris_centre, eyeball_centre, eyeball_radius)\n",
        "                        i_x0, i_y0 = iris_centre\n",
        "                        e_x0, e_y0 = eyeball_centre\n",
        "                        theta = -np.arcsin(np.clip((i_y0 - e_y0) / eyeball_radius, -1.0, 1.0))\n",
        "                        phi = np.arcsin(np.clip((i_x0 - e_x0) / (eyeball_radius * -np.cos(theta)),\n",
        "                                                -1.0, 1.0))\n",
        "                        current_gaze = np.array([theta, phi])\n",
        "                        gaze_history.append(current_gaze)\n",
        "                        gaze_history_max_len = 10\n",
        "                        if len(gaze_history) > gaze_history_max_len:\n",
        "                            gaze_history = gaze_history[-gaze_history_max_len:]\n",
        "                        draw_gaze(bgr, iris_centre, np.mean(gaze_history, axis=0),\n",
        "                                            length=120.0, thickness=1)\n",
        "                    else:\n",
        "                        gaze_history.clear()\n",
        "\n",
        "                    if can_use_eyelid:\n",
        "                        cv.polylines(\n",
        "                            bgr, [np.round(eyelid_landmarks).astype(np.int32).reshape(-1, 1, 2)],\n",
        "                            isClosed=True, color=(255, 255, 0), thickness=1, lineType=cv.LINE_AA,\n",
        "                        )\n",
        "\n",
        "                    if can_use_iris:\n",
        "                        cv.polylines(\n",
        "                            bgr, [np.round(iris_landmarks).astype(np.int32).reshape(-1, 1, 2)],\n",
        "                            isClosed=True, color=(0, 255, 255), thickness=1, lineType=cv.LINE_AA,\n",
        "                        )\n",
        "                        cv.drawMarker(\n",
        "                            bgr, tuple(np.round(iris_centre).astype(np.int32)),\n",
        "                            color=(0, 255, 255), markerType=cv.MARKER_CROSS, markerSize=4,\n",
        "                            thickness=1, line_type=cv.LINE_AA,\n",
        "                        )\n",
        "\n",
        "                    dtime = 1e3*(time.time() - start_time)\n",
        "                    if 'visualization' not in frame['time']:\n",
        "                        frame['time']['visualization'] = dtime\n",
        "                    else:\n",
        "                        frame['time']['visualization'] += dtime\n",
        "\n",
        "                    def _dtime(before_id, after_id):\n",
        "                        return int(1e3 * (frame['time'][after_id] - frame['time'][before_id]))\n",
        "\n",
        "                    def _dstr(title, before_id, after_id):\n",
        "                        return '%s: %dms' % (title, _dtime(before_id, after_id))\n",
        "\n",
        "                    if eye_index == len(frame['eyes']) - 1:\n",
        "                        # Calculate timings\n",
        "                        frame['time']['after_visualization'] = time.time()\n",
        "                        fps = int(np.round(1.0 / (time.time() - last_frame_time)))\n",
        "                        fps_history.append(fps)\n",
        "                        if len(fps_history) > 60:\n",
        "                            fps_history = fps_history[-60:]\n",
        "                        fps_str = '%d FPS' % np.mean(fps_history)\n",
        "                        last_frame_time = time.time()\n",
        "                        fh, fw, _ = bgr.shape\n",
        "                        cv.putText(bgr, fps_str, org=(fw - 110, fh - 20),\n",
        "                                   fontFace=cv.FONT_HERSHEY_DUPLEX, fontScale=0.8,\n",
        "                                   color=(0, 0, 0), thickness=1, lineType=cv.LINE_AA)\n",
        "                        cv.putText(bgr, fps_str, org=(fw - 111, fh - 21),\n",
        "                                   fontFace=cv.FONT_HERSHEY_DUPLEX, fontScale=0.79,\n",
        "                                   color=(255, 255, 255), thickness=1, lineType=cv.LINE_AA)\n",
        "                        if not args.headless:\n",
        "                            cv.imshow('vis', bgr)\n",
        "                        last_frame_index = frame_index\n",
        "\n",
        "                        # Record frame?\n",
        "                        if args.record_video:\n",
        "                            video_out_queue.put_nowait(frame_index)\n",
        "\n",
        "                        # Quit?\n",
        "                        if cv.waitKey(1) & 0xFF == ord('q'):\n",
        "                            return\n",
        "\n",
        "                        # Print timings\n",
        "                        if frame_index % 60 == 0:\n",
        "                            latency = _dtime('before_frame_read', 'after_visualization')\n",
        "                            processing = _dtime('after_frame_read', 'after_visualization')\n",
        "                            timing_string = ', '.join([\n",
        "                                _dstr('read', 'before_frame_read', 'after_frame_read'),\n",
        "                                _dstr('preproc', 'after_frame_read', 'after_preprocessing'),\n",
        "                                'infer: %dms' % int(frame['time']['inference']),\n",
        "                                'vis: %dms' % int(frame['time']['visualization']),\n",
        "                                'proc: %dms' % processing,\n",
        "                                'latency: %dms' % latency,\n",
        "                            ])\n",
        "                            print('%08d [%s] %s' % (frame_index, fps_str, timing_string))\n",
        "\n",
        "        visualize_thread = threading.Thread(target=_visualize_output, name='visualization')\n",
        "        visualize_thread.daemon = True\n",
        "        visualize_thread.start()\n",
        "\n",
        "        # Do inference forever\n",
        "        infer = model.inference_generator()\n",
        "        while True:\n",
        "            output = next(infer)\n",
        "            for frame_index in np.unique(output['frame_index']):\n",
        "                if frame_index not in data_source._frames:\n",
        "                    continue\n",
        "                frame = data_source._frames[frame_index]\n",
        "                if 'inference' in frame['time']:\n",
        "                    frame['time']['inference'] += output['inference_time']\n",
        "                else:\n",
        "                    frame['time']['inference'] = output['inference_time']\n",
        "            inferred_stuff_queue.put_nowait(output)\n",
        "\n",
        "            if not visualize_thread.isAlive():\n",
        "                break\n",
        "\n",
        "            if not data_source._open:\n",
        "                break\n",
        "\n",
        "        # Close video recording\n",
        "        if record_video and video_out is not None:\n",
        "            video_out_should_stop = True\n",
        "            video_out_queue.put_nowait(None)\n",
        "            with video_out_done:\n",
        "                video_out_done.wait()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIHJKqKxP1CT"
      },
      "source": [
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "\n",
        "##from core import BaseDataSource, BaseModel\n",
        "\n",
        "\n",
        "def _tf_mse(x, y):\n",
        "    \"\"\"Tensorflow call for mean-squared error.\"\"\"\n",
        "    return tf.reduce_mean(tf.squared_difference(x, y))\n",
        "\n",
        "\n",
        "class ELG(BaseModel):\n",
        "    \"\"\"ELG architecture as introduced in [Park et al. ETRA'18].\"\"\"\n",
        "\n",
        "    def __init__(self, tensorflow_session=None, first_layer_stride=1,\n",
        "                 num_modules=2, num_feature_maps=32, **kwargs):\n",
        "        \"\"\"Specify ELG-specific parameters.\"\"\"\n",
        "        self._hg_first_layer_stride = first_layer_stride\n",
        "        self._hg_num_modules = num_modules\n",
        "        self._hg_num_feature_maps= num_feature_maps\n",
        "\n",
        "        # Call parent class constructor\n",
        "        super().__init__(tensorflow_session, **kwargs)\n",
        "\n",
        "    _hg_first_layer_stride = 1\n",
        "    _hg_num_modules = 2\n",
        "    _hg_num_feature_maps = 32\n",
        "    _hg_num_landmarks = 18\n",
        "    _hg_num_residual_blocks = 1\n",
        "\n",
        "    @property\n",
        "    def identifier(self):\n",
        "        \"\"\"Identifier for model based on data sources and parameters.\"\"\"\n",
        "        first_data_source = next(iter(self._train_data.values()))\n",
        "        input_tensors = first_data_source.output_tensors\n",
        "        if self._data_format == 'NHWC':\n",
        "            _, eh, ew, _ = input_tensors['eye'].shape.as_list()\n",
        "        else:\n",
        "            _, _, eh, ew = input_tensors['eye'].shape.as_list()\n",
        "        return 'ELG_i%dx%d_f%dx%d_n%d_m%d' % (\n",
        "            ew, eh,\n",
        "            int(ew / self._hg_first_layer_stride),\n",
        "            int(eh / self._hg_first_layer_stride),\n",
        "            self._hg_num_feature_maps, self._hg_num_modules,\n",
        "        )\n",
        "\n",
        "    def train_loop_pre(self, current_step):\n",
        "        \"\"\"Run this at beginning of training loop.\"\"\"\n",
        "        # Set difficulty of training data\n",
        "        data_source = next(iter(self._train_data.values()))\n",
        "        data_source.set_difficulty(min((1. / 1e6) * current_step, 1.))\n",
        "\n",
        "    def build_model(self, data_sources: Dict[str, BaseDataSource], mode: str):\n",
        "        \"\"\"Build model.\"\"\"\n",
        "        print(\"called ELG build model\")\n",
        "        data_source = next(iter(data_sources.values()))\n",
        "        input_tensors = data_source.output_tensors\n",
        "        x = input_tensors['eye']\n",
        "        y1 = input_tensors['heatmaps'] if 'heatmaps' in input_tensors else None\n",
        "        y2 = input_tensors['landmarks'] if 'landmarks' in input_tensors else None\n",
        "        y3 = input_tensors['radius'] if 'radius' in input_tensors else None\n",
        "\n",
        "        with tf.variable_scope('input_data'):\n",
        "            self.summary.feature_maps('eyes', x, data_format=self._data_format_longer)\n",
        "            if y1 is not None:\n",
        "                self.summary.feature_maps('hmaps_true', y1, data_format=self._data_format_longer)\n",
        "\n",
        "        outputs = {}\n",
        "        loss_terms = {}\n",
        "        metrics = {}\n",
        "\n",
        "        with tf.variable_scope('hourglass'):\n",
        "            # TODO: Find better way to specify no. landmarks\n",
        "            if y1 is not None:\n",
        "                if self._data_format == 'NCHW':\n",
        "                    self._hg_num_landmarks = y1.shape.as_list()[1]\n",
        "                if self._data_format == 'NHWC':\n",
        "                    self._hg_num_landmarks = y1.shape.as_list()[3]\n",
        "            else:\n",
        "                self._hg_num_landmarks = 18\n",
        "            assert self._hg_num_landmarks == 18\n",
        "\n",
        "            # Prepare for Hourglass by downscaling via conv\n",
        "            with tf.variable_scope('pre'):\n",
        "                n = self._hg_num_feature_maps\n",
        "                x = self._apply_conv(x, num_features=n, kernel_size=7,\n",
        "                                     stride=self._hg_first_layer_stride)\n",
        "                x = tf.nn.relu(self._apply_bn(x))\n",
        "                x = self._build_residual_block(x, n, 2*n, name='res1')\n",
        "                x = self._build_residual_block(x, 2*n, n, name='res2')\n",
        "\n",
        "            # Hourglass blocks\n",
        "            x_prev = x\n",
        "            for i in range(self._hg_num_modules):\n",
        "                with tf.variable_scope('hg_%d' % (i + 1)):\n",
        "                    x = self._build_hourglass(x, steps_to_go=4, num_features=self._hg_num_feature_maps)\n",
        "                    x, h = self._build_hourglass_after(\n",
        "                        x_prev, x, do_merge=(i < (self._hg_num_modules - 1)),\n",
        "                    )\n",
        "                    self.summary.feature_maps('hmap%d' % i, h, data_format=self._data_format_longer)\n",
        "                    if y1 is not None:\n",
        "                        metrics['heatmap%d_mse' % (i + 1)] = _tf_mse(h, y1)\n",
        "                    x_prev = x\n",
        "            if y1 is not None:\n",
        "                loss_terms['heatmaps_mse'] = tf.reduce_mean([\n",
        "                    metrics['heatmap%d_mse' % (i + 1)] for i in range(self._hg_num_modules)\n",
        "                ])\n",
        "            x = h\n",
        "            outputs['heatmaps'] = x\n",
        "\n",
        "        # Soft-argmax\n",
        "        x = self._calculate_landmarks(x)\n",
        "        with tf.variable_scope('upscale'):\n",
        "            # Upscale since heatmaps are half-scale of original image\n",
        "            x *= self._hg_first_layer_stride\n",
        "            if y2 is not None:\n",
        "                metrics['landmarks_mse'] = _tf_mse(x, y2)\n",
        "            outputs['landmarks'] = x\n",
        "\n",
        "        # Fully-connected layers for radius regression\n",
        "        with tf.variable_scope('radius'):\n",
        "            x = tf.contrib.layers.flatten(tf.transpose(x, perm=[0, 2, 1]))\n",
        "            for i in range(3):\n",
        "                with tf.variable_scope('fc%d' % (i + 1)):\n",
        "                    x = tf.nn.relu(self._apply_bn(self._apply_fc(x, 100)))\n",
        "            with tf.variable_scope('out'):\n",
        "                x = self._apply_fc(x, 1)\n",
        "            outputs['radius'] = x\n",
        "            if y3 is not None:\n",
        "                metrics['radius_mse'] = _tf_mse(tf.reshape(x, [-1]), y3)\n",
        "                loss_terms['radius_mse'] = 1e-7 * metrics['radius_mse']\n",
        "            self.summary.histogram('radius', x)\n",
        "\n",
        "        # Define outputs\n",
        "        return outputs, loss_terms, metrics\n",
        "\n",
        "    def _apply_conv(self, tensor, num_features, kernel_size=3, stride=1):\n",
        "        return tf.layers.conv2d(\n",
        "            tensor,\n",
        "            num_features,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=stride,\n",
        "            padding='SAME',\n",
        "            kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            data_format=self._data_format_longer,\n",
        "            name='conv',\n",
        "        )\n",
        "\n",
        "    def _apply_fc(self, tensor, num_outputs):\n",
        "        return tf.layers.dense(\n",
        "            tensor,\n",
        "            num_outputs,\n",
        "            use_bias=True,\n",
        "            kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            name='fc',\n",
        "        )\n",
        "\n",
        "    def _apply_pool(self, tensor, kernel_size=3, stride=2):\n",
        "        tensor = tf.layers.max_pooling2d(\n",
        "            tensor,\n",
        "            pool_size=kernel_size,\n",
        "            strides=stride,\n",
        "            padding='SAME',\n",
        "            data_format=self._data_format_longer,\n",
        "            name='pool',\n",
        "        )\n",
        "        return tensor\n",
        "\n",
        "    def _apply_bn(self, tensor):\n",
        "        return tf.contrib.layers.batch_norm(\n",
        "            tensor,\n",
        "            scale=True,\n",
        "            center=True,\n",
        "            is_training=self.use_batch_statistics,\n",
        "            trainable=True,\n",
        "            data_format=self._data_format,\n",
        "            updates_collections=None,\n",
        "        )\n",
        "\n",
        "    def _build_residual_block(self, x, num_in, num_out, name='res_block'):\n",
        "        with tf.variable_scope(name):\n",
        "            half_num_out = max(int(num_out/2), 1)\n",
        "            c = x\n",
        "            with tf.variable_scope('conv1'):\n",
        "                c = tf.nn.relu(self._apply_bn(c))\n",
        "                c = self._apply_conv(c, num_features=half_num_out, kernel_size=1, stride=1)\n",
        "            with tf.variable_scope('conv2'):\n",
        "                c = tf.nn.relu(self._apply_bn(c))\n",
        "                c = self._apply_conv(c, num_features=half_num_out, kernel_size=3, stride=1)\n",
        "            with tf.variable_scope('conv3'):\n",
        "                c = tf.nn.relu(self._apply_bn(c))\n",
        "                c = self._apply_conv(c, num_features=num_out, kernel_size=1, stride=1)\n",
        "            with tf.variable_scope('skip'):\n",
        "                if num_in == num_out:\n",
        "                    s = tf.identity(x)\n",
        "                else:\n",
        "                    s = self._apply_conv(x, num_features=num_out, kernel_size=1, stride=1)\n",
        "            x = c + s\n",
        "        return x\n",
        "\n",
        "    def _build_hourglass(self, x, steps_to_go, num_features, depth=1):\n",
        "        with tf.variable_scope('depth%d' % depth):\n",
        "            # Upper branch\n",
        "            up1 = x\n",
        "            for i in range(self._hg_num_residual_blocks):\n",
        "                up1 = self._build_residual_block(up1, num_features, num_features,\n",
        "                                                 name='up1_%d' % (i + 1))\n",
        "            # Lower branch\n",
        "            low1 = self._apply_pool(x, kernel_size=2, stride=2)\n",
        "            for i in range(self._hg_num_residual_blocks):\n",
        "                low1 = self._build_residual_block(low1, num_features, num_features,\n",
        "                                                  name='low1_%d' % (i + 1))\n",
        "            # Recursive\n",
        "            low2 = None\n",
        "            if steps_to_go > 1:\n",
        "                low2 = self._build_hourglass(low1, steps_to_go - 1, num_features, depth=depth+1)\n",
        "            else:\n",
        "                low2 = low1\n",
        "                for i in range(self._hg_num_residual_blocks):\n",
        "                    low2 = self._build_residual_block(low2, num_features, num_features,\n",
        "                                                      name='low2_%d' % (i + 1))\n",
        "            # Additional residual blocks\n",
        "            low3 = low2\n",
        "            for i in range(self._hg_num_residual_blocks):\n",
        "                low3 = self._build_residual_block(low3, num_features, num_features,\n",
        "                                                  name='low3_%d' % (i + 1))\n",
        "            # Upsample\n",
        "            if self._data_format == 'NCHW':  # convert to NHWC\n",
        "                low3 = tf.transpose(low3, (0, 2, 3, 1))\n",
        "            up2 = tf.image.resize_bilinear(\n",
        "                    low3,\n",
        "                    up1.shape[1:3] if self._data_format == 'NHWC' else up1.shape[2:4],\n",
        "                    align_corners=True,\n",
        "                  )\n",
        "            if self._data_format == 'NCHW':  # convert back from NHWC\n",
        "                up2 = tf.transpose(up2, (0, 3, 1, 2))\n",
        "\n",
        "        return up1 + up2\n",
        "\n",
        "    def _build_hourglass_after(self, x_prev, x_now, do_merge=True):\n",
        "        with tf.variable_scope('after'):\n",
        "            for j in range(self._hg_num_residual_blocks):\n",
        "                x_now = self._build_residual_block(x_now, self._hg_num_feature_maps,\n",
        "                                                   self._hg_num_feature_maps,\n",
        "                                                   name='after_hg_%d' % (j + 1))\n",
        "            x_now = self._apply_conv(x_now, self._hg_num_feature_maps, kernel_size=1, stride=1)\n",
        "            x_now = self._apply_bn(x_now)\n",
        "            x_now = tf.nn.relu(x_now)\n",
        "\n",
        "            with tf.variable_scope('hmap'):\n",
        "                h = self._apply_conv(x_now, self._hg_num_landmarks, kernel_size=1, stride=1)\n",
        "\n",
        "        x_next = x_now\n",
        "        if do_merge:\n",
        "            with tf.variable_scope('merge'):\n",
        "                with tf.variable_scope('h'):\n",
        "                    x_hmaps = self._apply_conv(h, self._hg_num_feature_maps, kernel_size=1, stride=1)\n",
        "                with tf.variable_scope('x'):\n",
        "                    x_now = self._apply_conv(x_now, self._hg_num_feature_maps, kernel_size=1, stride=1)\n",
        "                x_next += x_prev + x_hmaps\n",
        "        return x_next, h\n",
        "\n",
        "    _softargmax_coords = None\n",
        "\n",
        "    def _calculate_landmarks(self, x):\n",
        "        \"\"\"Estimate landmark location from heatmaps.\"\"\"\n",
        "        with tf.variable_scope('argsoftmax'):\n",
        "            if self._data_format == 'NHWC':\n",
        "                _, h, w, _ = x.shape.as_list()\n",
        "            else:\n",
        "                _, _, h, w = x.shape.as_list()\n",
        "            if self._softargmax_coords is None:\n",
        "                # Assume normalized coordinate [0, 1] for numeric stability\n",
        "                ref_xs, ref_ys = np.meshgrid(np.linspace(0, 1.0, num=w, endpoint=True),\n",
        "                                             np.linspace(0, 1.0, num=h, endpoint=True),\n",
        "                                             indexing='xy')\n",
        "                ref_xs = np.reshape(ref_xs, [-1, h*w])\n",
        "                ref_ys = np.reshape(ref_ys, [-1, h*w])\n",
        "                self._softargmax_coords = (\n",
        "                    tf.constant(ref_xs, dtype=tf.float32),\n",
        "                    tf.constant(ref_ys, dtype=tf.float32),\n",
        "                )\n",
        "            ref_xs, ref_ys = self._softargmax_coords\n",
        "\n",
        "            # Assuming N x 18 x 45 x 75 (NCHW)\n",
        "            beta = 1e2\n",
        "            if self._data_format == 'NHWC':\n",
        "                x = tf.transpose(x, (0, 3, 1, 2))\n",
        "            x = tf.reshape(x, [-1, self._hg_num_landmarks, h*w])\n",
        "            x = tf.nn.softmax(beta * x, axis=-1)\n",
        "            lmrk_xs = tf.reduce_sum(ref_xs * x, axis=[2])\n",
        "            lmrk_ys = tf.reduce_sum(ref_ys * x, axis=[2])\n",
        "\n",
        "            # Return to actual coordinates ranges\n",
        "            return tf.stack([\n",
        "                lmrk_xs * (w - 1.0) + 0.5,\n",
        "                lmrk_ys * (h - 1.0) + 0.5,\n",
        "            ], axis=2)  # N x 18 x 2\n",
        "\n",
        "\n",
        "def estimate_gaze_from_landmarks(iris_landmarks, iris_centre, eyeball_centre, eyeball_radius,\n",
        "                                 initial_gaze=None):\n",
        "    \"\"\"Given iris edge landmarks and other coordinates, estimate gaze direction.\n",
        "    More correctly stated, estimate gaze from iris edge landmark coordinates, iris centre\n",
        "    coordinates, eyeball centre coordinates, and eyeball radius in pixels.\n",
        "    \"\"\"\n",
        "    e_x0, e_y0 = eyeball_centre\n",
        "    i_x0, i_y0 = iris_centre\n",
        "\n",
        "    if initial_gaze is not None:\n",
        "        theta, phi = initial_gaze\n",
        "        # theta = -theta\n",
        "    else:\n",
        "        theta = np.arcsin(np.clip((i_y0 - e_y0) / eyeball_radius, -1.0, 1.0))\n",
        "        phi = np.arcsin(np.clip((i_x0 - e_x0) / (eyeball_radius * -np.cos(theta)), -1.0, 1.0))\n",
        "\n",
        "    delta = 0.1 * np.pi\n",
        "    if iris_landmarks[0, 0] < iris_landmarks[4, 0]:  # flipped\n",
        "        alphas = np.flip(np.arange(0.0, 2.0 * np.pi, step=np.pi/4.0), axis=0)\n",
        "    else:\n",
        "        alphas = np.arange(-np.pi, np.pi, step=np.pi/4.0) + np.pi/4.0\n",
        "    sin_alphas = np.sin(alphas)\n",
        "    cos_alphas = np.cos(alphas)\n",
        "\n",
        "    def gaze_fit_loss_func(inputs):\n",
        "        theta, phi, delta, phase = inputs\n",
        "        sin_phase = np.sin(phase)\n",
        "        cos_phase = np.cos(phase)\n",
        "        # sin_alphas_shifted = np.sin(alphas + phase)\n",
        "        sin_alphas_shifted = sin_alphas * cos_phase + cos_alphas * sin_phase\n",
        "        # cos_alphas_shifted = np.cos(alphas + phase)\n",
        "        cos_alphas_shifted = cos_alphas * cos_phase - sin_alphas * sin_phase\n",
        "\n",
        "        sin_theta = np.sin(theta)\n",
        "        cos_theta = np.cos(theta)\n",
        "        sin_phi = np.sin(phi)\n",
        "        cos_phi = np.cos(phi)\n",
        "        sin_delta_sin = np.sin(delta * sin_alphas_shifted)\n",
        "        sin_delta_cos = np.sin(delta * cos_alphas_shifted)\n",
        "        cos_delta_sin = np.cos(delta * sin_alphas_shifted)\n",
        "        cos_delta_cos = np.cos(delta * cos_alphas_shifted)\n",
        "        # x = -np.cos(theta + delta * sin_alphas_shifted)\n",
        "        x1 = -cos_theta * cos_delta_sin + sin_theta * sin_delta_sin\n",
        "        # x *= np.sin(phi + delta * cos_alphas_shifted)\n",
        "        x2 = sin_phi * cos_delta_cos + cos_phi * sin_delta_cos\n",
        "        x = x1 * x2\n",
        "        # y = np.sin(theta + delta * sin_alphas_shifted)\n",
        "        y1 = sin_theta * cos_delta_sin\n",
        "        y2 = cos_theta * sin_delta_sin\n",
        "        y = y1 + y2\n",
        "\n",
        "        ix = e_x0 + eyeball_radius * x\n",
        "        iy = e_y0 + eyeball_radius * y\n",
        "        dx = ix - iris_landmarks[:, 0]\n",
        "        dy = iy - iris_landmarks[:, 1]\n",
        "        out = np.mean(dx ** 2 + dy ** 2)\n",
        "\n",
        "        # In addition, match estimated and actual iris centre\n",
        "        iris_dx = e_x0 + eyeball_radius * -cos_theta * sin_phi - i_x0\n",
        "        iris_dy = e_y0 + eyeball_radius * sin_theta - i_y0\n",
        "        out += iris_dx ** 2 + iris_dy ** 2\n",
        "\n",
        "        # sin_alphas_shifted = sin_alphas * cos_phase + cos_alphas * sin_phase\n",
        "        # cos_alphas_shifted = cos_alphas * cos_phase - sin_alphas * sin_phase\n",
        "        dsin_alphas_shifted_dphase = -sin_alphas * sin_phase + cos_alphas * cos_phase\n",
        "        dcos_alphas_shifted_dphase = -cos_alphas * sin_phase - sin_alphas * cos_phase\n",
        "\n",
        "        # sin_delta_sin = np.sin(delta * sin_alphas_shifted)\n",
        "        # sin_delta_cos = np.sin(delta * cos_alphas_shifted)\n",
        "        # cos_delta_sin = np.cos(delta * sin_alphas_shifted)\n",
        "        # cos_delta_cos = np.cos(delta * cos_alphas_shifted)\n",
        "        dsin_delta_sin_ddelta = cos_delta_sin * sin_alphas_shifted\n",
        "        dsin_delta_cos_ddelta = cos_delta_cos * cos_alphas_shifted\n",
        "        dcos_delta_sin_ddelta = -sin_delta_sin * sin_alphas_shifted\n",
        "        dcos_delta_cos_ddelta = -sin_delta_cos * cos_alphas_shifted\n",
        "        dsin_delta_sin_dphase = cos_delta_sin * delta * dsin_alphas_shifted_dphase\n",
        "        dsin_delta_cos_dphase = cos_delta_cos * delta * dcos_alphas_shifted_dphase\n",
        "        dcos_delta_sin_dphase = -sin_delta_sin * delta * dsin_alphas_shifted_dphase\n",
        "        dcos_delta_cos_dphase = -sin_delta_cos * delta * dcos_alphas_shifted_dphase\n",
        "\n",
        "        # x1 = -cos_theta * cos_delta_sin + sin_theta * sin_delta_sin\n",
        "        # x2 = sin_phi * cos_delta_cos + cos_phi * sin_delta_cos\n",
        "        dx1_dtheta = sin_theta * cos_delta_sin + cos_theta * sin_delta_sin\n",
        "        dx2_dtheta = 0.0\n",
        "        dx1_dphi = 0.0\n",
        "        dx2_dphi = cos_phi * cos_delta_cos - sin_phi * sin_delta_cos\n",
        "        dx1_ddelta = -cos_theta * dcos_delta_sin_ddelta + sin_theta * dsin_delta_sin_ddelta\n",
        "        dx2_ddelta = sin_phi * dcos_delta_cos_ddelta + cos_phi * dsin_delta_cos_ddelta\n",
        "        dx1_dphase = -cos_theta * dcos_delta_sin_dphase + sin_theta * dsin_delta_sin_dphase\n",
        "        dx2_dphase = sin_phi * dcos_delta_cos_dphase + cos_phi * dsin_delta_cos_dphase\n",
        "\n",
        "        # y1 = sin_theta * cos_delta_sin\n",
        "        # y2 = cos_theta * sin_delta_sin\n",
        "        dy1_dtheta = cos_theta * cos_delta_sin\n",
        "        dy2_dtheta = -sin_theta * sin_delta_sin\n",
        "        dy1_dphi = 0.0\n",
        "        dy2_dphi = 0.0\n",
        "        dy1_ddelta = sin_theta * dcos_delta_sin_ddelta\n",
        "        dy2_ddelta = cos_theta * dsin_delta_sin_ddelta\n",
        "        dy1_dphase = sin_theta * dcos_delta_sin_dphase\n",
        "        dy2_dphase = cos_theta * dsin_delta_sin_dphase\n",
        "\n",
        "        # x = x1 * x2\n",
        "        # y = y1 + y2\n",
        "        dx_dtheta = dx1_dtheta * x2 + x1 * dx2_dtheta\n",
        "        dx_dphi = dx1_dphi * x2 + x1 * dx2_dphi\n",
        "        dx_ddelta = dx1_ddelta * x2 + x1 * dx2_ddelta\n",
        "        dx_dphase = dx1_dphase * x2 + x1 * dx2_dphase\n",
        "        dy_dtheta = dy1_dtheta + dy2_dtheta\n",
        "        dy_dphi = dy1_dphi + dy2_dphi\n",
        "        dy_ddelta = dy1_ddelta + dy2_ddelta\n",
        "        dy_dphase = dy1_dphase + dy2_dphase\n",
        "\n",
        "        # ix = w_2 + eyeball_radius * x\n",
        "        # iy = h_2 + eyeball_radius * y\n",
        "        dix_dtheta = eyeball_radius * dx_dtheta\n",
        "        dix_dphi = eyeball_radius * dx_dphi\n",
        "        dix_ddelta = eyeball_radius * dx_ddelta\n",
        "        dix_dphase = eyeball_radius * dx_dphase\n",
        "        diy_dtheta = eyeball_radius * dy_dtheta\n",
        "        diy_dphi = eyeball_radius * dy_dphi\n",
        "        diy_ddelta = eyeball_radius * dy_ddelta\n",
        "        diy_dphase = eyeball_radius * dy_dphase\n",
        "\n",
        "        # dx = ix - iris_landmarks[:, 0]\n",
        "        # dy = iy - iris_landmarks[:, 1]\n",
        "        ddx_dtheta = dix_dtheta\n",
        "        ddx_dphi = dix_dphi\n",
        "        ddx_ddelta = dix_ddelta\n",
        "        ddx_dphase = dix_dphase\n",
        "        ddy_dtheta = diy_dtheta\n",
        "        ddy_dphi = diy_dphi\n",
        "        ddy_ddelta = diy_ddelta\n",
        "        ddy_dphase = diy_dphase\n",
        "\n",
        "        # out = dx ** 2 + dy ** 2\n",
        "        dout_dtheta = np.mean(2 * (dx * ddx_dtheta + dy * ddy_dtheta))\n",
        "        dout_dphi = np.mean(2 * (dx * ddx_dphi + dy * ddy_dphi))\n",
        "        dout_ddelta = np.mean(2 * (dx * ddx_ddelta + dy * ddy_ddelta))\n",
        "        dout_dphase = np.mean(2 * (dx * ddx_dphase + dy * ddy_dphase))\n",
        "\n",
        "        # iris_dx = e_x0 + eyeball_radius * -cos_theta * sin_phi - i_x0\n",
        "        # iris_dy = e_y0 + eyeball_radius * sin_theta - i_y0\n",
        "        # out += iris_dx ** 2 + iris_dy ** 2\n",
        "        dout_dtheta += 2 * eyeball_radius * (sin_theta * sin_phi * iris_dx + cos_theta * iris_dy)\n",
        "        dout_dphi += 2 * eyeball_radius * (-cos_theta * cos_phi * iris_dx)\n",
        "\n",
        "        return out, np.array([dout_dtheta, dout_dphi, dout_ddelta, dout_dphase])\n",
        "\n",
        "    phase = 0.02\n",
        "    result = scipy.optimize.minimize(gaze_fit_loss_func, x0=[theta, phi, delta, phase],\n",
        "                                     bounds=(\n",
        "                                         (-0.4*np.pi, 0.4*np.pi),\n",
        "                                         (-0.4*np.pi, 0.4*np.pi),\n",
        "                                         (0.01*np.pi, 0.5*np.pi),\n",
        "                                         (-np.pi, np.pi),\n",
        "                                     ),\n",
        "                                     jac=True,\n",
        "                                     tol=1e-6,\n",
        "                                     method='TNC',\n",
        "                                     options={\n",
        "                                         # 'disp': True,\n",
        "                                         'gtol': 1e-6,\n",
        "                                         'maxiter': 100,\n",
        "                                    })\n",
        "    if result.success:\n",
        "        theta, phi, delta, phase = result.x\n",
        "\n",
        "    return np.array([-theta, phi])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfiyvCt-EETQ"
      },
      "source": [
        "\"\"\"Main script for training a model for gaze estimation.\"\"\"\n",
        "import argparse\n",
        "\n",
        "import coloredlogs\n",
        "import tensorflow as tf\n",
        "\n",
        "def mainELG():\n",
        "\n",
        "    # Set global log level\n",
        "    \"\"\"parser = argparse.ArgumentParser(description='Train a gaze estimation model.')\n",
        "    parser.add_argument('-v', type=str, help='logging level', default='info',\n",
        "                        choices=['debug', 'info', 'warning', 'error', 'critical'])\n",
        "    args = parser.parse_args()\"\"\"\n",
        "    coloredlogs.install(\n",
        "        datefmt='%d/%m %H:%M',\n",
        "        fmt='%(asctime)s %(levelname)s %(message)s',\n",
        "        level=\"info\".upper(),   ##args.v.upper(),\n",
        "    )\n",
        "\n",
        "    # Initialize Tensorflow session\n",
        "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as session:\n",
        "\n",
        "        # Declare some parameters\n",
        "        batch_size = 32\n",
        "\n",
        "        # Define some model-specific parameters\n",
        "        elg_first_layer_stride = 1\n",
        "        elg_num_modules = 3\n",
        "        elg_num_feature_maps = 32\n",
        "\n",
        "        # Define training data source\n",
        "        #from datasources import UnityEyes\n",
        "        unityeyes = UnityEyes(\n",
        "            session,\n",
        "            batch_size=batch_size,\n",
        "            data_format='NCHW',\n",
        "            unityeyes_path=\"/content/gdrive/My Drive/test-data-Eye-tracking/imgs/imgs\",\n",
        "            min_after_dequeue=1000,\n",
        "            generate_heatmaps=True,\n",
        "            shuffle=True,\n",
        "            staging=True,\n",
        "            eye_image_shape=(36, 60),\n",
        "            heatmaps_scale=1.0 / elg_first_layer_stride,\n",
        "        )\n",
        "        unityeyes.set_augmentation_range('translation', 2.0, 10.0)\n",
        "        unityeyes.set_augmentation_range('rotation', 1.0, 10.0)\n",
        "        unityeyes.set_augmentation_range('intensity', 0.5, 20.0)\n",
        "        unityeyes.set_augmentation_range('blur', 0.1, 1.0)\n",
        "        unityeyes.set_augmentation_range('scale', 0.01, 0.1)\n",
        "        unityeyes.set_augmentation_range('rescale', 1.0, 0.5)\n",
        "        unityeyes.set_augmentation_range('num_line', 0.0, 2.0)\n",
        "        unityeyes.set_augmentation_range('heatmap_sigma', 7.5, 2.5)\n",
        "\n",
        "        # Define model\n",
        "        #from models import ELG\n",
        "        model = ELG(\n",
        "            # Tensorflow session\n",
        "            # Note: The same session must be used for the model and the data sources.\n",
        "            session,\n",
        "\n",
        "            # Model configuration parameters\n",
        "            # first_layer_stride describes how much the input image is downsampled before producing\n",
        "            #                    feature maps for eventual heatmaps regression\n",
        "            # num_modules defines the number of hourglass modules, and thus the number of times repeated\n",
        "            #             coarse-to-fine refinement is done.\n",
        "            # num_feature_maps describes how many feature maps are refined over the entire network.\n",
        "            first_layer_stride=elg_first_layer_stride,\n",
        "            num_feature_maps=elg_num_feature_maps,\n",
        "            num_modules=elg_num_modules,\n",
        "\n",
        "            # The learning schedule describes in which order which part of the network should be\n",
        "            # trained and with which learning rate.\n",
        "            #\n",
        "            # A standard network would have one entry (dict) in this argument where all model\n",
        "            # parameters are optimized. To do this, you must specify which variables must be\n",
        "            # optimized and this is done by specifying which prefixes to look for.\n",
        "            # The prefixes are defined by using `tf.variable_scope`.\n",
        "            #\n",
        "            # The loss terms which can be specified depends on model specifications, specifically\n",
        "            # the `loss_terms` output of `BaseModel::build_model`.\n",
        "            learning_schedule=[\n",
        "                {\n",
        "                    'loss_terms_to_optimize': {\n",
        "                        'heatmaps_mse': ['hourglass'],\n",
        "                        'radius_mse': ['radius'],\n",
        "                    },\n",
        "                    'learning_rate': 1e-3,\n",
        "                },\n",
        "            ],\n",
        "\n",
        "            # Data sources for training (and testing).\n",
        "            train_data={'synthetic': unityeyes},\n",
        "        )\n",
        "\n",
        "        # Train this model for a set number of epochs\n",
        "        model.train(\n",
        "            num_epochs=100,\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR86NtbtLsPU"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"Main script for gaze direction inference from webcam feed.\"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import queue\n",
        "import threading\n",
        "import time\n",
        "\n",
        "import coloredlogs\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\"\"\"from datasources import Video, Webcam\n",
        "from models import ELG\n",
        "import util.gaze\"\"\"\n",
        "\n",
        "##if __name__ == '__main__':\n",
        "def test_elg():\n",
        "    # Set global log level\n",
        "    \"\"\"parser = argparse.ArgumentParser(description='Demonstration of landmarks localization.')\n",
        "    parser.add_argument('-v', type=str, help='logging level', default='info',\n",
        "                        choices=['debug', 'info', 'warning', 'error', 'critical'])\n",
        "    parser.add_argument('--from_video', type=str, help='Use this video path instead of webcam')\n",
        "    parser.add_argument('--record_video', type=str, help='Output path of video of demonstration.')\n",
        "    parser.add_argument('--fullscreen', action='store_true')\n",
        "    parser.add_argument('--headless', action='store_true')\n",
        "\n",
        "    parser.add_argument('--fps', type=int, default=60, help='Desired sampling rate of webcam')\n",
        "    parser.add_argument('--camera_id', type=int, default=0, help='ID of webcam to use')\"\"\"\n",
        "\n",
        "    ##Store local\n",
        "    video_path = \"/content/\"\n",
        "    video_name = \"test7.mp4\"\n",
        "    from_video = video_path + video_name\n",
        "    record_video = \"/content/outputs/output.mp4\"\n",
        "    fullscreen = False\n",
        "    headless = False\n",
        "    fps = 60\n",
        "    ##args = parser.parse_args()\n",
        "    coloredlogs.install(\n",
        "        datefmt='%d/%m %H:%M',\n",
        "        fmt='%(asctime)s %(levelname)s %(message)s',\n",
        "        level=\"debug\".upper(),   ##args.v.upper(),\n",
        "    )\n",
        "\n",
        "    # Check if GPU is available\n",
        "    from tensorflow.python.client import device_lib\n",
        "    session_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "    gpu_available = False\n",
        "    try:\n",
        "        gpus = [d for d in device_lib.list_local_devices(config=session_config)\n",
        "                if d.device_type == 'GPU']\n",
        "        gpu_available = len(gpus) > 0\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Initialize Tensorflow session\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "    with tf.Session(config=session_config) as session:\n",
        "\n",
        "        # Declare some parameters\n",
        "        batch_size = 2\n",
        "\n",
        "        # Define webcam stream data source\n",
        "        # Change data_format='NHWC' if not using CUDA\n",
        "        if from_video:\n",
        "            assert os.path.isfile(from_video)\n",
        "            data_source = Video(from_video,\n",
        "                                tensorflow_session=session, batch_size=batch_size,\n",
        "                                data_format='NCHW' if gpu_available else 'NHWC',\n",
        "                                eye_image_shape=(108, 180))\n",
        "        else:\n",
        "            data_source = Webcam(tensorflow_session=session, batch_size=batch_size,\n",
        "                                 camera_id=args.camera_id, fps=args.fps,\n",
        "                                 data_format='NCHW' if gpu_available else 'NHWC',\n",
        "                                 eye_image_shape=(36, 60))\n",
        "\n",
        "        # Define model\n",
        "        if from_video:\n",
        "            model = ELG(\n",
        "                session, train_data={'videostream': data_source},\n",
        "                first_layer_stride=3,\n",
        "                num_modules=3,\n",
        "                num_feature_maps=64,\n",
        "                learning_schedule=[\n",
        "                    {\n",
        "                        'loss_terms_to_optimize': {'dummy': ['hourglass', 'radius']},\n",
        "                    },\n",
        "                ],\n",
        "            )\n",
        "        else:\n",
        "            model = ELG(\n",
        "                session, train_data={'videostream': data_source},\n",
        "                first_layer_stride=1,\n",
        "                num_modules=2,\n",
        "                num_feature_maps=32,\n",
        "                learning_schedule=[\n",
        "                    {\n",
        "                        'loss_terms_to_optimize': {'dummy': ['hourglass', 'radius']},\n",
        "                    },\n",
        "                ],\n",
        "            )\n",
        "\n",
        "        # Record output frames to file if requested\n",
        "        if record_video:\n",
        "            video_out = None\n",
        "            video_out_queue = queue.Queue()\n",
        "            video_out_should_stop = False\n",
        "            video_out_done = threading.Condition()\n",
        "\n",
        "            def _record_frame():\n",
        "                \n",
        "                print(\"Record thread started\")\n",
        "                nonlocal video_out\n",
        "                last_frame_time = None\n",
        "                out_fps = 30\n",
        "                out_frame_interval = 1.0 / out_fps\n",
        "                while not video_out_should_stop:\n",
        "                    print(\"Creating frame for video_out\")\n",
        "                    frame_index = video_out_queue.get()\n",
        "                    if frame_index is None:\n",
        "                        print(\"No frame index from video_out_queue: breaks here \")\n",
        "                        break\n",
        "                    assert frame_index in data_source._frames\n",
        "                    frame = data_source._frames[frame_index]['bgr']\n",
        "                    h, w, _ = frame.shape\n",
        "                    if video_out is None:\n",
        "                        print(\"Writing to video file\")\n",
        "                        video_out = cv.VideoWriter(\n",
        "                            record_video, cv.VideoWriter_fourcc(*\"mp4v\"),  ##'H264'),\n",
        "                            out_fps, (w, h),\n",
        "                        )\n",
        "                    now_time = time.time()\n",
        "                    if last_frame_time is not None:\n",
        "                        time_diff = now_time - last_frame_time\n",
        "                        while time_diff > 0.0:\n",
        "                            #cv.imwrite('kang'+str(frameIndexx)+'.jpg',frame)\n",
        "                            #frameIndexx += 1\n",
        "                            video_out.write(frame)\n",
        "                            time_diff -= out_frame_interval\n",
        "                    last_frame_time = now_time\n",
        "                video_out.release()\n",
        "                with video_out_done:\n",
        "                    video_out_done.notify_all()\n",
        "            record_thread = threading.Thread(target=_record_frame, name='record')\n",
        "            record_thread.daemon = True\n",
        "            record_thread.start()\n",
        "\n",
        "        # Begin visualization thread\n",
        "        inferred_stuff_queue = queue.Queue()\n",
        "\n",
        "        def _visualize_output():\n",
        "            last_frame_index = 0\n",
        "            last_frame_time = time.time()\n",
        "            fps_history = []\n",
        "            all_gaze_histories = []\n",
        "            print(\"In visualise output\")\n",
        "            \"\"\"if fullscreen:\n",
        "                cv.namedWindow('vis', cv.WND_PROP_FULLSCREEN)\n",
        "                cv.setWindowProperty('vis', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\"\"\"\n",
        "\n",
        "            while True:\n",
        "                # If no output to visualize, show unannotated frame\n",
        "                if inferred_stuff_queue.empty():\n",
        "                    next_frame_index = last_frame_index + 1\n",
        "                    if next_frame_index in data_source._frames:\n",
        "                        next_frame = data_source._frames[next_frame_index]\n",
        "                        if 'faces' in next_frame and len(next_frame['faces']) == 0:\n",
        "                            \"\"\"if not headless:\n",
        "                                cv.imshow('vis', next_frame['bgr'])\"\"\"\n",
        "                            if record_video:\n",
        "                                print(\"Adding frame index to video_out_queue\")\n",
        "                                video_out_queue.put_nowait(next_frame_index)\n",
        "                            last_frame_index = next_frame_index\n",
        "                    if cv.waitKey(1) & 0xFF == ord('q'):\n",
        "                        return\n",
        "                    continue\n",
        "\n",
        "                # Get output from neural network and visualize\n",
        "                output = inferred_stuff_queue.get()\n",
        "                bgr = None\n",
        "                for j in range(batch_size):\n",
        "                    frame_index = output['frame_index'][j]\n",
        "                    if frame_index not in data_source._frames:\n",
        "                        continue\n",
        "                    frame = data_source._frames[frame_index]\n",
        "                    \n",
        "                    # Decide which landmarks are usable\n",
        "                    heatmaps_amax = np.amax(output['heatmaps'][j, :].reshape(-1, 18), axis=0)\n",
        "                    can_use_eye = np.all(heatmaps_amax > 0.7)\n",
        "                    can_use_eyelid = np.all(heatmaps_amax[0:8] > 0.75)\n",
        "                    can_use_iris = np.all(heatmaps_amax[8:16] > 0.8)\n",
        "\n",
        "                    start_time = time.time()\n",
        "                    eye_index = output['eye_index'][j]\n",
        "                    bgr = frame['bgr']\n",
        "                    #cv.imwrite('kang'+str(j)+'.jpg',frame)\n",
        "                    #frameIndexx += 1\n",
        "                    eye = frame['eyes'][eye_index]\n",
        "                    \n",
        "                    eye_image = eye['image']\n",
        "                    eye_side = eye['side']\n",
        "                    eye_landmarks = output['landmarks'][j, :]\n",
        "                    eye_radius = output['radius'][j][0]\n",
        "                    if eye_side == 'left':\n",
        "                        eye_landmarks[:, 0] = eye_image.shape[1] - eye_landmarks[:, 0]\n",
        "                        eye_image = np.fliplr(eye_image)\n",
        "\n",
        "                    # Embed eye image and annotate for picture-in-picture\n",
        "                    eye_upscale = 2\n",
        "                    eye_image_raw = cv.cvtColor(cv.equalizeHist(eye_image), cv.COLOR_GRAY2BGR)\n",
        "                    eye_image_raw = cv.resize(eye_image_raw, (0, 0), fx=eye_upscale, fy=eye_upscale)\n",
        "                    eye_image_annotated = np.copy(eye_image_raw)\n",
        "                    if can_use_eyelid:\n",
        "                        cv.polylines(\n",
        "                            eye_image_annotated,\n",
        "                            [np.round(eye_upscale*eye_landmarks[0:8]).astype(np.int32)\n",
        "                                                                     .reshape(-1, 1, 2)],\n",
        "                            isClosed=True, color=(255, 255, 0), thickness=1, lineType=cv.LINE_AA, ##Yellow Line\n",
        "                        )\n",
        "                    if can_use_iris:\n",
        "                        cv.polylines(\n",
        "                            eye_image_annotated,\n",
        "                            [np.round(eye_upscale*eye_landmarks[8:16]).astype(np.int32)\n",
        "                                                                      .reshape(-1, 1, 2)],\n",
        "                            isClosed=True, color=(0, 255, 255), thickness=1, lineType=cv.LINE_AA, ##Blue Line\n",
        "                        )\n",
        "                        cv.drawMarker(\n",
        "                            eye_image_annotated,\n",
        "                            tuple(np.round(eye_upscale*eye_landmarks[16, :]).astype(np.int32)),\n",
        "                            color=(0, 255, 255), markerType=cv.MARKER_CROSS, markerSize=4, ##Blue CROSS\n",
        "                            thickness=1, line_type=cv.LINE_AA,\n",
        "                        )\n",
        "                    face_index = int(eye_index / 2)\n",
        "                    eh, ew, _ = eye_image_raw.shape\n",
        "                    v0 = face_index * 2 * eh\n",
        "                    v1 = v0 + eh\n",
        "                    v2 = v1 + eh\n",
        "                    u0 = 0 if eye_side == 'left' else ew\n",
        "                    u1 = u0 + ew\n",
        "                    bgr[v0:v1, u0:u1] = eye_image_raw\n",
        "                    bgr[v1:v2, u0:u1] = eye_image_annotated\n",
        "\n",
        "                    # Visualize preprocessing results\n",
        "                    frame_landmarks = (frame['smoothed_landmarks']\n",
        "                                       if 'smoothed_landmarks' in frame\n",
        "                                       else frame['landmarks'])\n",
        "                    for f, face in enumerate(frame['faces']):\n",
        "                        for landmark in frame_landmarks[f][:-1]:\n",
        "                            cv.drawMarker(bgr, tuple(np.round(landmark).astype(np.int32)),\n",
        "                                          color=(0, 0, 255), markerType=cv.MARKER_STAR,  ##BLUE STAR\n",
        "                                          markerSize=2, thickness=1, line_type=cv.LINE_AA)\n",
        "                        cv.rectangle(\n",
        "                            bgr, tuple(np.round(face[:2]).astype(np.int32)),\n",
        "                            tuple(np.round(np.add(face[:2], face[2:])).astype(np.int32)),\n",
        "                            color=(0, 255, 255), thickness=1, lineType=cv.LINE_AA,\n",
        "                        )\n",
        "\n",
        "                    # Transform predictions\n",
        "                    eye_landmarks = np.concatenate([eye_landmarks,\n",
        "                                                    [[eye_landmarks[-1, 0] + eye_radius,\n",
        "                                                      eye_landmarks[-1, 1]]]])\n",
        "                    eye_landmarks = np.asmatrix(np.pad(eye_landmarks, ((0, 0), (0, 1)),\n",
        "                                                       'constant', constant_values=1.0))\n",
        "                    eye_landmarks = (eye_landmarks *\n",
        "                                     eye['inv_landmarks_transform_mat'].T)[:, :2]\n",
        "                    eye_landmarks = np.asarray(eye_landmarks)\n",
        "                    eyelid_landmarks = eye_landmarks[0:8, :]\n",
        "                    iris_landmarks = eye_landmarks[8:16, :]\n",
        "                    iris_centre = eye_landmarks[16, :]\n",
        "                    eyeball_centre = eye_landmarks[17, :]\n",
        "                    eyeball_radius = np.linalg.norm(eye_landmarks[18, :] -\n",
        "                                                    eye_landmarks[17, :])\n",
        "\n",
        "                    # Smooth and visualize gaze direction\n",
        "                    num_total_eyes_in_frame = len(frame['eyes'])\n",
        "                    if len(all_gaze_histories) != num_total_eyes_in_frame:\n",
        "                        all_gaze_histories = [list() for _ in range(num_total_eyes_in_frame)]\n",
        "                    gaze_history = all_gaze_histories[eye_index]\n",
        "                    if can_use_eye:\n",
        "                        # Visualize landmarks\n",
        "                        cv.drawMarker(  # Eyeball centre\n",
        "                            bgr, tuple(np.round(eyeball_centre).astype(np.int32)),\n",
        "                            color=(0, 255, 0), markerType=cv.MARKER_CROSS, markerSize=4, ##GREEN CENTRE of eyeball\n",
        "                            thickness=1, line_type=cv.LINE_AA,\n",
        "                        )\n",
        "                        # cv.circle(  # Eyeball outline\n",
        "                        #     bgr, tuple(np.round(eyeball_centre).astype(np.int32)),\n",
        "                        #     int(np.round(eyeball_radius)), color=(0, 255, 0),\n",
        "                        #     thickness=1, lineType=cv.LINE_AA,\n",
        "                        # )\n",
        "\n",
        "                        # Draw \"gaze\"\n",
        "                        # from models.elg import estimate_gaze_from_landmarks\n",
        "                        # current_gaze = estimate_gaze_from_landmarks(\n",
        "                        #     iris_landmarks, iris_centre, eyeball_centre, eyeball_radius)\n",
        "                        i_x0, i_y0 = iris_centre\n",
        "                        e_x0, e_y0 = eyeball_centre\n",
        "                        theta = -np.arcsin(np.clip((i_y0 - e_y0) / eyeball_radius, -1.0, 1.0))\n",
        "                        phi = np.arcsin(np.clip((i_x0 - e_x0) / (eyeball_radius * -np.cos(theta)),\n",
        "                                                -1.0, 1.0))\n",
        "                        current_gaze = np.array([theta, phi])\n",
        "                        gaze_history.append(current_gaze)\n",
        "                        gaze_history_max_len = 10\n",
        "                        if len(gaze_history) > gaze_history_max_len:\n",
        "                            gaze_history = gaze_history[-gaze_history_max_len:]\n",
        "                        draw_gaze(bgr, iris_centre, np.mean(gaze_history, axis=0),\n",
        "                                            length=120.0, thickness=1)\n",
        "                    else:\n",
        "                        gaze_history.clear()\n",
        "\n",
        "                    if can_use_eyelid:\n",
        "                        cv.polylines(\n",
        "                            bgr, [np.round(eyelid_landmarks).astype(np.int32).reshape(-1, 1, 2)],\n",
        "                            isClosed=True, color=(255, 255, 0), thickness=1, lineType=cv.LINE_AA,\n",
        "                        )\n",
        "\n",
        "                    if can_use_iris:\n",
        "                        cv.polylines(\n",
        "                            bgr, [np.round(iris_landmarks).astype(np.int32).reshape(-1, 1, 2)],\n",
        "                            isClosed=True, color=(0, 255, 255), thickness=1, lineType=cv.LINE_AA,  ##BLUE LINE ON EYE white\n",
        "                        )\n",
        "                        cv.drawMarker(\n",
        "                            bgr, tuple(np.round(iris_centre).astype(np.int32)),\n",
        "                            color=(0, 255, 255), markerType=cv.MARKER_CROSS, markerSize=4,\n",
        "                            thickness=1, line_type=cv.LINE_AA,\n",
        "                        )\n",
        "\n",
        "                    dtime = 1e3*(time.time() - start_time)\n",
        "                    if 'visualization' not in frame['time']:\n",
        "                        frame['time']['visualization'] = dtime\n",
        "                    else:\n",
        "                        frame['time']['visualization'] += dtime\n",
        "\n",
        "                    def _dtime(before_id, after_id):\n",
        "                        return int(1e3 * (frame['time'][after_id] - frame['time'][before_id]))\n",
        "\n",
        "                    def _dstr(title, before_id, after_id):\n",
        "                        return '%s: %dms' % (title, _dtime(before_id, after_id))\n",
        "\n",
        "                    if eye_index == len(frame['eyes']) - 1:\n",
        "                        # Calculate timings\n",
        "                        frame['time']['after_visualization'] = time.time()\n",
        "                        fps = int(np.round(1.0 / (time.time() - last_frame_time)))\n",
        "                        fps_history.append(fps)\n",
        "                        if len(fps_history) > 60:\n",
        "                            fps_history = fps_history[-60:]\n",
        "                        fps_str = '%d FPS' % np.mean(fps_history)\n",
        "                        last_frame_time = time.time()\n",
        "                        fh, fw, _ = bgr.shape\n",
        "                        cv.putText(bgr, fps_str, org=(fw - 110, fh - 20),\n",
        "                                   fontFace=cv.FONT_HERSHEY_DUPLEX, fontScale=0.8,\n",
        "                                   color=(0, 0, 0), thickness=1, lineType=cv.LINE_AA)\n",
        "                        cv.putText(bgr, fps_str, org=(fw - 111, fh - 21),\n",
        "                                   fontFace=cv.FONT_HERSHEY_DUPLEX, fontScale=0.79,\n",
        "                                   color=(255, 255, 255), thickness=1, lineType=cv.LINE_AA)\n",
        "                        \"\"\"if not headless:\n",
        "                            cv.imshow('vis', bgr)\"\"\"\n",
        "                        last_frame_index = frame_index\n",
        "\n",
        "                        # Record frame?\n",
        "                        if record_video:\n",
        "                            video_out_queue.put_nowait(frame_index)\n",
        "\n",
        "                        # Quit?\n",
        "                        if cv.waitKey(1) & 0xFF == ord('q'):\n",
        "                            return\n",
        "\n",
        "                        # Print timings\n",
        "                        if frame_index % 60 == 0:\n",
        "                            latency = _dtime('before_frame_read', 'after_visualization')\n",
        "                            processing = _dtime('after_frame_read', 'after_visualization')\n",
        "                            timing_string = ', '.join([\n",
        "                                _dstr('read', 'before_frame_read', 'after_frame_read'),\n",
        "                                _dstr('preproc', 'after_frame_read', 'after_preprocessing'),\n",
        "                                'infer: %dms' % int(frame['time']['inference']),\n",
        "                                'vis: %dms' % int(frame['time']['visualization']),\n",
        "                                'proc: %dms' % processing,\n",
        "                                'latency: %dms' % latency,\n",
        "                            ])\n",
        "                            print('%08d [%s] %s' % (frame_index, fps_str, timing_string))\n",
        "\n",
        "        visualize_thread = threading.Thread(target=_visualize_output, name='visualization')\n",
        "        visualize_thread.daemon = True\n",
        "        visualize_thread.start()\n",
        "\n",
        "        # Do inference forever\n",
        "        infer = model.inference_generator()\n",
        "        while True:\n",
        "            output = next(infer)\n",
        "            for frame_index in np.unique(output['frame_index']):\n",
        "                if frame_index not in data_source._frames:\n",
        "                    continue\n",
        "                frame = data_source._frames[frame_index]\n",
        "                \"\"\"if 'inference' in frame['time']:\n",
        "                    frame['time']['inference'] += output['inference_time']\n",
        "                else:\"\"\"\n",
        "                frame['time']['inference'] = output['inference_time']\n",
        "            inferred_stuff_queue.put_nowait(output)\n",
        "\n",
        "            if not visualize_thread.isAlive():\n",
        "                break\n",
        "\n",
        "            if not data_source._open:\n",
        "                break\n",
        "\n",
        "        # Close video recording\n",
        "        if record_video and video_out is not None:\n",
        "            print(\"Video_out is not Null here\",video_out )\n",
        "            video_out_should_stop = True\n",
        "            video_out_queue.put_nowait(None)\n",
        "            with video_out_done:\n",
        "                video_out_done.wait()\n",
        "            print(\"Video_out_done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rJAGRIcjCFW"
      },
      "source": [
        "\"\"\"UnityEyes data source for gaze estimation.\"\"\"\n",
        "import os\n",
        "from threading import Lock\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import ujson\n",
        "\n",
        "\"\"\"from core import BaseDataSource\n",
        "import util.gaze\n",
        "import util.heatmap\"\"\"\n",
        "\n",
        "\n",
        "class TestUnityEyes(BaseDataSource):\n",
        "    \"\"\"UnityEyes data loading class.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 tensorflow_session: tf.Session,\n",
        "                 batch_size: int,\n",
        "                 unityeyes_path: str,\n",
        "                 testing=False,\n",
        "                 generate_heatmaps=False,\n",
        "                 eye_image_shape=(36, 60),\n",
        "                 heatmaps_scale=1.0,\n",
        "                 **kwargs):\n",
        "        \"\"\"Create queues and threads to read and preprocess data.\"\"\"\n",
        "        self._short_name = 'TestUnityEyes'\n",
        "        if testing:\n",
        "            self._short_name += ':test'\n",
        "\n",
        "        # Cache some parameters\n",
        "        self._eye_image_shape = eye_image_shape\n",
        "        self._heatmaps_scale = heatmaps_scale\n",
        "\n",
        "        # Create global index over all specified keys\n",
        "        self._images_path = unityeyes_path\n",
        "        self._file_stems = sorted([p[:-5] for p in os.listdir(unityeyes_path)\n",
        "                                   if p.endswith('.json')])\n",
        "        self._num_entries = len(self._file_stems)\n",
        "        print(\"Number of entries trained before\",self._num_entries)\n",
        "        self._num_entries = 3000\n",
        "        print(\"Number of entries trained after\",self._num_entries)\n",
        "        self._mutex = Lock()\n",
        "        self._current_index = 0\n",
        "\n",
        "        # Define bounds for noise values for different augmentation types\n",
        "        self._difficulty = 0.0\n",
        "        self._augmentation_ranges = {  # (easy, hard)\n",
        "            'translation': (2.0, 10.0),\n",
        "            'rotation': (0.1, 2.0),\n",
        "            'intensity': (0.5, 20.0),\n",
        "            'blur': (0.1, 1.0),\n",
        "            'scale': (0.01, 0.1),\n",
        "            'rescale': (1.0, 0.2),\n",
        "            'num_line': (0.0, 2.0),\n",
        "            'heatmap_sigma': (5.0, 2.5),\n",
        "        }\n",
        "        self._generate_heatmaps = generate_heatmaps\n",
        "\n",
        "        # Call parent class constructor\n",
        "        super().__init__(tensorflow_session, batch_size=batch_size, testing=testing, **kwargs)\n",
        "\n",
        "    @property\n",
        "    def num_entries(self):\n",
        "        \"\"\"Number of entries in this data source.\"\"\"\n",
        "        return self._num_entries\n",
        "\n",
        "    @property\n",
        "    def short_name(self):\n",
        "        \"\"\"Short name specifying source UnityEyes.\"\"\"\n",
        "        return self._short_name\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset index.\"\"\"\n",
        "        with self._mutex:\n",
        "            super().reset()\n",
        "            self._current_index = 0\n",
        "\n",
        "    def entry_generator(self, yield_just_one=False):\n",
        "        \"\"\"Read entry from TestUnityEyes.\"\"\"\n",
        "        try:\n",
        "            while range(1) if yield_just_one else True:\n",
        "                with self._mutex:\n",
        "                    if self._current_index >= self.num_entries:\n",
        "                        if self.testing:\n",
        "                            break\n",
        "                        else:\n",
        "                            self._current_index = 0\n",
        "                    current_index = self._current_index\n",
        "                    self._current_index += 1\n",
        "\n",
        "                file_stem = self._file_stems[current_index]\n",
        "                jpg_path = '%s/%s.jpg' % (self._images_path, file_stem)\n",
        "                json_path = '%s/%s.json' % (self._images_path, file_stem)\n",
        "                if not os.path.isfile(jpg_path) or not os.path.isfile(json_path):\n",
        "                    continue\n",
        "                with open(json_path, 'r') as f:\n",
        "                    json_data = ujson.load(f)\n",
        "                entry = {\n",
        "                    'full_image': cv.imread(jpg_path, cv.IMREAD_GRAYSCALE),\n",
        "                    'json_data': json_data,\n",
        "                }\n",
        "                assert entry['full_image'] is not None\n",
        "                yield entry\n",
        "        finally:\n",
        "            # Execute any cleanup operations as necessary\n",
        "            pass\n",
        "\n",
        "    def set_difficulty(self, difficulty):\n",
        "        \"\"\"Set difficulty of training data.\"\"\"\n",
        "        assert isinstance(difficulty, float)\n",
        "        assert 0.0 <= difficulty <= 1.0\n",
        "        self._difficulty = difficulty\n",
        "\n",
        "    def set_augmentation_range(self, augmentation_type, easy_value, hard_value):\n",
        "        \"\"\"Set 'range' for a known augmentation type.\"\"\"\n",
        "        assert isinstance(augmentation_type, str)\n",
        "        assert augmentation_type in self._augmentation_ranges\n",
        "        assert isinstance(easy_value, float) or isinstance(easy_value, int)\n",
        "        assert isinstance(hard_value, float) or isinstance(hard_value, int)\n",
        "        self._augmentation_ranges[augmentation_type] = (easy_value, hard_value)\n",
        "\n",
        "    def preprocess_entry(self, entry):\n",
        "        \"\"\"Use annotations to segment eyes and calculate gaze direction.\"\"\"\n",
        "        full_image = entry['full_image']\n",
        "        json_data = entry['json_data']\n",
        "        del entry['full_image']\n",
        "        del entry['json_data']\n",
        "\n",
        "        ih, iw = full_image.shape\n",
        "        iw_2, ih_2 = 0.5 * iw, 0.5 * ih\n",
        "        oh, ow = self._eye_image_shape\n",
        "\n",
        "        def process_coords(coords_list):\n",
        "            coords = [eval(l) for l in coords_list]\n",
        "            return np.array([(x, ih-y, z) for (x, y, z) in coords])\n",
        "        interior_landmarks = process_coords(json_data['interior_margin_2d'])\n",
        "        caruncle_landmarks = process_coords(json_data['caruncle_2d'])\n",
        "        iris_landmarks = process_coords(json_data['iris_2d'])\n",
        "\n",
        "        random_multipliers = []\n",
        "\n",
        "        def value_from_type(augmentation_type):\n",
        "            # Scale to be in range\n",
        "            easy_value, hard_value = self._augmentation_ranges[augmentation_type]\n",
        "            value = (hard_value - easy_value) * self._difficulty + easy_value\n",
        "            value = (np.clip(value, easy_value, hard_value)\n",
        "                     if easy_value < hard_value\n",
        "                     else np.clip(value, hard_value, easy_value))\n",
        "            return value\n",
        "\n",
        "        def noisy_value_from_type(augmentation_type):\n",
        "            # Get normal distributed random value\n",
        "            if len(random_multipliers) == 0:\n",
        "                random_multipliers.extend(\n",
        "                        list(np.random.normal(size=(len(self._augmentation_ranges),))))\n",
        "            return random_multipliers.pop() * value_from_type(augmentation_type)\n",
        "\n",
        "        # Only select almost frontal images\n",
        "        h_pitch, h_yaw, _ = eval(json_data['head_pose'])\n",
        "        if h_pitch > 180.0:  # Need to correct pitch\n",
        "            h_pitch -= 360.0\n",
        "        h_yaw -= 180.0  # Need to correct yaw\n",
        "        if abs(h_pitch) > 20 or abs(h_yaw) > 20:\n",
        "            return None\n",
        "\n",
        "        # Prepare to segment eye image\n",
        "        left_corner = np.mean(caruncle_landmarks[:, :2], axis=0)\n",
        "        right_corner = interior_landmarks[8, :2]\n",
        "        eye_width = 1.5 * abs(left_corner[0] - right_corner[0])\n",
        "        eye_middle = np.mean([np.amin(interior_landmarks[:, :2], axis=0),\n",
        "                              np.amax(interior_landmarks[:, :2], axis=0)], axis=0)\n",
        "\n",
        "        # Centre axes to eyeball centre\n",
        "        translate_mat = np.asmatrix(np.eye(3))\n",
        "        translate_mat[:2, 2] = [[-iw_2], [-ih_2]]\n",
        "\n",
        "        # Rotate eye image if requested\n",
        "        rotate_mat = np.asmatrix(np.eye(3))\n",
        "        rotation_noise = noisy_value_from_type('rotation')\n",
        "        if rotation_noise > 0:\n",
        "            rotate_angle = np.radians(rotation_noise)\n",
        "            cos_rotate = np.cos(rotate_angle)\n",
        "            sin_rotate = np.sin(rotate_angle)\n",
        "            rotate_mat[0, 0] = cos_rotate\n",
        "            rotate_mat[0, 1] = -sin_rotate\n",
        "            rotate_mat[1, 0] = sin_rotate\n",
        "            rotate_mat[1, 1] = cos_rotate\n",
        "\n",
        "        # Scale image to fit output dimensions (with a little bit of noise)\n",
        "        scale_mat = np.asmatrix(np.eye(3))\n",
        "        scale = 1. + noisy_value_from_type('scale')\n",
        "        scale_inv = 1. / scale\n",
        "        np.fill_diagonal(scale_mat, ow / eye_width * scale)\n",
        "        original_eyeball_radius = 71.7593\n",
        "        eyeball_radius = original_eyeball_radius * scale_mat[0, 0]  # See: https://goo.gl/ZnXgDE\n",
        "        ##entry['radius'] = np.float32(eyeball_radius)\n",
        "\n",
        "        # Re-centre eye image such that eye fits (based on determined `eye_middle`)\n",
        "        recentre_mat = np.asmatrix(np.eye(3))\n",
        "        recentre_mat[0, 2] = iw/2 - eye_middle[0] + 0.5 * eye_width * scale_inv\n",
        "        recentre_mat[1, 2] = ih/2 - eye_middle[1] + 0.5 * oh / ow * eye_width * scale_inv\n",
        "        recentre_mat[0, 2] += noisy_value_from_type('translation')  # x\n",
        "        recentre_mat[1, 2] += noisy_value_from_type('translation')  # y\n",
        "\n",
        "        # Apply transforms\n",
        "        transform_mat = recentre_mat * scale_mat * rotate_mat * translate_mat\n",
        "        eye = cv.warpAffine(full_image, transform_mat[:2, :3], (ow, oh))\n",
        "\n",
        "        # Convert look vector to gaze direction in polar angles\n",
        "        look_vec = np.array(eval(json_data['eye_details']['look_vec']))[:3]\n",
        "        look_vec[0] = -look_vec[0]\n",
        "        ##look_vector = 1 row 3 cols-> [ x y z ]\n",
        "        original_gaze = vector_to_pitchyaw(look_vec.reshape((1, 3))).flatten() ##util.gaze.\n",
        "        look_vec = rotate_mat * look_vec.reshape(3, 1)\n",
        "        gaze = vector_to_pitchyaw(look_vec.reshape((1, 3))).flatten() ##util.gaze.\n",
        "        if gaze[1] > 0.0:\n",
        "            gaze[1] = np.pi - gaze[1]\n",
        "        elif gaze[1] < 0.0:\n",
        "            gaze[1] = -(np.pi + gaze[1])\n",
        "        entry['gaze'] = gaze.astype(np.float32)\n",
        "\n",
        "        # Draw line randomly\n",
        "        num_line_noise = int(np.round(noisy_value_from_type('num_line')))\n",
        "        if num_line_noise > 0:\n",
        "            line_rand_nums = np.random.rand(5 * num_line_noise)\n",
        "            for i in range(num_line_noise):\n",
        "                j = 5 * i\n",
        "                lx0, ly0 = int(ow * line_rand_nums[j]), oh\n",
        "                lx1, ly1 = ow, int(oh * line_rand_nums[j + 1])\n",
        "                direction = line_rand_nums[j + 2]\n",
        "                if direction < 0.25:\n",
        "                    lx1 = ly0 = 0\n",
        "                elif direction < 0.5:\n",
        "                    lx1 = 0\n",
        "                elif direction < 0.75:\n",
        "                    ly0 = 0\n",
        "                line_colour = int(255 * line_rand_nums[j + 3])\n",
        "                eye = cv.line(eye, (lx0, ly0), (lx1, ly1),\n",
        "                              color=(line_colour, line_colour, line_colour),\n",
        "                              thickness=max(1, int(6*line_rand_nums[j + 4])),\n",
        "                              lineType=cv.LINE_AA)\n",
        "\n",
        "        # Rescale image if required\n",
        "        rescale_max = value_from_type('rescale')\n",
        "        if rescale_max < 1.0:\n",
        "            rescale_noise = np.random.uniform(low=rescale_max, high=1.0)\n",
        "            interpolation = cv.INTER_CUBIC\n",
        "            eye = cv.resize(eye, dsize=(0, 0), fx=rescale_noise, fy=rescale_noise,\n",
        "                            interpolation=interpolation)\n",
        "            eye = cv.equalizeHist(eye)\n",
        "            eye = cv.resize(eye, dsize=(ow, oh), interpolation=interpolation)\n",
        "\n",
        "        # Add rgb noise to eye image\n",
        "        intensity_noise = int(value_from_type('intensity'))\n",
        "        if intensity_noise > 0:\n",
        "            eye = eye.astype(np.int16)\n",
        "            eye += np.random.randint(low=-intensity_noise, high=intensity_noise,\n",
        "                                     size=eye.shape, dtype=np.int16)\n",
        "            cv.normalize(eye, eye, alpha=0, beta=255, norm_type=cv.NORM_MINMAX)\n",
        "            eye = eye.astype(np.uint8)\n",
        "\n",
        "        # Add blur to eye image\n",
        "        blur_noise = noisy_value_from_type('blur')\n",
        "        if blur_noise > 0:\n",
        "            eye = cv.GaussianBlur(eye, (7, 7), 0.5 + np.abs(blur_noise))\n",
        "\n",
        "        # Histogram equalization and preprocessing for NN\n",
        "        eye = cv.equalizeHist(eye) ## increase contrast of the image\n",
        "        eye = eye.astype(np.float32)\n",
        "        eye *= 2.0 / 255.0\n",
        "        eye -= 1.0\n",
        "        eye = np.expand_dims(eye, -1 if self.data_format == 'NHWC' else 0)\n",
        "        entry['eye'] = eye\n",
        "\n",
        "        # Select and transform landmark coordinates\n",
        "        iris_centre = np.asarray([\n",
        "            iw_2 + original_eyeball_radius * -np.cos(original_gaze[0]) * np.sin(original_gaze[1]),\n",
        "            ih_2 + original_eyeball_radius * -np.sin(original_gaze[0]),\n",
        "        ])\n",
        "        landmarks = np.concatenate([interior_landmarks[::2, :2],  # 8\n",
        "                                    iris_landmarks[::4, :2],  # 8\n",
        "                                    iris_centre.reshape((1, 2)),\n",
        "                                    [[iw_2, ih_2]],  # Eyeball centre\n",
        "                                    ])  # 18 in total\n",
        "        landmarks = np.asmatrix(np.pad(landmarks, ((0, 0), (0, 1)), 'constant',\n",
        "                                       constant_values=1))\n",
        "        landmarks = np.asarray(landmarks * transform_mat.T)\n",
        "        landmarks = landmarks[:, :2]  # We only need x, y\n",
        "        entry['landmarks'] = landmarks.astype(np.float32)\n",
        "\n",
        "        # Generate heatmaps if necessary\n",
        "        \"\"\"if self._generate_heatmaps:\n",
        "            # Should be half-scale (compared to eye image). # util.heatmap.gaussian_2d\n",
        "            entry['heatmaps'] = np.asarray([\n",
        "                    gaussian_2d(\n",
        "                    shape=(self._heatmaps_scale*oh, self._heatmaps_scale*ow),\n",
        "                    centre=self._heatmaps_scale*landmark,\n",
        "                    sigma=value_from_type('heatmap_sigma'),\n",
        "                )\n",
        "                for landmark in entry['landmarks']\n",
        "            ]).astype(np.float32)\n",
        "            if self.data_format == 'NHWC':\n",
        "                entry['heatmaps'] = np.transpose(entry['heatmaps'], (1, 2, 0))\"\"\"\n",
        "\n",
        "        return entry\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1gIcT4DN77Z"
      },
      "source": [
        "##test_elg()\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import queue\n",
        "import threading\n",
        "import time\n",
        "\n",
        "import coloredlogs\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\"\"\"from datasources import Video, Webcam\n",
        "from models import ELG\n",
        "import util.gaze\"\"\"\n",
        "mean_angular_errors = [] \n",
        "predicted = []\n",
        "actual_gaze = []\n",
        "def testUnity_elg():\n",
        "    \n",
        "    \n",
        "    coloredlogs.install(\n",
        "        datefmt='%d/%m %H:%M',\n",
        "        fmt='%(asctime)s %(levelname)s %(message)s',\n",
        "        level=\"debug\".upper(),   \n",
        "    )\n",
        "\n",
        "    # Check if GPU is available\n",
        "    from tensorflow.python.client import device_lib\n",
        "    session_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "    gpu_available = False\n",
        "    try:\n",
        "        gpus = [d for d in device_lib.list_local_devices(config=session_config)\n",
        "                if d.device_type == 'GPU']\n",
        "        gpu_available = len(gpus) > 0\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "\n",
        "    # Initialize Tensorflow session\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "    with tf.Session(config=session_config) as session:\n",
        "\n",
        "        # Declare some parameters\n",
        "        batch_size = 2\n",
        "\n",
        "        \n",
        "        data_source = TestUnityEyes(\n",
        "            tensorflow_session=session,\n",
        "            batch_size=batch_size,\n",
        "            data_format='NCHW' if gpu_available else 'NHWC',\n",
        "            unityeyes_path=image_dir, #\"/content/gdrive/My Drive/test-data-Eye-tracking/imgs/imgs\",\n",
        "            #min_after_dequeue=1000,\n",
        "            generate_heatmaps=False, #True,\n",
        "            #shuffle=True,\n",
        "            #staging=True,\n",
        "            eye_image_shape=(36, 60)  #,\n",
        "            #heatmaps_scale=1.0 / elg_first_layer_stride,\n",
        "        )\n",
        "        # Define model\n",
        "        model = ELG(\n",
        "                session, train_data={'testUnity': data_source},\n",
        "                first_layer_stride=1,\n",
        "                num_modules=2,\n",
        "                num_feature_maps=32,\n",
        "                learning_schedule=[\n",
        "                    {\n",
        "                        'loss_terms_to_optimize': {'dummy': ['hourglass', 'radius']},\n",
        "                    },\n",
        "                ],\n",
        "            )\n",
        "            \n",
        "        print(\"model complete\")\n",
        "        infer = model.inference_generator()\n",
        "        print(\"inference complete\", infer)\n",
        "        i = 0\n",
        "        while True:\n",
        "          output = next(infer)\n",
        "          ##print(\"output\",output)\n",
        "          #print(\"output['landmarks']\",output['landmarks'], output['landmarks'].shape)\n",
        "          eye_landmarks = output['landmarks'][1, :]\n",
        "          eye_radius = output['radius'][1][0]\n",
        "\n",
        "          eye_landmarks = np.concatenate([eye_landmarks,\n",
        "                                          [[eye_landmarks[-1, 0] + eye_radius,\n",
        "                                            eye_landmarks[-1, 1]]]])\n",
        "          eye_landmarks = np.asmatrix(np.pad(eye_landmarks, ((0, 0), (0, 1)),\n",
        "                                              'constant', constant_values=1.0))\n",
        "          \"\"\"eye_landmarks = (eye_landmarks *\n",
        "                            eye['inv_landmarks_transform_mat'].T)[:, :2]\"\"\"\n",
        "          eye_landmarks = eye_landmarks[:, :2]\n",
        "          eye_landmarks = np.asarray(eye_landmarks)\n",
        "          eyelid_landmarks = eye_landmarks[0:8, :]\n",
        "          iris_landmarks = eye_landmarks[8:16, :]\n",
        "          print(\"iris centre \",eye_landmarks[16, :])\n",
        "          iris_centre = eye_landmarks[16, :]\n",
        "          eyeball_centre = eye_landmarks[17, :]\n",
        "          eyeball_radius = np.linalg.norm(eye_landmarks[18, :] -\n",
        "                                          eye_landmarks[17, :])\n",
        "          \n",
        "          i_x0, i_y0 = iris_centre\n",
        "          e_x0, e_y0 = eyeball_centre\n",
        "          theta = -np.arcsin(np.clip((i_y0 - e_y0) / eyeball_radius, -1.0, 1.0))\n",
        "          phi = np.arcsin(np.clip((i_x0 - e_x0) / (eyeball_radius * -np.cos(theta)),\n",
        "                                  -1.0, 1.0))\n",
        "          current_gaze = np.array([theta, phi])\n",
        "          print(\"current_gaze\",current_gaze,current_gaze.shape)\n",
        "          current_gaze = np.atleast_2d(current_gaze)\n",
        "          predicted_gaze = np.atleast_2d(output['gaze'][1])\n",
        "          print(\"predicted gaze\",predicted_gaze, predicted_gaze.shape)\n",
        "          error = angular_error(current_gaze,predicted_gaze)\n",
        "          print(\"angular_error\",error)\n",
        "          mean_angular_errors.append(error)\n",
        "          predicted.append(predicted_gaze)\n",
        "          actual_gaze.append(current_gaze)\n",
        "          print(i)\n",
        "          i+=1\n",
        "\n",
        "          \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo1zlPVzWs2c"
      },
      "source": [
        "testUnity_elg()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv_6ZxAasP5Q"
      },
      "source": [
        "import pandas as pd\n",
        "predicted_df = pd.DataFrame(np.concatenate(predicted), columns = ['Yaw','Pitch'])\n",
        "actual_df = pd.DataFrame(np.concatenate(actual_gaze), columns = ['Yaw','Pitch'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHV_Ey6zOWfd"
      },
      "source": [
        "np.mean(mean_angular_errors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFgsJM7OZxhe"
      },
      "source": [
        "mean_angular_errors = [] \n",
        "predicted = []\n",
        "actual_gaze = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy3sf5NmK9kM"
      },
      "source": [
        "predicted_df.head()\n",
        "actual_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrO-STo1K-ms"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(actual_df['Pitch'], predicted_df[\"Pitch\"], c='crimson')\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(actual_df['Pitch'], predicted_df[\"Pitch\"])\n",
        "ax.plot([actual_df['Pitch'].min(), actual_df['Pitch'].max()], [actual_df['Pitch'].min(), actual_df['Pitch'].max()], 'k--', lw=4)\n",
        "ax.set_xlabel('Measured')\n",
        "ax.set_ylabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um0z7pfwQTkw"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(actual_df['Yaw'], predicted_df[\"Yaw\"])\n",
        "ax.plot([actual_df['Yaw'].min(), actual_df['Yaw'].max()], [actual_df['Yaw'].min(), actual_df['Yaw'].max()], 'k--', lw=4)\n",
        "ax.set_xlabel('Measured')\n",
        "ax.set_ylabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "155L34aNZPSS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}